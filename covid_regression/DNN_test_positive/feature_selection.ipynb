{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wehs7661/deep_learning_projects/blob/master/covid_regression/DNN_test_positive/feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgqHiYQXU2Q2"
      },
      "source": [
        "# **Exploring feature selection and regularization: Taking the COVID-19 positive rate prediction as an example**\n",
        "\n",
        "In this notebook, we will explore the application of various feature selection methods to the neural network for predicting the COVID-19 positive rate, using the datasets adapted from [daily surveys conducted by the Delphi Group @ CMU](https://delphi.cmu.edu/covidcast/). This notebook is adapted from the [notebook](https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb) written by Heng-Jui Chang @ NTUEE as the first homework in the class of 2021 Machine Learning class taught by Dr. Hung-Yi Lee @ NTUEE. The final result of the original assignment is a CSV file containing the predictions of the COVID-19 positive rate generated from the neural network built and trained in the assignment, which should be uploaded to the corresponding [Kaggle competition](https://www.kaggle.com/c/ml2021spring-hw1/data) to assess the model by calculating the test loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jgt1COHZMrx"
      },
      "source": [
        "## **1. Statement of the problem**\n",
        "Now, before we dive into what feature selection methods we are going to explore, let's first understand our task by taking a look at the training set and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qop5PHg5W9zG",
        "outputId": "f5da6f1e-1bee-44d8-97c2-56258eae8937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 87.0MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 106MB/s]\n"
          ]
        }
      ],
      "source": [
        "# The data below can also be downloaded from Kaggle.\n",
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "RVgMle8OU1UC",
        "outputId": "c41cd6db-02cb-4841-8349-c259050e79e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id   AL   AK   AZ   AR   CA   CO   CT   FL   GA   ID   IL   IN   IA  \\\n",
              "0        0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1        1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2        2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3        3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4        4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2696  2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2697  2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2698  2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2699  2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "       KS   KY   LA   MD   MA   MI   MN   MS   MO   NE   NV   NJ   NM   NY  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "       NC   OH   OK   OR   PA   RI   SC   TX   UT   VA   WA   WV   WI  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "\n",
              "           cli       ili  hh_cmnty_cli  nohh_cmnty_cli  wearing_mask  \\\n",
              "0     0.814610  0.771356     25.648907       21.242063     84.644672   \n",
              "1     0.838995  0.807767     25.679101       21.280270     84.005294   \n",
              "2     0.897802  0.887893     26.060544       21.503832     84.438618   \n",
              "3     0.972842  0.965496     25.754087       21.016210     84.133873   \n",
              "4     0.955306  0.963079     25.947015       20.941798     83.995931   \n",
              "...        ...       ...           ...             ...           ...   \n",
              "2695  0.655823  0.659976     25.265366       20.468897     91.011756   \n",
              "2696  0.598352  0.602552     25.299465       20.756444     90.682057   \n",
              "2697  0.586713  0.597559     25.271178       20.770195     90.866100   \n",
              "2698  0.576435  0.595312     24.607461       20.176201     90.846126   \n",
              "2699  0.562426  0.572969     24.020275       19.654514     90.928655   \n",
              "\n",
              "      travel_outside_state  work_outside_home       shop  restaurant  \\\n",
              "0                13.462475          36.519841  63.139094   23.835119   \n",
              "1                13.467716          36.637887  63.318650   23.688882   \n",
              "2                13.038611          36.429119  62.434539   23.812411   \n",
              "3                12.581952          36.416557  62.024517   23.682974   \n",
              "4                12.938675          37.014578  62.116842   23.593983   \n",
              "...                    ...                ...        ...         ...   \n",
              "2695              6.801897          32.727184  50.265694   15.188547   \n",
              "2696              7.152368          33.638563  50.050349   15.462823   \n",
              "2697              6.857209          33.959012  50.024971   15.090116   \n",
              "2698              6.851475          33.932384  49.885129   14.779264   \n",
              "2699              6.642911          33.822577  50.056772   14.961085   \n",
              "\n",
              "      spent_time  large_event  public_transit    anxious  depressed  \\\n",
              "0      44.726055    16.946929        1.716262  15.494193  12.043275   \n",
              "1      44.385166    16.463551        1.664819  15.299228  12.051505   \n",
              "2      43.430423    16.151527        1.602635  15.409449  12.088688   \n",
              "3      43.196313    16.123386        1.641863  15.230063  11.809047   \n",
              "4      43.362200    16.159971        1.677523  15.717207  12.355918   \n",
              "...          ...          ...             ...        ...        ...   \n",
              "2695   31.597793     8.013637        1.768811  14.699027  11.227049   \n",
              "2696   31.656358     8.239559        1.789015  14.808636  11.371546   \n",
              "2697   30.839219     7.849525        1.760094  14.617563  11.163213   \n",
              "2698   30.617100     7.754800        1.780730  14.513419  11.281241   \n",
              "2699   30.595194     7.744075        1.921828  14.160990  11.163526   \n",
              "\n",
              "      felt_isolated  worried_become_ill  worried_finances  tested_positive  \\\n",
              "0         17.000647           53.439316         43.279629        19.586492   \n",
              "1         16.552264           53.256795         43.622728        20.151838   \n",
              "2         16.702086           53.991549         43.604229        20.704935   \n",
              "3         16.506973           54.185521         42.665766        21.292911   \n",
              "4         16.273294           53.637069         42.972417        21.166656   \n",
              "...             ...                 ...               ...              ...   \n",
              "2695      18.814486           68.115748         38.478143        13.869286   \n",
              "2696      19.257324           67.691795         38.953184        13.434180   \n",
              "2697      18.742673           68.024690         38.920206        13.008853   \n",
              "2698      18.539741           67.855755         39.224244        12.725638   \n",
              "2699      18.702564           67.731162         38.740651        12.613441   \n",
              "\n",
              "         cli.1     ili.1  hh_cmnty_cli.1  nohh_cmnty_cli.1  wearing_mask.1  \\\n",
              "0     0.838995  0.807767       25.679101         21.280270       84.005294   \n",
              "1     0.897802  0.887893       26.060544         21.503832       84.438618   \n",
              "2     0.972842  0.965496       25.754087         21.016210       84.133873   \n",
              "3     0.955306  0.963079       25.947015         20.941798       83.995931   \n",
              "4     0.947513  0.968764       26.350501         21.109971       83.819531   \n",
              "...        ...       ...             ...               ...             ...   \n",
              "2695  0.598352  0.602552       25.299465         20.756444       90.682057   \n",
              "2696  0.586713  0.597559       25.271178         20.770195       90.866100   \n",
              "2697  0.576435  0.595312       24.607461         20.176201       90.846126   \n",
              "2698  0.562426  0.572969       24.020275         19.654514       90.928655   \n",
              "2699  0.600671  0.611160       23.797738         19.519105       90.957424   \n",
              "\n",
              "      travel_outside_state.1  work_outside_home.1     shop.1  restaurant.1  \\\n",
              "0                  13.467716            36.637887  63.318650     23.688882   \n",
              "1                  13.038611            36.429119  62.434539     23.812411   \n",
              "2                  12.581952            36.416557  62.024517     23.682974   \n",
              "3                  12.938675            37.014578  62.116842     23.593983   \n",
              "4                  12.452336            36.270021  61.294809     22.576992   \n",
              "...                      ...                  ...        ...           ...   \n",
              "2695                7.152368            33.638563  50.050349     15.462823   \n",
              "2696                6.857209            33.959012  50.024971     15.090116   \n",
              "2697                6.851475            33.932384  49.885129     14.779264   \n",
              "2698                6.642911            33.822577  50.056772     14.961085   \n",
              "2699                6.800289            33.196095  49.620924     14.609582   \n",
              "\n",
              "      spent_time.1  large_event.1  public_transit.1  anxious.1  depressed.1  \\\n",
              "0        44.385166      16.463551          1.664819  15.299228    12.051505   \n",
              "1        43.430423      16.151527          1.602635  15.409449    12.088688   \n",
              "2        43.196313      16.123386          1.641863  15.230063    11.809047   \n",
              "3        43.362200      16.159971          1.677523  15.717207    12.355918   \n",
              "4        42.954574      15.544373          1.578030  15.295650    12.218123   \n",
              "...            ...            ...               ...        ...          ...   \n",
              "2695     31.656358       8.239559          1.789015  14.808636    11.371546   \n",
              "2696     30.839219       7.849525          1.760094  14.617563    11.163213   \n",
              "2697     30.617100       7.754800          1.780730  14.513419    11.281241   \n",
              "2698     30.595194       7.744075          1.921828  14.160990    11.163526   \n",
              "2699     30.420998       7.687974          1.992580  14.409427    11.330301   \n",
              "\n",
              "      felt_isolated.1  worried_become_ill.1  worried_finances.1  \\\n",
              "0           16.552264             53.256795           43.622728   \n",
              "1           16.702086             53.991549           43.604229   \n",
              "2           16.506973             54.185521           42.665766   \n",
              "3           16.273294             53.637069           42.972417   \n",
              "4           16.045504             52.446223           42.907472   \n",
              "...               ...                   ...                 ...   \n",
              "2695        19.257324             67.691795           38.953184   \n",
              "2696        18.742673             68.024690           38.920206   \n",
              "2697        18.539741             67.855755           39.224244   \n",
              "2698        18.702564             67.731162           38.740651   \n",
              "2699        19.134697             67.795100           38.595125   \n",
              "\n",
              "      tested_positive.1     cli.2     ili.2  hh_cmnty_cli.2  nohh_cmnty_cli.2  \\\n",
              "0             20.151838  0.897802  0.887893       26.060544         21.503832   \n",
              "1             20.704935  0.972842  0.965496       25.754087         21.016210   \n",
              "2             21.292911  0.955306  0.963079       25.947015         20.941798   \n",
              "3             21.166656  0.947513  0.968764       26.350501         21.109971   \n",
              "4             19.896607  0.883833  0.893020       26.480624         21.003982   \n",
              "...                 ...       ...       ...             ...               ...   \n",
              "2695          13.434180  0.586713  0.597559       25.271178         20.770195   \n",
              "2696          13.008853  0.576435  0.595312       24.607461         20.176201   \n",
              "2697          12.725638  0.562426  0.572969       24.020275         19.654514   \n",
              "2698          12.613441  0.600671  0.611160       23.797738         19.519105   \n",
              "2699          12.477227  0.560519  0.571126       23.467835         19.174193   \n",
              "\n",
              "      wearing_mask.2  travel_outside_state.2  work_outside_home.2     shop.2  \\\n",
              "0          84.438618               13.038611            36.429119  62.434539   \n",
              "1          84.133873               12.581952            36.416557  62.024517   \n",
              "2          83.995931               12.938675            37.014578  62.116842   \n",
              "3          83.819531               12.452336            36.270021  61.294809   \n",
              "4          84.049437               12.224644            35.380198  60.664482   \n",
              "...              ...                     ...                  ...        ...   \n",
              "2695       90.866100                6.857209            33.959012  50.024971   \n",
              "2696       90.846126                6.851475            33.932384  49.885129   \n",
              "2697       90.928655                6.642911            33.822577  50.056772   \n",
              "2698       90.957424                6.800289            33.196095  49.620924   \n",
              "2699       91.110463                6.931543            33.096657  49.510599   \n",
              "\n",
              "      restaurant.2  spent_time.2  large_event.2  public_transit.2  anxious.2  \\\n",
              "0        23.812411     43.430423      16.151527          1.602635  15.409449   \n",
              "1        23.682974     43.196313      16.123386          1.641863  15.230063   \n",
              "2        23.593983     43.362200      16.159971          1.677523  15.717207   \n",
              "3        22.576992     42.954574      15.544373          1.578030  15.295650   \n",
              "4        22.091433     43.290957      15.214655          1.641667  14.778802   \n",
              "...            ...           ...            ...               ...        ...   \n",
              "2695     15.090116     30.839219       7.849525          1.760094  14.617563   \n",
              "2696     14.779264     30.617100       7.754800          1.780730  14.513419   \n",
              "2697     14.961085     30.595194       7.744075          1.921828  14.160990   \n",
              "2698     14.609582     30.420998       7.687974          1.992580  14.409427   \n",
              "2699     14.464053     30.469791       7.692942          1.966064  14.616400   \n",
              "\n",
              "      depressed.2  felt_isolated.2  worried_become_ill.2  worried_finances.2  \\\n",
              "0       12.088688        16.702086             53.991549           43.604229   \n",
              "1       11.809047        16.506973             54.185521           42.665766   \n",
              "2       12.355918        16.273294             53.637069           42.972417   \n",
              "3       12.218123        16.045504             52.446223           42.907472   \n",
              "4       12.417256        16.134238             52.560315           43.321985   \n",
              "...           ...              ...                   ...                 ...   \n",
              "2695    11.163213        18.742673             68.024690           38.920206   \n",
              "2696    11.281241        18.539741             67.855755           39.224244   \n",
              "2697    11.163526        18.702564             67.731162           38.740651   \n",
              "2698    11.330301        19.134697             67.795100           38.595125   \n",
              "2699    11.522773        19.295834             68.284078           38.453820   \n",
              "\n",
              "      tested_positive.2  \n",
              "0             20.704935  \n",
              "1             21.292911  \n",
              "2             21.166656  \n",
              "3             19.896607  \n",
              "4             20.178428  \n",
              "...                 ...  \n",
              "2695          13.008853  \n",
              "2696          12.725638  \n",
              "2697          12.613441  \n",
              "2698          12.477227  \n",
              "2699          11.811719  \n",
              "\n",
              "[2700 rows x 95 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca81e681-3837-4594-91ac-15da62175446\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>AL</th>\n",
              "      <th>AK</th>\n",
              "      <th>AZ</th>\n",
              "      <th>AR</th>\n",
              "      <th>CA</th>\n",
              "      <th>CO</th>\n",
              "      <th>CT</th>\n",
              "      <th>FL</th>\n",
              "      <th>GA</th>\n",
              "      <th>ID</th>\n",
              "      <th>IL</th>\n",
              "      <th>IN</th>\n",
              "      <th>IA</th>\n",
              "      <th>KS</th>\n",
              "      <th>KY</th>\n",
              "      <th>LA</th>\n",
              "      <th>MD</th>\n",
              "      <th>MA</th>\n",
              "      <th>MI</th>\n",
              "      <th>MN</th>\n",
              "      <th>MS</th>\n",
              "      <th>MO</th>\n",
              "      <th>NE</th>\n",
              "      <th>NV</th>\n",
              "      <th>NJ</th>\n",
              "      <th>NM</th>\n",
              "      <th>NY</th>\n",
              "      <th>NC</th>\n",
              "      <th>OH</th>\n",
              "      <th>OK</th>\n",
              "      <th>OR</th>\n",
              "      <th>PA</th>\n",
              "      <th>RI</th>\n",
              "      <th>SC</th>\n",
              "      <th>TX</th>\n",
              "      <th>UT</th>\n",
              "      <th>VA</th>\n",
              "      <th>WA</th>\n",
              "      <th>WV</th>\n",
              "      <th>WI</th>\n",
              "      <th>cli</th>\n",
              "      <th>ili</th>\n",
              "      <th>hh_cmnty_cli</th>\n",
              "      <th>nohh_cmnty_cli</th>\n",
              "      <th>wearing_mask</th>\n",
              "      <th>travel_outside_state</th>\n",
              "      <th>work_outside_home</th>\n",
              "      <th>shop</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>spent_time</th>\n",
              "      <th>large_event</th>\n",
              "      <th>public_transit</th>\n",
              "      <th>anxious</th>\n",
              "      <th>depressed</th>\n",
              "      <th>felt_isolated</th>\n",
              "      <th>worried_become_ill</th>\n",
              "      <th>worried_finances</th>\n",
              "      <th>tested_positive</th>\n",
              "      <th>cli.1</th>\n",
              "      <th>ili.1</th>\n",
              "      <th>hh_cmnty_cli.1</th>\n",
              "      <th>nohh_cmnty_cli.1</th>\n",
              "      <th>wearing_mask.1</th>\n",
              "      <th>travel_outside_state.1</th>\n",
              "      <th>work_outside_home.1</th>\n",
              "      <th>shop.1</th>\n",
              "      <th>restaurant.1</th>\n",
              "      <th>spent_time.1</th>\n",
              "      <th>large_event.1</th>\n",
              "      <th>public_transit.1</th>\n",
              "      <th>anxious.1</th>\n",
              "      <th>depressed.1</th>\n",
              "      <th>felt_isolated.1</th>\n",
              "      <th>worried_become_ill.1</th>\n",
              "      <th>worried_finances.1</th>\n",
              "      <th>tested_positive.1</th>\n",
              "      <th>cli.2</th>\n",
              "      <th>ili.2</th>\n",
              "      <th>hh_cmnty_cli.2</th>\n",
              "      <th>nohh_cmnty_cli.2</th>\n",
              "      <th>wearing_mask.2</th>\n",
              "      <th>travel_outside_state.2</th>\n",
              "      <th>work_outside_home.2</th>\n",
              "      <th>shop.2</th>\n",
              "      <th>restaurant.2</th>\n",
              "      <th>spent_time.2</th>\n",
              "      <th>large_event.2</th>\n",
              "      <th>public_transit.2</th>\n",
              "      <th>anxious.2</th>\n",
              "      <th>depressed.2</th>\n",
              "      <th>felt_isolated.2</th>\n",
              "      <th>worried_become_ill.2</th>\n",
              "      <th>worried_finances.2</th>\n",
              "      <th>tested_positive.2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.814610</td>\n",
              "      <td>0.771356</td>\n",
              "      <td>25.648907</td>\n",
              "      <td>21.242063</td>\n",
              "      <td>84.644672</td>\n",
              "      <td>13.462475</td>\n",
              "      <td>36.519841</td>\n",
              "      <td>63.139094</td>\n",
              "      <td>23.835119</td>\n",
              "      <td>44.726055</td>\n",
              "      <td>16.946929</td>\n",
              "      <td>1.716262</td>\n",
              "      <td>15.494193</td>\n",
              "      <td>12.043275</td>\n",
              "      <td>17.000647</td>\n",
              "      <td>53.439316</td>\n",
              "      <td>43.279629</td>\n",
              "      <td>19.586492</td>\n",
              "      <td>0.838995</td>\n",
              "      <td>0.807767</td>\n",
              "      <td>25.679101</td>\n",
              "      <td>21.280270</td>\n",
              "      <td>84.005294</td>\n",
              "      <td>13.467716</td>\n",
              "      <td>36.637887</td>\n",
              "      <td>63.318650</td>\n",
              "      <td>23.688882</td>\n",
              "      <td>44.385166</td>\n",
              "      <td>16.463551</td>\n",
              "      <td>1.664819</td>\n",
              "      <td>15.299228</td>\n",
              "      <td>12.051505</td>\n",
              "      <td>16.552264</td>\n",
              "      <td>53.256795</td>\n",
              "      <td>43.622728</td>\n",
              "      <td>20.151838</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.838995</td>\n",
              "      <td>0.807767</td>\n",
              "      <td>25.679101</td>\n",
              "      <td>21.280270</td>\n",
              "      <td>84.005294</td>\n",
              "      <td>13.467716</td>\n",
              "      <td>36.637887</td>\n",
              "      <td>63.318650</td>\n",
              "      <td>23.688882</td>\n",
              "      <td>44.385166</td>\n",
              "      <td>16.463551</td>\n",
              "      <td>1.664819</td>\n",
              "      <td>15.299228</td>\n",
              "      <td>12.051505</td>\n",
              "      <td>16.552264</td>\n",
              "      <td>53.256795</td>\n",
              "      <td>43.622728</td>\n",
              "      <td>20.151838</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "      <td>0.947513</td>\n",
              "      <td>0.968764</td>\n",
              "      <td>26.350501</td>\n",
              "      <td>21.109971</td>\n",
              "      <td>83.819531</td>\n",
              "      <td>12.452336</td>\n",
              "      <td>36.270021</td>\n",
              "      <td>61.294809</td>\n",
              "      <td>22.576992</td>\n",
              "      <td>42.954574</td>\n",
              "      <td>15.544373</td>\n",
              "      <td>1.578030</td>\n",
              "      <td>15.295650</td>\n",
              "      <td>12.218123</td>\n",
              "      <td>16.045504</td>\n",
              "      <td>52.446223</td>\n",
              "      <td>42.907472</td>\n",
              "      <td>19.896607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "      <td>0.947513</td>\n",
              "      <td>0.968764</td>\n",
              "      <td>26.350501</td>\n",
              "      <td>21.109971</td>\n",
              "      <td>83.819531</td>\n",
              "      <td>12.452336</td>\n",
              "      <td>36.270021</td>\n",
              "      <td>61.294809</td>\n",
              "      <td>22.576992</td>\n",
              "      <td>42.954574</td>\n",
              "      <td>15.544373</td>\n",
              "      <td>1.578030</td>\n",
              "      <td>15.295650</td>\n",
              "      <td>12.218123</td>\n",
              "      <td>16.045504</td>\n",
              "      <td>52.446223</td>\n",
              "      <td>42.907472</td>\n",
              "      <td>19.896607</td>\n",
              "      <td>0.883833</td>\n",
              "      <td>0.893020</td>\n",
              "      <td>26.480624</td>\n",
              "      <td>21.003982</td>\n",
              "      <td>84.049437</td>\n",
              "      <td>12.224644</td>\n",
              "      <td>35.380198</td>\n",
              "      <td>60.664482</td>\n",
              "      <td>22.091433</td>\n",
              "      <td>43.290957</td>\n",
              "      <td>15.214655</td>\n",
              "      <td>1.641667</td>\n",
              "      <td>14.778802</td>\n",
              "      <td>12.417256</td>\n",
              "      <td>16.134238</td>\n",
              "      <td>52.560315</td>\n",
              "      <td>43.321985</td>\n",
              "      <td>20.178428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2695</th>\n",
              "      <td>2695</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655823</td>\n",
              "      <td>0.659976</td>\n",
              "      <td>25.265366</td>\n",
              "      <td>20.468897</td>\n",
              "      <td>91.011756</td>\n",
              "      <td>6.801897</td>\n",
              "      <td>32.727184</td>\n",
              "      <td>50.265694</td>\n",
              "      <td>15.188547</td>\n",
              "      <td>31.597793</td>\n",
              "      <td>8.013637</td>\n",
              "      <td>1.768811</td>\n",
              "      <td>14.699027</td>\n",
              "      <td>11.227049</td>\n",
              "      <td>18.814486</td>\n",
              "      <td>68.115748</td>\n",
              "      <td>38.478143</td>\n",
              "      <td>13.869286</td>\n",
              "      <td>0.598352</td>\n",
              "      <td>0.602552</td>\n",
              "      <td>25.299465</td>\n",
              "      <td>20.756444</td>\n",
              "      <td>90.682057</td>\n",
              "      <td>7.152368</td>\n",
              "      <td>33.638563</td>\n",
              "      <td>50.050349</td>\n",
              "      <td>15.462823</td>\n",
              "      <td>31.656358</td>\n",
              "      <td>8.239559</td>\n",
              "      <td>1.789015</td>\n",
              "      <td>14.808636</td>\n",
              "      <td>11.371546</td>\n",
              "      <td>19.257324</td>\n",
              "      <td>67.691795</td>\n",
              "      <td>38.953184</td>\n",
              "      <td>13.434180</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2696</th>\n",
              "      <td>2696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.598352</td>\n",
              "      <td>0.602552</td>\n",
              "      <td>25.299465</td>\n",
              "      <td>20.756444</td>\n",
              "      <td>90.682057</td>\n",
              "      <td>7.152368</td>\n",
              "      <td>33.638563</td>\n",
              "      <td>50.050349</td>\n",
              "      <td>15.462823</td>\n",
              "      <td>31.656358</td>\n",
              "      <td>8.239559</td>\n",
              "      <td>1.789015</td>\n",
              "      <td>14.808636</td>\n",
              "      <td>11.371546</td>\n",
              "      <td>19.257324</td>\n",
              "      <td>67.691795</td>\n",
              "      <td>38.953184</td>\n",
              "      <td>13.434180</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2697</th>\n",
              "      <td>2697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2698</th>\n",
              "      <td>2698</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "      <td>0.600671</td>\n",
              "      <td>0.611160</td>\n",
              "      <td>23.797738</td>\n",
              "      <td>19.519105</td>\n",
              "      <td>90.957424</td>\n",
              "      <td>6.800289</td>\n",
              "      <td>33.196095</td>\n",
              "      <td>49.620924</td>\n",
              "      <td>14.609582</td>\n",
              "      <td>30.420998</td>\n",
              "      <td>7.687974</td>\n",
              "      <td>1.992580</td>\n",
              "      <td>14.409427</td>\n",
              "      <td>11.330301</td>\n",
              "      <td>19.134697</td>\n",
              "      <td>67.795100</td>\n",
              "      <td>38.595125</td>\n",
              "      <td>12.477227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699</th>\n",
              "      <td>2699</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "      <td>0.600671</td>\n",
              "      <td>0.611160</td>\n",
              "      <td>23.797738</td>\n",
              "      <td>19.519105</td>\n",
              "      <td>90.957424</td>\n",
              "      <td>6.800289</td>\n",
              "      <td>33.196095</td>\n",
              "      <td>49.620924</td>\n",
              "      <td>14.609582</td>\n",
              "      <td>30.420998</td>\n",
              "      <td>7.687974</td>\n",
              "      <td>1.992580</td>\n",
              "      <td>14.409427</td>\n",
              "      <td>11.330301</td>\n",
              "      <td>19.134697</td>\n",
              "      <td>67.795100</td>\n",
              "      <td>38.595125</td>\n",
              "      <td>12.477227</td>\n",
              "      <td>0.560519</td>\n",
              "      <td>0.571126</td>\n",
              "      <td>23.467835</td>\n",
              "      <td>19.174193</td>\n",
              "      <td>91.110463</td>\n",
              "      <td>6.931543</td>\n",
              "      <td>33.096657</td>\n",
              "      <td>49.510599</td>\n",
              "      <td>14.464053</td>\n",
              "      <td>30.469791</td>\n",
              "      <td>7.692942</td>\n",
              "      <td>1.966064</td>\n",
              "      <td>14.616400</td>\n",
              "      <td>11.522773</td>\n",
              "      <td>19.295834</td>\n",
              "      <td>68.284078</td>\n",
              "      <td>38.453820</td>\n",
              "      <td>11.811719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2700 rows  95 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca81e681-3837-4594-91ac-15da62175446')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca81e681-3837-4594-91ac-15da62175446 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca81e681-3837-4594-91ac-15da62175446');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd \n",
        "pd.set_option('display.max_columns', None) # to show all coumns below\n",
        "df = pd.read_csv('covid.train.csv'); df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YF4MckYUDV"
      },
      "source": [
        "As shown above, the training set has 2700 samples and 95 columns, which include:\n",
        "- 1 column of the IDs\n",
        "- 40 columns showing the states encoded to one-hot vectors\n",
        "- 18 columns of Day 1 features\n",
        "- 18 columns of Day 2 features\n",
        "- 18 columns of Day 3 features\n",
        "\n",
        "More specifically, the 18 features for each day include the follows:\n",
        "- 4 columns of COVID-like illness, including\n",
        "  - `ili`: Percentage of people having influenza-like illness\n",
        "  - `cli`: Percentage of people having COVID-like illness\n",
        "  - `hh_cmnty_cli`: Percentage of people reporintg illness in their local community, including their household\n",
        "  - `noww_cmnty_cli`: Percentage of people reporting illness in their local community, not including their household \n",
        "- 8 columns of behavior indicators, including `wearing_mask`, `travel_outside_state`, `work_outside_home`, `shop`, `restaurant`, `spent_time`, `large_event`, and `public_transit`. Most names of indicators are self-explanatory, except for `spent_time`, which is the percentage of respondents who spent time indoors with someone who isn't currently staying with them in the past 24 hours. \n",
        "- 5 columns of mental health indicators, including `anxious`, `depressed`, `felt_isolated`, `worried_become_ill`, and `worried_finances`.\n",
        "- 1 column showing the percentage of people who tested positive\n",
        "\n",
        "All indicators above are expressed in percentages. For more details about how the data was collected and how the indicators were designed, please visit [this site](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/fb-survey.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9Ftke3YoHe"
      },
      "source": [
        "As for the test set, there are 893 samples and 94 columns, with the only missing column compared with the training set being the test positive rate of Day 3 that we are going to predict using any of the 93 possible indicators as the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27jKiP6rYoJ5"
      },
      "source": [
        "## **2. Building our first neural network without feature selection**\n",
        "### **2-1. Setting things up**\n",
        "First, we import the following packages, define a function for getting the device to be used, set the random seed and specify settings for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DIhcg6udZ4PC"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import rc\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    # set a random seed for reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "rc('font', **{\n",
        "    'family': 'sans-serif',\n",
        "    'sans-serif': ['DejaVu Sans'],\n",
        "    'size': 10\n",
        "})\n",
        "# Set the font used for MathJax - more on this later\n",
        "rc('mathtext', **{'default': 'regular'})\n",
        "plt.rc('font', family='serif') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "reoowFb3RPR9"
      },
      "outputs": [],
      "source": [
        "def change_color(ax, color):\n",
        "    # For better presentation in the webpage\n",
        "    ax.tick_params(color=color, labelcolor=color)\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_edgecolor(color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w508z2QwalsY"
      },
      "source": [
        "### **2-2. Data preprocessing**\n",
        "First of all, we define `COVID19Dataset`, which reads in `.csv` files, extracts features, splits `covid.train.csv` into the training/validation sets (with a ratio of 9:1) and normalizes features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QPa7FA2cafMI"
      },
      "outputs": [],
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self, path, mode='train', feats=list(range(93))):\n",
        "        \"\"\"\n",
        "        Prepares a dataset as specified.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            The path of the training dataset or the test set. \n",
        "        mode : str\n",
        "            How the dataset should be prepared. Available options include \"train\", \n",
        "            \"dev\", and \"test\".\n",
        "        feats: \n",
        "            The list of feature indices to consider.\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "        # Step 1: Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float) # note that the ID has been left out\n",
        "        \n",
        "        # Step 2: Prepare the dataset\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Step 3: Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) / self.data[:, 40:].std(dim=0, keepdim=True)\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print(f'Finished reading the {mode} set of COVID19 Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q9aZm3Scv0V"
      },
      "source": [
        "Also, we need data loaders to load in the datasets as needed.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cmGk9G-TcvWu"
      },
      "outputs": [],
      "source": [
        "def prep_dataloader(path, mode, batch_size, feats):\n",
        "    dataset = COVID19Dataset(path, mode=mode, feats=feats)\n",
        "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, pin_memory=True)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrfOFYLdL_Z"
      },
      "source": [
        "Then, we define a general neural network by defining the class `NeuralNet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WckvU-n8dLNL"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim, n_relu=64, n_hidden_layers=1):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.nn = nn.Sequential()\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.nn.add_module(f'Linear {i}', torch.nn.Linear(input_dim, n_relu))\n",
        "            self.nn.add_module(f'Activation {i}', torch.nn.ReLU())\n",
        "            input_dim = n_relu\n",
        "        self.nn.add_module('Output', torch.nn.Linear(n_relu, 1))\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nn(x).squeeze(1)\n",
        "        return x\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        return self.criterion(pred, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzyKk9hSe53P"
      },
      "source": [
        "Also, here we have funcitons for training, validating, and testing the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AOUWGGRNfATd"
      },
      "outputs": [],
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print(f'Saving model (epoch = {epoch + 1}, validation loss = {min_mse:.4f}')\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T0HskN4JfJ3S"
      },
      "outputs": [],
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "43InwkygfNU-"
      },
      "outputs": [],
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY00oirGfc9J"
      },
      "source": [
        "Then, we define the function `nn_workflow` to combine all three functions above, as we are going to test out a lot of different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LweZV5sGfjCn"
      },
      "outputs": [],
      "source": [
        "def nn_workflow(config, tr_path='covid.train.csv', tt_path='covid.test.csv'):\n",
        "    device = get_device()                    # get the current available device ('cpu' or 'cuda')\n",
        "    os.makedirs('models', exist_ok=True)     # The trained model will be saved to ./models/\n",
        "\n",
        "    tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], feats=config['feats'])\n",
        "    dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], feats=config['feats'])\n",
        "    tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], feats=config['feats'])\n",
        "\n",
        "    model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "\n",
        "    model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)    # start training the model!\n",
        "\n",
        "    return {'tr_set': tr_set, 'dv_set': dv_set, 'tt_set': tt_set}, model, model_loss_record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pzx291htlu"
      },
      "source": [
        "Now, let's build and train our first neural network, which will use all the possible 93 indicators as features (i.e. no feature selection at all). Note that the hyper-parameters defined in `config` are different from the ones used in the orginal notebook written by Chang as this set of parameters work better with the neural networks we are going to build later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD83G5LBg4Xg",
        "outputId": "3ed1d100-704b-4402-a1cd-75808ce0cbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 93)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 93)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 93)\n",
            "Saving model (epoch = 1, validation loss = 277.5056\n",
            "Saving model (epoch = 2, validation loss = 227.9606\n",
            "Saving model (epoch = 3, validation loss = 165.7882\n",
            "Saving model (epoch = 4, validation loss = 103.9075\n",
            "Saving model (epoch = 5, validation loss = 60.1904\n",
            "Saving model (epoch = 6, validation loss = 39.3025\n",
            "Saving model (epoch = 7, validation loss = 32.0575\n",
            "Saving model (epoch = 8, validation loss = 29.1716\n",
            "Saving model (epoch = 9, validation loss = 27.1532\n",
            "Saving model (epoch = 10, validation loss = 25.3724\n",
            "Saving model (epoch = 11, validation loss = 23.7637\n",
            "Saving model (epoch = 12, validation loss = 22.2342\n",
            "Saving model (epoch = 13, validation loss = 20.6818\n",
            "Saving model (epoch = 14, validation loss = 19.1392\n",
            "Saving model (epoch = 15, validation loss = 17.5353\n",
            "Saving model (epoch = 16, validation loss = 15.9335\n",
            "Saving model (epoch = 17, validation loss = 14.3596\n",
            "Saving model (epoch = 18, validation loss = 12.7888\n",
            "Saving model (epoch = 19, validation loss = 11.3467\n",
            "Saving model (epoch = 20, validation loss = 9.9713\n",
            "Saving model (epoch = 21, validation loss = 8.7542\n",
            "Saving model (epoch = 22, validation loss = 7.7740\n",
            "Saving model (epoch = 23, validation loss = 6.9172\n",
            "Saving model (epoch = 24, validation loss = 6.2011\n",
            "Saving model (epoch = 25, validation loss = 5.5877\n",
            "Saving model (epoch = 26, validation loss = 5.1050\n",
            "Saving model (epoch = 27, validation loss = 4.7114\n",
            "Saving model (epoch = 28, validation loss = 4.3157\n",
            "Saving model (epoch = 29, validation loss = 4.0372\n",
            "Saving model (epoch = 30, validation loss = 3.7689\n",
            "Saving model (epoch = 31, validation loss = 3.5747\n",
            "Saving model (epoch = 32, validation loss = 3.3544\n",
            "Saving model (epoch = 33, validation loss = 3.1893\n",
            "Saving model (epoch = 34, validation loss = 3.0904\n",
            "Saving model (epoch = 35, validation loss = 2.9067\n",
            "Saving model (epoch = 36, validation loss = 2.8057\n",
            "Saving model (epoch = 37, validation loss = 2.6935\n",
            "Saving model (epoch = 38, validation loss = 2.6203\n",
            "Saving model (epoch = 39, validation loss = 2.5168\n",
            "Saving model (epoch = 40, validation loss = 2.4697\n",
            "Saving model (epoch = 41, validation loss = 2.3777\n",
            "Saving model (epoch = 42, validation loss = 2.3252\n",
            "Saving model (epoch = 43, validation loss = 2.2825\n",
            "Saving model (epoch = 44, validation loss = 2.2282\n",
            "Saving model (epoch = 45, validation loss = 2.1590\n",
            "Saving model (epoch = 46, validation loss = 2.1153\n",
            "Saving model (epoch = 47, validation loss = 2.0583\n",
            "Saving model (epoch = 48, validation loss = 2.0425\n",
            "Saving model (epoch = 49, validation loss = 2.0039\n",
            "Saving model (epoch = 50, validation loss = 2.0028\n",
            "Saving model (epoch = 51, validation loss = 1.9189\n",
            "Saving model (epoch = 52, validation loss = 1.9046\n",
            "Saving model (epoch = 53, validation loss = 1.8833\n",
            "Saving model (epoch = 54, validation loss = 1.8517\n",
            "Saving model (epoch = 55, validation loss = 1.8342\n",
            "Saving model (epoch = 56, validation loss = 1.7836\n",
            "Saving model (epoch = 58, validation loss = 1.7182\n",
            "Saving model (epoch = 59, validation loss = 1.6872\n",
            "Saving model (epoch = 61, validation loss = 1.6851\n",
            "Saving model (epoch = 62, validation loss = 1.6137\n",
            "Saving model (epoch = 63, validation loss = 1.6095\n",
            "Saving model (epoch = 65, validation loss = 1.6090\n",
            "Saving model (epoch = 66, validation loss = 1.5474\n",
            "Saving model (epoch = 67, validation loss = 1.5186\n",
            "Saving model (epoch = 69, validation loss = 1.4957\n",
            "Saving model (epoch = 70, validation loss = 1.4774\n",
            "Saving model (epoch = 71, validation loss = 1.4696\n",
            "Saving model (epoch = 73, validation loss = 1.4355\n",
            "Saving model (epoch = 75, validation loss = 1.4117\n",
            "Saving model (epoch = 76, validation loss = 1.4071\n",
            "Saving model (epoch = 78, validation loss = 1.4013\n",
            "Saving model (epoch = 79, validation loss = 1.3550\n",
            "Saving model (epoch = 82, validation loss = 1.3404\n",
            "Saving model (epoch = 83, validation loss = 1.3136\n",
            "Saving model (epoch = 84, validation loss = 1.3001\n",
            "Saving model (epoch = 85, validation loss = 1.2911\n",
            "Saving model (epoch = 86, validation loss = 1.2795\n",
            "Saving model (epoch = 88, validation loss = 1.2674\n",
            "Saving model (epoch = 91, validation loss = 1.2495\n",
            "Saving model (epoch = 94, validation loss = 1.2461\n",
            "Saving model (epoch = 95, validation loss = 1.2202\n",
            "Saving model (epoch = 96, validation loss = 1.2024\n",
            "Saving model (epoch = 98, validation loss = 1.1953\n",
            "Saving model (epoch = 100, validation loss = 1.1916\n",
            "Saving model (epoch = 101, validation loss = 1.1853\n",
            "Saving model (epoch = 102, validation loss = 1.1514\n",
            "Saving model (epoch = 104, validation loss = 1.1486\n",
            "Saving model (epoch = 105, validation loss = 1.1404\n",
            "Saving model (epoch = 107, validation loss = 1.1319\n",
            "Saving model (epoch = 108, validation loss = 1.1280\n",
            "Saving model (epoch = 110, validation loss = 1.1074\n",
            "Saving model (epoch = 112, validation loss = 1.0934\n",
            "Saving model (epoch = 114, validation loss = 1.0889\n",
            "Saving model (epoch = 116, validation loss = 1.0662\n",
            "Saving model (epoch = 120, validation loss = 1.0508\n",
            "Saving model (epoch = 122, validation loss = 1.0389\n",
            "Saving model (epoch = 125, validation loss = 1.0355\n",
            "Saving model (epoch = 126, validation loss = 1.0329\n",
            "Saving model (epoch = 128, validation loss = 1.0257\n",
            "Saving model (epoch = 133, validation loss = 1.0228\n",
            "Saving model (epoch = 134, validation loss = 1.0153\n",
            "Saving model (epoch = 135, validation loss = 1.0127\n",
            "Saving model (epoch = 136, validation loss = 1.0007\n",
            "Saving model (epoch = 137, validation loss = 0.9913\n",
            "Saving model (epoch = 139, validation loss = 0.9863\n",
            "Saving model (epoch = 143, validation loss = 0.9817\n",
            "Saving model (epoch = 144, validation loss = 0.9794\n",
            "Saving model (epoch = 145, validation loss = 0.9548\n",
            "Saving model (epoch = 148, validation loss = 0.9543\n",
            "Saving model (epoch = 149, validation loss = 0.9517\n",
            "Saving model (epoch = 150, validation loss = 0.9391\n",
            "Saving model (epoch = 153, validation loss = 0.9361\n",
            "Saving model (epoch = 157, validation loss = 0.9098\n",
            "Saving model (epoch = 169, validation loss = 0.8989\n",
            "Saving model (epoch = 170, validation loss = 0.8953\n",
            "Saving model (epoch = 175, validation loss = 0.8875\n",
            "Saving model (epoch = 177, validation loss = 0.8704\n",
            "Saving model (epoch = 184, validation loss = 0.8673\n",
            "Saving model (epoch = 192, validation loss = 0.8495\n",
            "Saving model (epoch = 208, validation loss = 0.8353\n",
            "Saving model (epoch = 216, validation loss = 0.8295\n",
            "Saving model (epoch = 225, validation loss = 0.8163\n",
            "Saving model (epoch = 238, validation loss = 0.8145\n",
            "Saving model (epoch = 240, validation loss = 0.8058\n",
            "Saving model (epoch = 263, validation loss = 0.8050\n",
            "Saving model (epoch = 277, validation loss = 0.7898\n",
            "Saving model (epoch = 328, validation loss = 0.7832\n",
            "Saving model (epoch = 403, validation loss = 0.7811\n",
            "Saving model (epoch = 424, validation loss = 0.7727\n",
            "Saving model (epoch = 550, validation loss = 0.7726\n",
            "Saving model (epoch = 558, validation loss = 0.7653\n",
            "Saving model (epoch = 627, validation loss = 0.7599\n",
            "Finished training after 1128 epochs\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'feats': list(range(93)),        # consider all features (i.e. no feature selection)    \n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the defaults\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Q9EADzicUQ"
      },
      "source": [
        "As shown above, the model was trained for 1128 epochs and the minimum loss validation loss was 0.7604. Below we show the time seris of the losses and the parity plot between our predictions and the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "BLb8V8GRRDfn",
        "outputId": "548faa56-7659-4cfb-e8a9-28f1bea94954"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "color = 'black'\n",
        "\n",
        "# Prepare data for the left panel\n",
        "total_steps = len(loss_record['train'])\n",
        "x_1 = range(total_steps)\n",
        "x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "\n",
        "# Plot the left panel: The training/validation loss as a function of time\n",
        "ax0 = fig.add_subplot(121)\n",
        "plt.plot(x_1, loss_record['train'], c='C0', label='Training loss')\n",
        "plt.plot(x_2, loss_record['dev'], c='C1', label='Validation loss')\n",
        "plt.ylim(0.0, 5.)\n",
        "plt.xlabel('Number of training steps', color=color, fontsize=11)\n",
        "plt.ylabel('MSE validation loss', color=color, fontsize=11)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "change_color(ax0, color)\n",
        "\n",
        "# Prepare data for the right panel\n",
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "model.eval()\n",
        "preds, targets = [], []\n",
        "for x, y in datasets['dv_set']:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "        preds.append(pred.detach().cpu())\n",
        "        targets.append(y.detach().cpu())\n",
        "preds = torch.cat(preds, dim=0).numpy()\n",
        "targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "# Plot the right figure: The parity plot between our predictions and the labels\n",
        "ax1 = fig.add_subplot(122)\n",
        "plt.scatter(targets, preds, c='#ed9f5c', alpha=0.5)\n",
        "plt.plot([-0.2, 35], [-0.2, 35], color='C0')\n",
        "plt.xlim(-0.2, 35)\n",
        "plt.ylim(-0.2, 35)\n",
        "plt.xlabel('Ground truth value', color=color, fontsize=11)\n",
        "plt.ylabel('Predicted value', color=color, fontsize=11)\n",
        "plt.grid()\n",
        "\n",
        "change_color(ax1, color)\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_no_selection.png', dpi=600, transparent=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEOvpnaixWR"
      },
      "source": [
        "The most importantly, we need to save predictions and upload it to Kaggle to assess the model by calculating the test loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oLLQV2YelE0X"
      },
      "outputs": [],
      "source": [
        "def save_pred(tt_set, model, pred_path):\n",
        "    device = get_device()\n",
        "    preds = test(tt_set, model, device)\n",
        "\n",
        "    with open(pred_path, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dcvkUJNkmD0Q"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_0.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYSFU8g6mZao"
      },
      "source": [
        "As can be examined on Kaggle, the predictions in `pred_0.csv` have a test loss of 2.0972, which is much higher than the minimum validation loss of 0.7604. In fact, as examined in my [another notebook](https://github.com/wehs7661/deep_learning_projects/blob/master/covid_regression/DNN_test_positive/covid_regression_test_positive.ipynb), this model actually had an issue of overfitting as consider all the 93 features. Therefore, let's try out different feature selection methods to see if the test loss can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xMYp2YQZeHv"
      },
      "source": [
        "## **3. Filter-based feature selection**\n",
        "Filter-based selection methods filter out features based on a user-selected metric. For example, one could examine the absolute value of the Pearson's correlation coefficient ($r$) between the target and each of the features and keep the top $k$ features that have the highest value of $r$. In `scikit-learn`, the following metrics are available. (Check the [documentation](https://scikit-learn.org/stable/modules/feature_selection.html) of scikit-learn for more informaiton.)\n",
        "- For regression\n",
        "  - Pearson correlation coefficient ([`r_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression))\n",
        "  - F-statistics ([`f_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression))\n",
        "  - Mutual information for a continuous target variable ([`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression))\n",
        "- For classification\n",
        "  - $\\chi^2$ value ([chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2))\n",
        "  - ANOVA F-value ([`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2))\n",
        "  - Mutual information for a discrete target variable ([`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif))\n",
        "\n",
        "\n",
        "Notably, some people also use principal component analysis (PCA) to estimate the contribution of each of the transformed features (i.e. principal components, which are linear combinations of the original features) to the variance of the output variable, and leave out the ones that contribute the least. Here, we will just focus on non-transformed features to make different methods more directly comparable. Specifically, below we try all the three metrics for regression in our filter-based feature selection. Note that we use `SelectKBest` to select the top $k$ features with the highest scores, but one could also select the highest scoring percentage of features with `SelectPercentile` or other tools like `SelectFpr`, `SelectFdr`, `SelectFwe`, or `GenericUnivariateSelect`. Here, we will only use `SelectKBest` for now. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MNcSq0fWWPCQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import r_regression             # metric 1 \n",
        "from sklearn.feature_selection import f_regression             # metric 2\n",
        "from sklearn.feature_selection import mutual_info_regression   # metric 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notably, there is no need to standardize the input data since these metrics are all indepedent of scaling/translation of data."
      ],
      "metadata": {
        "id": "qI2pjK89flkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oqmqC9EPZG8I"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('covid.train.csv')\n",
        "x = data[data.columns[1:94]]\n",
        "y = data[data.columns[94]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "prplGftBeabc"
      },
      "outputs": [],
      "source": [
        "def filter_features(x, y, score_func):\n",
        "    best_feats = SelectKBest(score_func=score_func, k=5)  # the value of k doesn't matter since we will decide how many to pick later\n",
        "    fit = best_feats.fit(x, y)\n",
        "\n",
        "    result = pd.concat([pd.DataFrame(data.columns[1:94]), pd.DataFrame(fit.scores_)], axis=1)\n",
        "    result.columns = ['Features', 'Score']\n",
        "    print(result.nlargest(15, 'Score'))  # print the best 15 features\n",
        "\n",
        "    # Below we generate a list of indices corresponding to all features ranked from the highest score to the lowest\n",
        "    idx_list = [result.nlargest(len(result), 'Score').iloc[i].name for i in range(len(result))] \n",
        "\n",
        "    return idx_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awJdC4PcXyPA"
      },
      "source": [
        "First, we try to use the Pearson correlation coefficient as the metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W3hoyaSgXU5",
        "outputId": "9c3327c2-d065-488f-c5f0-c7481c9c99c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Features     Score\n",
            "75   tested_positive.1  0.991012\n",
            "57     tested_positive  0.981165\n",
            "42        hh_cmnty_cli  0.879724\n",
            "60      hh_cmnty_cli.1  0.879438\n",
            "78      hh_cmnty_cli.2  0.878218\n",
            "43      nohh_cmnty_cli  0.869938\n",
            "61    nohh_cmnty_cli.1  0.869278\n",
            "79    nohh_cmnty_cli.2  0.867535\n",
            "40                 cli  0.838504\n",
            "58               cli.1  0.838224\n",
            "76               cli.2  0.835751\n",
            "41                 ili  0.830527\n",
            "59               ili.1  0.829200\n",
            "77               ili.2  0.826075\n",
            "92  worried_finances.2  0.485843\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', 15) # to show all rows below\n",
        "idx_list_0 = filter_features(x, y, r_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAhASFxMnWo0"
      },
      "source": [
        "Then, we use F-statistics as the metric by using `f_regression`, which is derived from `r_regression` and will rank features in the same order if all the features are positively correlated with the target. Also note that the scores of `f_regression` should be non-negative, while the scores of `r_regression` lie in [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsMciZI7gl3z",
        "outputId": "4a4bbcbb-66c4-4d2a-e721-404763577a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Features          Score\n",
            "75   tested_positive.1  148069.658278\n",
            "57     tested_positive   69603.872591\n",
            "42        hh_cmnty_cli    9235.492094\n",
            "60      hh_cmnty_cli.1    9209.019558\n",
            "78      hh_cmnty_cli.2    9097.375172\n",
            "43      nohh_cmnty_cli    8395.421300\n",
            "61    nohh_cmnty_cli.1    8343.255927\n",
            "79    nohh_cmnty_cli.2    8208.176435\n",
            "40                 cli    6388.906849\n",
            "58               cli.1    6374.548000\n",
            "76               cli.2    6250.008702\n",
            "41                 ili    5998.922880\n",
            "59               ili.1    5937.588576\n",
            "77               ili.2    5796.947672\n",
            "92  worried_finances.2     833.613191\n"
          ]
        }
      ],
      "source": [
        "idx_list_1 = filter_features(x, y, f_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rlwLPiX67-"
      },
      "source": [
        "Finally, we try the mutual information, which should also lead to non-negative scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wgGhlK9grbG",
        "outputId": "0500aaf1-c4ee-4a1d-ff3d-97c1cff8c676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Features     Score\n",
            "75  tested_positive.1  2.047046\n",
            "57    tested_positive  1.664485\n",
            "78     hh_cmnty_cli.2  0.980131\n",
            "42       hh_cmnty_cli  0.974385\n",
            "60     hh_cmnty_cli.1  0.971687\n",
            "43     nohh_cmnty_cli  0.967492\n",
            "79   nohh_cmnty_cli.2  0.960283\n",
            "61   nohh_cmnty_cli.1  0.956382\n",
            "58              cli.1  0.767732\n",
            "77              ili.2  0.763468\n",
            "40                cli  0.755316\n",
            "59              ili.1  0.743957\n",
            "76              cli.2  0.740273\n",
            "41                ili  0.732601\n",
            "84       restaurant.2  0.551297\n"
          ]
        }
      ],
      "source": [
        "idx_list_2 = filter_features(x, y, mutual_info_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqWvlxBQBepf"
      },
      "source": [
        "As can be examined below, different metrics lead to different orders of the feature ranking, but the first 14 features selected by different metrics are all the same one. Therefore, we will use these 14 features for later training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nE65UAli_G",
        "outputId": "233f1c11-687b-4f00-9274-e990fcf73df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92]\n",
            "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92]\n",
            "[75, 57, 78, 42, 60, 43, 79, 61, 58, 77, 40, 59, 76, 41, 84]\n"
          ]
        }
      ],
      "source": [
        "print(idx_list_0[:15])\n",
        "print(idx_list_1[:15])\n",
        "print(idx_list_2[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B9xjlR6EL5_"
      },
      "source": [
        "Below we train a new neural network with only 14 features, using exactly the same hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkaDhTunqyo8",
        "outputId": "f1fa9016-b3da-4eb4-9327-81a2aae3e866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 14)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 14)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 14)\n",
            "Saving model (epoch = 1, validation loss = 8.8931\n",
            "Saving model (epoch = 2, validation loss = 8.7802\n",
            "Saving model (epoch = 3, validation loss = 5.7027\n",
            "Saving model (epoch = 4, validation loss = 3.9171\n",
            "Saving model (epoch = 5, validation loss = 2.9314\n",
            "Saving model (epoch = 6, validation loss = 2.1810\n",
            "Saving model (epoch = 7, validation loss = 1.6694\n",
            "Saving model (epoch = 8, validation loss = 1.4024\n",
            "Saving model (epoch = 9, validation loss = 1.2429\n",
            "Saving model (epoch = 10, validation loss = 1.1950\n",
            "Saving model (epoch = 11, validation loss = 1.1886\n",
            "Saving model (epoch = 12, validation loss = 1.1400\n",
            "Saving model (epoch = 13, validation loss = 1.1336\n",
            "Saving model (epoch = 15, validation loss = 1.1213\n",
            "Saving model (epoch = 18, validation loss = 1.1101\n",
            "Saving model (epoch = 20, validation loss = 1.0970\n",
            "Saving model (epoch = 21, validation loss = 1.0921\n",
            "Saving model (epoch = 22, validation loss = 1.0915\n",
            "Saving model (epoch = 24, validation loss = 1.0901\n",
            "Saving model (epoch = 26, validation loss = 1.0735\n",
            "Saving model (epoch = 29, validation loss = 1.0610\n",
            "Saving model (epoch = 32, validation loss = 1.0526\n",
            "Saving model (epoch = 36, validation loss = 1.0418\n",
            "Saving model (epoch = 39, validation loss = 1.0261\n",
            "Saving model (epoch = 42, validation loss = 1.0222\n",
            "Saving model (epoch = 43, validation loss = 1.0207\n",
            "Saving model (epoch = 44, validation loss = 1.0147\n",
            "Saving model (epoch = 46, validation loss = 1.0084\n",
            "Saving model (epoch = 47, validation loss = 1.0007\n",
            "Saving model (epoch = 52, validation loss = 0.9845\n",
            "Saving model (epoch = 56, validation loss = 0.9832\n",
            "Saving model (epoch = 57, validation loss = 0.9713\n",
            "Saving model (epoch = 62, validation loss = 0.9551\n",
            "Saving model (epoch = 64, validation loss = 0.9494\n",
            "Saving model (epoch = 69, validation loss = 0.9358\n",
            "Saving model (epoch = 70, validation loss = 0.9334\n",
            "Saving model (epoch = 75, validation loss = 0.9307\n",
            "Saving model (epoch = 76, validation loss = 0.9162\n",
            "Saving model (epoch = 79, validation loss = 0.9088\n",
            "Saving model (epoch = 80, validation loss = 0.9071\n",
            "Saving model (epoch = 83, validation loss = 0.9019\n",
            "Saving model (epoch = 89, validation loss = 0.8961\n",
            "Saving model (epoch = 90, validation loss = 0.8824\n",
            "Saving model (epoch = 92, validation loss = 0.8769\n",
            "Saving model (epoch = 98, validation loss = 0.8678\n",
            "Saving model (epoch = 101, validation loss = 0.8641\n",
            "Saving model (epoch = 102, validation loss = 0.8570\n",
            "Saving model (epoch = 107, validation loss = 0.8473\n",
            "Saving model (epoch = 112, validation loss = 0.8420\n",
            "Saving model (epoch = 116, validation loss = 0.8390\n",
            "Saving model (epoch = 119, validation loss = 0.8325\n",
            "Saving model (epoch = 122, validation loss = 0.8265\n",
            "Saving model (epoch = 128, validation loss = 0.8206\n",
            "Saving model (epoch = 131, validation loss = 0.8186\n",
            "Saving model (epoch = 133, validation loss = 0.8152\n",
            "Saving model (epoch = 138, validation loss = 0.8139\n",
            "Saving model (epoch = 141, validation loss = 0.8102\n",
            "Saving model (epoch = 146, validation loss = 0.8056\n",
            "Saving model (epoch = 155, validation loss = 0.8003\n",
            "Saving model (epoch = 160, validation loss = 0.7990\n",
            "Saving model (epoch = 167, validation loss = 0.7966\n",
            "Saving model (epoch = 168, validation loss = 0.7946\n",
            "Saving model (epoch = 186, validation loss = 0.7908\n",
            "Saving model (epoch = 188, validation loss = 0.7888\n",
            "Saving model (epoch = 192, validation loss = 0.7886\n",
            "Saving model (epoch = 196, validation loss = 0.7884\n",
            "Saving model (epoch = 203, validation loss = 0.7861\n",
            "Saving model (epoch = 211, validation loss = 0.7860\n",
            "Saving model (epoch = 215, validation loss = 0.7848\n",
            "Saving model (epoch = 217, validation loss = 0.7840\n",
            "Saving model (epoch = 226, validation loss = 0.7833\n",
            "Saving model (epoch = 228, validation loss = 0.7822\n",
            "Saving model (epoch = 237, validation loss = 0.7817\n",
            "Saving model (epoch = 244, validation loss = 0.7811\n",
            "Saving model (epoch = 323, validation loss = 0.7805\n",
            "Saving model (epoch = 376, validation loss = 0.7797\n",
            "Saving model (epoch = 440, validation loss = 0.7795\n",
            "Saving model (epoch = 496, validation loss = 0.7794\n",
            "Saving model (epoch = 742, validation loss = 0.7793\n",
            "Finished training after 1243 epochs\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'feats': idx_list_0[:14],        # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVZm5xEcSsC0"
      },
      "source": [
        "Again, we save our predictions and upload them to Kaggle and calculate the test loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I-B5diHvrHwC"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YxVeesSxC7"
      },
      "source": [
        "As can be examined on Kaggle, the test loss of our model with feature selection goes down to 0.9168, which is much lower than the original test loss. Notably, the validation loss of the second model is slightly higher than the one in the first model, but the test loss is much lower. This indicates that the original model without feature selection indeed had an issue of overfitting such that the model trained on the validation set failed to generalize to unseen examples in the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C_oAw54ZeKI"
      },
      "source": [
        "## **4. Wrapper-based feature selection**\n",
        "In contrast to filter-based methods, which do not incorporate a specific machine learning algorithm, wrapper-based methods create models with different subset of input features and evaluate them on a specific machine learning method to find the optimal features that maximize the model performance. Common examples of wrapper-based feature selection methods include recursive feature elimination (RFE), sequential feature selection (SFS), and permutation importance. Generally, wrapper-based methods are much slower than the filter-based methods in terms of time complexity because they iteratively fit the datasets to different models.\n",
        "\n",
        "### **4-1. Recursive feature elimination (RFE)**\n",
        "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_` attribute or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('covid.train.csv')\n",
        "x = data[data.columns[1:94]]\n",
        "y = data[data.columns[94]]"
      ],
      "metadata": {
        "id": "Qsc7a5m1jH1O"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will use the negative root-mean-squared error as the scording metric (so the higher score the better) in the 5-fold cross validation for determining the number of features to be selected. The input data need to be standardized since MSE is sensitive to the scale of the data. Notably, here we will not consider the categorial features (i.e. the one-hot vectors) but just numerical features to save time. After all, the one-hot vectors do seem less important in our previous analysis. "
      ],
      "metadata": {
        "id": "5fYtstaIjrCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[x.columns[40:]] = (x[x.columns[40:]] - x[x.columns[40:]].mean(axis=0)) / (x[x.columns[40:]].std(axis=0))\n",
        "y = (y - y.mean(axis=0)) / (y.std(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP1Qc4fY0BR5",
        "outputId": "4c611f58-7ae5-403e-87dc-a3974a7c2d44"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "rfe_selector = RFECV(estimator=LinearRegression(), scoring='neg_mean_squared_error')\n",
        "rfe_selector.fit(x[x.columns[40:]], y)\n",
        "rfe_support = rfe_selector.get_support()\n",
        "rfe_feature = x[x.columns[40:]].iloc[:, rfe_support].columns.tolist()\n",
        "feats_str = '\\n  '.join(rfe_feature)\n",
        "print(f'The following {len(rfe_feature)} features are selected: \\n  {feats_str}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NQeGp1clGww",
        "outputId": "487b6dbb-b2c3-470e-999f-1fb47c6192cb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following 11 features are selected: \n",
            "  cli\n",
            "  hh_cmnty_cli\n",
            "  nohh_cmnty_cli\n",
            "  ili.1\n",
            "  hh_cmnty_cli.1\n",
            "  nohh_cmnty_cli.1\n",
            "  travel_outside_state.1\n",
            "  tested_positive.1\n",
            "  ili.2\n",
            "  hh_cmnty_cli.2\n",
            "  travel_outside_state.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, 11 featues were selected. Below we retrieve their indices in the original set of features. Note that the type of the elements in `rfe_support` is `np.bool_` instead of `bool`."
      ],
      "metadata": {
        "id": "Ytj0ceg1tk9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfe_idx = [40 + i for i in range(53) if rfe_support[i] == np.bool_(True)]\n",
        "rfe_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLxsYqFysvoc",
        "outputId": "46b87c07-d6da-45ee-8086-7882a01e777c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 42, 43, 59, 60, 61, 63, 75, 77, 78, 81]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we train a new model based on these 11 features:"
      ],
      "metadata": {
        "id": "nxasKnNqt7H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': rfe_idx,        # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaIFew-GtNpN",
        "outputId": "195e2cad-4402-4a2b-aeeb-0b852a9f759e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 11)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 11)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 11)\n",
            "Saving model (epoch = 1, validation loss = 75.0152\n",
            "Saving model (epoch = 2, validation loss = 17.5906\n",
            "Saving model (epoch = 3, validation loss = 15.9701\n",
            "Saving model (epoch = 4, validation loss = 14.0017\n",
            "Saving model (epoch = 5, validation loss = 12.2875\n",
            "Saving model (epoch = 6, validation loss = 10.9457\n",
            "Saving model (epoch = 7, validation loss = 9.7390\n",
            "Saving model (epoch = 8, validation loss = 8.6220\n",
            "Saving model (epoch = 9, validation loss = 7.6320\n",
            "Saving model (epoch = 10, validation loss = 6.7921\n",
            "Saving model (epoch = 11, validation loss = 6.1777\n",
            "Saving model (epoch = 12, validation loss = 5.3648\n",
            "Saving model (epoch = 13, validation loss = 4.7591\n",
            "Saving model (epoch = 14, validation loss = 4.2104\n",
            "Saving model (epoch = 15, validation loss = 3.7067\n",
            "Saving model (epoch = 16, validation loss = 3.2420\n",
            "Saving model (epoch = 17, validation loss = 2.8267\n",
            "Saving model (epoch = 18, validation loss = 2.4732\n",
            "Saving model (epoch = 19, validation loss = 2.1516\n",
            "Saving model (epoch = 20, validation loss = 1.9501\n",
            "Saving model (epoch = 21, validation loss = 1.6549\n",
            "Saving model (epoch = 22, validation loss = 1.4711\n",
            "Saving model (epoch = 23, validation loss = 1.3318\n",
            "Saving model (epoch = 24, validation loss = 1.2155\n",
            "Saving model (epoch = 25, validation loss = 1.1421\n",
            "Saving model (epoch = 26, validation loss = 1.0627\n",
            "Saving model (epoch = 27, validation loss = 1.0142\n",
            "Saving model (epoch = 28, validation loss = 0.9779\n",
            "Saving model (epoch = 29, validation loss = 0.9568\n",
            "Saving model (epoch = 30, validation loss = 0.9434\n",
            "Saving model (epoch = 31, validation loss = 0.9372\n",
            "Saving model (epoch = 32, validation loss = 0.9087\n",
            "Saving model (epoch = 35, validation loss = 0.8935\n",
            "Saving model (epoch = 37, validation loss = 0.8880\n",
            "Saving model (epoch = 40, validation loss = 0.8850\n",
            "Saving model (epoch = 44, validation loss = 0.8837\n",
            "Saving model (epoch = 48, validation loss = 0.8831\n",
            "Saving model (epoch = 50, validation loss = 0.8813\n",
            "Saving model (epoch = 53, validation loss = 0.8799\n",
            "Saving model (epoch = 57, validation loss = 0.8766\n",
            "Saving model (epoch = 63, validation loss = 0.8728\n",
            "Saving model (epoch = 64, validation loss = 0.8718\n",
            "Saving model (epoch = 66, validation loss = 0.8704\n",
            "Saving model (epoch = 68, validation loss = 0.8703\n",
            "Saving model (epoch = 70, validation loss = 0.8690\n",
            "Saving model (epoch = 81, validation loss = 0.8656\n",
            "Saving model (epoch = 87, validation loss = 0.8639\n",
            "Saving model (epoch = 89, validation loss = 0.8624\n",
            "Saving model (epoch = 92, validation loss = 0.8615\n",
            "Saving model (epoch = 99, validation loss = 0.8597\n",
            "Saving model (epoch = 103, validation loss = 0.8578\n",
            "Saving model (epoch = 113, validation loss = 0.8536\n",
            "Saving model (epoch = 121, validation loss = 0.8519\n",
            "Saving model (epoch = 127, validation loss = 0.8493\n",
            "Saving model (epoch = 137, validation loss = 0.8474\n",
            "Saving model (epoch = 142, validation loss = 0.8451\n",
            "Saving model (epoch = 143, validation loss = 0.8444\n",
            "Saving model (epoch = 145, validation loss = 0.8433\n",
            "Saving model (epoch = 147, validation loss = 0.8429\n",
            "Saving model (epoch = 148, validation loss = 0.8420\n",
            "Saving model (epoch = 150, validation loss = 0.8416\n",
            "Saving model (epoch = 159, validation loss = 0.8404\n",
            "Saving model (epoch = 166, validation loss = 0.8366\n",
            "Saving model (epoch = 167, validation loss = 0.8363\n",
            "Saving model (epoch = 179, validation loss = 0.8340\n",
            "Saving model (epoch = 185, validation loss = 0.8322\n",
            "Saving model (epoch = 187, validation loss = 0.8310\n",
            "Saving model (epoch = 191, validation loss = 0.8308\n",
            "Saving model (epoch = 193, validation loss = 0.8301\n",
            "Saving model (epoch = 203, validation loss = 0.8275\n",
            "Saving model (epoch = 206, validation loss = 0.8269\n",
            "Saving model (epoch = 216, validation loss = 0.8248\n",
            "Saving model (epoch = 235, validation loss = 0.8226\n",
            "Saving model (epoch = 237, validation loss = 0.8214\n",
            "Saving model (epoch = 246, validation loss = 0.8184\n",
            "Saving model (epoch = 249, validation loss = 0.8175\n",
            "Saving model (epoch = 250, validation loss = 0.8174\n",
            "Saving model (epoch = 275, validation loss = 0.8160\n",
            "Saving model (epoch = 277, validation loss = 0.8135\n",
            "Saving model (epoch = 279, validation loss = 0.8126\n",
            "Saving model (epoch = 281, validation loss = 0.8110\n",
            "Saving model (epoch = 288, validation loss = 0.8106\n",
            "Saving model (epoch = 303, validation loss = 0.8081\n",
            "Saving model (epoch = 308, validation loss = 0.8080\n",
            "Saving model (epoch = 317, validation loss = 0.8057\n",
            "Saving model (epoch = 325, validation loss = 0.8038\n",
            "Saving model (epoch = 340, validation loss = 0.8028\n",
            "Saving model (epoch = 349, validation loss = 0.8009\n",
            "Saving model (epoch = 351, validation loss = 0.8007\n",
            "Saving model (epoch = 373, validation loss = 0.7998\n",
            "Saving model (epoch = 389, validation loss = 0.7983\n",
            "Saving model (epoch = 394, validation loss = 0.7982\n",
            "Saving model (epoch = 395, validation loss = 0.7975\n",
            "Saving model (epoch = 400, validation loss = 0.7964\n",
            "Saving model (epoch = 402, validation loss = 0.7956\n",
            "Saving model (epoch = 413, validation loss = 0.7949\n",
            "Saving model (epoch = 418, validation loss = 0.7948\n",
            "Saving model (epoch = 437, validation loss = 0.7931\n",
            "Saving model (epoch = 452, validation loss = 0.7920\n",
            "Saving model (epoch = 469, validation loss = 0.7917\n",
            "Saving model (epoch = 479, validation loss = 0.7910\n",
            "Saving model (epoch = 487, validation loss = 0.7904\n",
            "Saving model (epoch = 497, validation loss = 0.7902\n",
            "Saving model (epoch = 501, validation loss = 0.7899\n",
            "Saving model (epoch = 517, validation loss = 0.7897\n",
            "Saving model (epoch = 538, validation loss = 0.7896\n",
            "Saving model (epoch = 551, validation loss = 0.7880\n",
            "Saving model (epoch = 564, validation loss = 0.7860\n",
            "Saving model (epoch = 588, validation loss = 0.7856\n",
            "Saving model (epoch = 609, validation loss = 0.7856\n",
            "Saving model (epoch = 616, validation loss = 0.7841\n",
            "Saving model (epoch = 650, validation loss = 0.7837\n",
            "Finished training after 1151 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we save the predictions generated by this model and upload them to Kaggle."
      ],
      "metadata": {
        "id": "uer13oxgw0TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_2.csv')"
      ],
      "metadata": {
        "id": "52yatovki_BA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, the test loss calculated by Kaggle was around 0.92094, which is pretty close to what we got from the model using the filter-based methods. Notably, `RFECV` is also available in `yellowbrick`. `RFECV` in `yellowbrick` basically works the same as `RFECV` in `scikit-learn` but is more convenient to plot the cross-validation score as a funciton of the number of selected features, as shown below."
      ],
      "metadata": {
        "id": "fyk98Qn1xPyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sopcVq3BFe7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "0b03319c-24fc-4328-fa66-70d5c59add5c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f98bdace1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from yellowbrick.model_selection.rfecv import RFECV\n",
        "rfe_selector = RFECV(LinearRegression(), X=x[x.columns[40:]], y=y, cv=5, scoring='neg_mean_squared_error')\n",
        "rfe_selector.fit(x[x.columns[40:]], y)\n",
        "rfe_selector.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4-2. Sequential feature selection (SFS)**\n",
        "To perform an exhaustive search, we will need to evaluate $\\sum_{i}^m C^{m}_{i}$ models, where $m$ is the number of features. In our case, $m=93$ and $\\sum_{i}^m C^{m}_{i}=9.9 \\times 10^{27}$, which is apparently far more than we could afford. (Even if we only consider the 53 numerical features, $\\sum_{i}^m C^{m}_{i}=9.0 \\times 10^{15}$.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1-DreUjGvAZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.math import num_combinations\n",
        "print(np.sum([num_combinations(93, i) for i in range(93)]))\n",
        "print(np.sum([num_combinations(53, i) for i in range(53)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-PgI3LRfzdl",
        "outputId": "f56abf03-de16-4321-91f9-3b47e6fccbf5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9903520314283042199192993791\n",
            "9007199254740991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, sequential feature selection (SFS) methods are generally a good approxmation of the exhaustive search. Being a family of greedy search algorithms, they can effectively reduce the dimension of the search space by making the locally optimal choice at each iteration. \n",
        "\n",
        "There are several common variations of SFS methods, including: \n",
        "- Sequential forward selection (or simply forward selection)\n",
        "- Sequential backward selection (or simply backward elimination)\n",
        "- Sequential floating forward selection\n",
        "- Sequential floating backward selection\n",
        "\n",
        "Both `scikit-learn` and `mlxtend` can perform the first two variations, but only `mlxtend` can perform the floating varitions, so we will use `mlxtend` to apply all the four methods listed above. Below we use the standarized datasets we generated previously. Similarly, we will ignore the categorical features as we did before. For more information about the theory of SFS and how to use `mlxtend` to apply SFS and some additional analysis not presented here, please refer to the [user manual of `mlextend`](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)."
      ],
      "metadata": {
        "id": "MW5MLJEpfy7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4-2-1. Sequential forward selection**"
      ],
      "metadata": {
        "id": "ITkXx5hfvmYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import sys \n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ],
      "metadata": {
        "id": "RPUFmPf29dKm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sfs1 = SFS(LinearRegression(),\n",
        "           k_features='best', \n",
        "           forward=True, \n",
        "           floating=False, \n",
        "           scoring='neg_mean_squared_error', \n",
        "           cv=5)\n",
        "\n",
        "sfs1 = sfs1.fit(x[x.columns[40:]], y)"
      ],
      "metadata": {
        "id": "BEYR895K8Tx9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, we let `SFS` decide the optimal number of features using 5-fold cross validation, using the negative MSE as the scoring function. To inspect the results, we can check the attributes of `sfs1` as follows, which are very informative."
      ],
      "metadata": {
        "id": "MF3pDw4HsAE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the features below are just ranked based on their indices, not importance.\n",
        "print(f'{len(sfs1.k_feature_idx_)} features were chosen. These features include the follows: \\n {sfs1.k_feature_names_}.')\n",
        "print(f'Also, the corresponding indices (in the original feature set) are: \\n{np.array(sfs1.k_feature_idx_) + 40}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jepL_xAossE_",
        "outputId": "f92b7d3a-8797-4f22-8bda-f85c543567ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31 features were chosen. These features include the follows: \n",
            " ('cli', 'ili', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'shop', 'spent_time', 'large_event', 'public_transit', 'anxious', 'depressed', 'tested_positive', 'cli.1', 'ili.1', 'hh_cmnty_cli.1', 'nohh_cmnty_cli.1', 'shop.1', 'spent_time.1', 'large_event.1', 'public_transit.1', 'anxious.1', 'depressed.1', 'tested_positive.1', 'cli.2', 'ili.2', 'hh_cmnty_cli.2', 'nohh_cmnty_cli.2', 'work_outside_home.2', 'shop.2', 'spent_time.2', 'large_event.2', 'public_transit.2').\n",
            "Also, the corresponding indices (in the original feature set) are: \n",
            "[40 41 42 43 47 49 50 51 52 53 57 58 59 60 61 65 67 68 69 70 71 75 76 77\n",
            " 78 79 82 83 85 86 87]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From `sfs1_subsets_` (which is a dictionary with the keys as numbers from 1 to 93), we could know more information about each of the 93 iterations, including the 5 cross validation scores, their average, and the selected features. For example, below we print out the first 10 iterations."
      ],
      "metadata": {
        "id": "LsiuXYI7teGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the feature indices shown here are the indices in the feature subset\n",
        "for i in range(10):\n",
        "    key = list(sfs1.subsets_.keys())[i]\n",
        "    print(f'Iteration {i + 1}: {sfs1.subsets_[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7bkujQ3ujXh",
        "outputId": "f175601b-d2f0-4e37-f990-091b1bc4d9e2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: {'feature_idx': (35,), 'cv_scores': array([-0.01927195, -0.02355226, -0.02212969, -0.01004986, -0.01492524]), 'avg_score': -0.017985798638964386, 'feature_names': ('tested_positive.1',)}\n",
            "Iteration 2: {'feature_idx': (35, 37), 'cv_scores': array([-0.01891412, -0.02281663, -0.02090635, -0.0094241 , -0.01434518]), 'avg_score': -0.017281275444620457, 'feature_names': ('tested_positive.1', 'ili.2')}\n",
            "Iteration 3: {'feature_idx': (19, 35, 37), 'cv_scores': array([-0.01738046, -0.02107815, -0.01960367, -0.00904374, -0.01422079]), 'avg_score': -0.016265363703469315, 'feature_names': ('ili.1', 'tested_positive.1', 'ili.2')}\n",
            "Iteration 4: {'feature_idx': (19, 35, 37, 39), 'cv_scores': array([-0.01724881, -0.02095589, -0.01954365, -0.00906664, -0.01423112]), 'avg_score': -0.016209222151400672, 'feature_names': ('ili.1', 'tested_positive.1', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 5: {'feature_idx': (3, 19, 35, 37, 39), 'cv_scores': array([-0.01769202, -0.02046882, -0.01931546, -0.00904753, -0.01380816]), 'avg_score': -0.016066395834261154, 'feature_names': ('nohh_cmnty_cli', 'ili.1', 'tested_positive.1', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 6: {'feature_idx': (3, 17, 19, 35, 37, 39), 'cv_scores': array([-0.0177064 , -0.02045046, -0.01930475, -0.00906142, -0.0138114 ]), 'avg_score': -0.016066884826907164, 'feature_names': ('nohh_cmnty_cli', 'tested_positive', 'ili.1', 'tested_positive.1', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 7: {'feature_idx': (3, 17, 19, 35, 36, 37, 39), 'cv_scores': array([-0.01771257, -0.02044944, -0.01930486, -0.00906847, -0.013813  ]), 'avg_score': -0.016069667224845145, 'feature_names': ('nohh_cmnty_cli', 'tested_positive', 'ili.1', 'tested_positive.1', 'cli.2', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 8: {'feature_idx': (0, 3, 17, 19, 35, 36, 37, 39), 'cv_scores': array([-0.0177222 , -0.02046015, -0.01929656, -0.00905621, -0.01382424]), 'avg_score': -0.016071872320389473, 'feature_names': ('cli', 'nohh_cmnty_cli', 'tested_positive', 'ili.1', 'tested_positive.1', 'cli.2', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 9: {'feature_idx': (0, 3, 12, 17, 19, 35, 36, 37, 39), 'cv_scores': array([-0.01791478, -0.02039202, -0.01927551, -0.00904365, -0.01374266]), 'avg_score': -0.016073726095918138, 'feature_names': ('cli', 'nohh_cmnty_cli', 'anxious', 'tested_positive', 'ili.1', 'tested_positive.1', 'cli.2', 'ili.2', 'nohh_cmnty_cli.2')}\n",
            "Iteration 10: {'feature_idx': (0, 3, 12, 17, 19, 30, 35, 36, 37, 39), 'cv_scores': array([-0.01787046, -0.02052619, -0.01917907, -0.00898205, -0.01369623]), 'avg_score': -0.016050800740215282, 'feature_names': ('cli', 'nohh_cmnty_cli', 'anxious', 'tested_positive', 'ili.1', 'anxious.1', 'tested_positive.1', 'cli.2', 'ili.2', 'nohh_cmnty_cli.2')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also try to figure out which feature got selected in each round."
      ],
      "metadata": {
        "id": "3JnE6fU4wTZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = []\n",
        "\n",
        "for i in range(len(sfs1.subsets_.keys())):\n",
        "    key = list(sfs1.subsets_.keys())[i]\n",
        "    if i == 0:\n",
        "        idx.append(sfs1.subsets_[key]['feature_idx'][0])\n",
        "        feats_idx_previous = set(sfs1.subsets_[key]['feature_idx'])\n",
        "    else:\n",
        "        feats_idx_current = set(sfs1.subsets_[key]['feature_idx'])\n",
        "        idx.append(list(feats_idx_current - feats_idx_previous)[0])\n",
        "        feats_idx_previous = set(sfs1.subsets_[key]['feature_idx'])\n",
        "\n",
        "print('The feature got selected in each iteration:')\n",
        "for i in range(len(sfs1.subsets_.keys())):\n",
        "    print\n",
        "    print(f'Iteration {i}: {x.columns[idx[i] + 40]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xGf8b7hwHcF",
        "outputId": "aab63fb4-7983-4203-d6bf-aee5023e65f0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The feature got selected in each iteration:\n",
            "Iteration 0: tested_positive.1\n",
            "Iteration 1: ili.2\n",
            "Iteration 2: ili.1\n",
            "Iteration 3: nohh_cmnty_cli.2\n",
            "Iteration 4: nohh_cmnty_cli\n",
            "Iteration 5: tested_positive\n",
            "Iteration 6: cli.2\n",
            "Iteration 7: cli\n",
            "Iteration 8: anxious\n",
            "Iteration 9: anxious.1\n",
            "Iteration 10: nohh_cmnty_cli.1\n",
            "Iteration 11: ili\n",
            "Iteration 12: cli.1\n",
            "Iteration 13: depressed.1\n",
            "Iteration 14: depressed\n",
            "Iteration 15: public_transit.1\n",
            "Iteration 16: public_transit.2\n",
            "Iteration 17: hh_cmnty_cli\n",
            "Iteration 18: hh_cmnty_cli.1\n",
            "Iteration 19: hh_cmnty_cli.2\n",
            "Iteration 20: public_transit\n",
            "Iteration 21: work_outside_home.2\n",
            "Iteration 22: large_event\n",
            "Iteration 23: spent_time.2\n",
            "Iteration 24: spent_time\n",
            "Iteration 25: large_event.2\n",
            "Iteration 26: large_event.1\n",
            "Iteration 27: spent_time.1\n",
            "Iteration 28: shop.1\n",
            "Iteration 29: shop.2\n",
            "Iteration 30: shop\n",
            "Iteration 31: work_outside_home.1\n",
            "Iteration 32: work_outside_home\n",
            "Iteration 33: restaurant.2\n",
            "Iteration 34: restaurant\n",
            "Iteration 35: restaurant.1\n",
            "Iteration 36: anxious.2\n",
            "Iteration 37: worried_finances\n",
            "Iteration 38: worried_finances.1\n",
            "Iteration 39: worried_finances.2\n",
            "Iteration 40: felt_isolated\n",
            "Iteration 41: felt_isolated.1\n",
            "Iteration 42: felt_isolated.2\n",
            "Iteration 43: depressed.2\n",
            "Iteration 44: worried_become_ill\n",
            "Iteration 45: worried_become_ill.1\n",
            "Iteration 46: worried_become_ill.2\n",
            "Iteration 47: travel_outside_state\n",
            "Iteration 48: travel_outside_state.2\n",
            "Iteration 49: travel_outside_state.1\n",
            "Iteration 50: wearing_mask.2\n",
            "Iteration 51: wearing_mask\n",
            "Iteration 52: wearing_mask.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attribute `get_metric_dict` provide a lot of (actually more) information for each iteration as well. Below we tabulate the information."
      ],
      "metadata": {
        "id": "FR8qqYxRytgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_dict_1 = sfs1.get_metric_dict(confidence_interval=0.95)\n",
        "result = pd.DataFrame.from_dict(metric_dict_1).T\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "3gghhLZnzA2u",
        "outputId": "5436380f-0951-4b9c-e58b-1dfdbb19a0ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          feature_idx  \\\n",
              "1                                               (35,)   \n",
              "2                                            (35, 37)   \n",
              "3                                        (19, 35, 37)   \n",
              "4                                    (19, 35, 37, 39)   \n",
              "5                                 (3, 19, 35, 37, 39)   \n",
              "..                                                ...   \n",
              "49  (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
              "50  (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
              "51  (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
              "52  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
              "53  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
              "\n",
              "                                            cv_scores avg_score  \\\n",
              "1   [-0.019271950041066428, -0.023552255915925396,... -0.017986   \n",
              "2   [-0.018914123466337623, -0.022816625697143225,... -0.017281   \n",
              "3   [-0.017380464225647575, -0.021078154742476436,... -0.016265   \n",
              "4   [-0.017248812668502405, -0.020955886309990377,... -0.016209   \n",
              "5   [-0.017692017636190886, -0.02046882453236554, ... -0.016066   \n",
              "..                                                ...       ...   \n",
              "49  [-0.018294340754169355, -0.021483850075892, -0... -0.016524   \n",
              "50  [-0.018250753757065835, -0.021440402455474194,... -0.016527   \n",
              "51  [-0.018402161716038944, -0.022792611560741798,... -0.016825   \n",
              "52  [-0.01841236593978077, -0.02296467978242637, -... -0.016866   \n",
              "53  [-0.018387552805482585, -0.02296031021853254, ...  -0.01687   \n",
              "\n",
              "                                        feature_names  ci_bound   std_dev  \\\n",
              "1                                (tested_positive.1,)  0.006353  0.004943   \n",
              "2                          (tested_positive.1, ili.2)  0.006212  0.004833   \n",
              "3                   (ili.1, tested_positive.1, ili.2)   0.00551  0.004287   \n",
              "4   (ili.1, tested_positive.1, ili.2, nohh_cmnty_c...  0.005443  0.004235   \n",
              "5   (nohh_cmnty_cli, ili.1, tested_positive.1, ili...   0.00536   0.00417   \n",
              "..                                                ...       ...       ...   \n",
              "49  (cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...  0.005805  0.004516   \n",
              "50  (cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...  0.005793  0.004507   \n",
              "51  (cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...  0.006214  0.004835   \n",
              "52  (cli, ili, hh_cmnty_cli, nohh_cmnty_cli, weari...  0.006262  0.004872   \n",
              "53  (cli, ili, hh_cmnty_cli, nohh_cmnty_cli, weari...   0.00625  0.004862   \n",
              "\n",
              "     std_err  \n",
              "1   0.002471  \n",
              "2   0.002417  \n",
              "3   0.002143  \n",
              "4   0.002117  \n",
              "5   0.002085  \n",
              "..       ...  \n",
              "49  0.002258  \n",
              "50  0.002254  \n",
              "51  0.002417  \n",
              "52  0.002436  \n",
              "53  0.002431  \n",
              "\n",
              "[53 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fdac171-6d8f-4985-bbda-76936f8aef59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_idx</th>\n",
              "      <th>cv_scores</th>\n",
              "      <th>avg_score</th>\n",
              "      <th>feature_names</th>\n",
              "      <th>ci_bound</th>\n",
              "      <th>std_dev</th>\n",
              "      <th>std_err</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(35,)</td>\n",
              "      <td>[-0.019271950041066428, -0.023552255915925396,...</td>\n",
              "      <td>-0.017986</td>\n",
              "      <td>(tested_positive.1,)</td>\n",
              "      <td>0.006353</td>\n",
              "      <td>0.004943</td>\n",
              "      <td>0.002471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(35, 37)</td>\n",
              "      <td>[-0.018914123466337623, -0.022816625697143225,...</td>\n",
              "      <td>-0.017281</td>\n",
              "      <td>(tested_positive.1, ili.2)</td>\n",
              "      <td>0.006212</td>\n",
              "      <td>0.004833</td>\n",
              "      <td>0.002417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(19, 35, 37)</td>\n",
              "      <td>[-0.017380464225647575, -0.021078154742476436,...</td>\n",
              "      <td>-0.016265</td>\n",
              "      <td>(ili.1, tested_positive.1, ili.2)</td>\n",
              "      <td>0.00551</td>\n",
              "      <td>0.004287</td>\n",
              "      <td>0.002143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(19, 35, 37, 39)</td>\n",
              "      <td>[-0.017248812668502405, -0.020955886309990377,...</td>\n",
              "      <td>-0.016209</td>\n",
              "      <td>(ili.1, tested_positive.1, ili.2, nohh_cmnty_c...</td>\n",
              "      <td>0.005443</td>\n",
              "      <td>0.004235</td>\n",
              "      <td>0.002117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(3, 19, 35, 37, 39)</td>\n",
              "      <td>[-0.017692017636190886, -0.02046882453236554, ...</td>\n",
              "      <td>-0.016066</td>\n",
              "      <td>(nohh_cmnty_cli, ili.1, tested_positive.1, ili...</td>\n",
              "      <td>0.00536</td>\n",
              "      <td>0.00417</td>\n",
              "      <td>0.002085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>(0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>[-0.018294340754169355, -0.021483850075892, -0...</td>\n",
              "      <td>-0.016524</td>\n",
              "      <td>(cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...</td>\n",
              "      <td>0.005805</td>\n",
              "      <td>0.004516</td>\n",
              "      <td>0.002258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>(0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>[-0.018250753757065835, -0.021440402455474194,...</td>\n",
              "      <td>-0.016527</td>\n",
              "      <td>(cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...</td>\n",
              "      <td>0.005793</td>\n",
              "      <td>0.004507</td>\n",
              "      <td>0.002254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>(0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>[-0.018402161716038944, -0.022792611560741798,...</td>\n",
              "      <td>-0.016825</td>\n",
              "      <td>(cli, ili, hh_cmnty_cli, nohh_cmnty_cli, trave...</td>\n",
              "      <td>0.006214</td>\n",
              "      <td>0.004835</td>\n",
              "      <td>0.002417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>[-0.01841236593978077, -0.02296467978242637, -...</td>\n",
              "      <td>-0.016866</td>\n",
              "      <td>(cli, ili, hh_cmnty_cli, nohh_cmnty_cli, weari...</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>0.002436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>[-0.018387552805482585, -0.02296031021853254, ...</td>\n",
              "      <td>-0.01687</td>\n",
              "      <td>(cli, ili, hh_cmnty_cli, nohh_cmnty_cli, weari...</td>\n",
              "      <td>0.00625</td>\n",
              "      <td>0.004862</td>\n",
              "      <td>0.002431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>53 rows  7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fdac171-6d8f-4985-bbda-76936f8aef59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fdac171-6d8f-4985-bbda-76936f8aef59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fdac171-6d8f-4985-bbda-76936f8aef59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, we can choose to plot the average CV score as a function of the number of features as follows."
      ],
      "metadata": {
        "id": "5wP8Yec3zWnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_sequential_feature_selection as plt_sfs \n",
        "\n",
        "fig = plt_sfs(metric_dict_1, kind='std_dev') # use the standard deviation as the error bar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "u3y920pczWKW",
        "outputId": "427bfdc3-64aa-4c8b-e4c6-f98883078265"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, since 31 features were chosen, we build a model based on these 31 features and assess its predicitons as below:"
      ],
      "metadata": {
        "id": "yaOWbEYb1_tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feats_list = np.array(sfs1.k_feature_idx_) + 40\n",
        "config = {\n",
        "    'feats': feats_list,             # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GmJky_1_N0",
        "outputId": "4de9f9ac-4806-4468-d639-1c78ba5eff09"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 31)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 31)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 31)\n",
            "Saving model (epoch = 1, validation loss = 58.9460\n",
            "Saving model (epoch = 2, validation loss = 39.5200\n",
            "Saving model (epoch = 3, validation loss = 28.8573\n",
            "Saving model (epoch = 4, validation loss = 19.9973\n",
            "Saving model (epoch = 5, validation loss = 13.5494\n",
            "Saving model (epoch = 6, validation loss = 9.5294\n",
            "Saving model (epoch = 7, validation loss = 7.6577\n",
            "Saving model (epoch = 8, validation loss = 6.8709\n",
            "Saving model (epoch = 9, validation loss = 6.2661\n",
            "Saving model (epoch = 10, validation loss = 5.7987\n",
            "Saving model (epoch = 11, validation loss = 5.3214\n",
            "Saving model (epoch = 12, validation loss = 4.8833\n",
            "Saving model (epoch = 13, validation loss = 4.5334\n",
            "Saving model (epoch = 14, validation loss = 4.1827\n",
            "Saving model (epoch = 15, validation loss = 3.9350\n",
            "Saving model (epoch = 16, validation loss = 3.6769\n",
            "Saving model (epoch = 17, validation loss = 3.2944\n",
            "Saving model (epoch = 18, validation loss = 3.0645\n",
            "Saving model (epoch = 19, validation loss = 2.8637\n",
            "Saving model (epoch = 20, validation loss = 2.6708\n",
            "Saving model (epoch = 21, validation loss = 2.4778\n",
            "Saving model (epoch = 22, validation loss = 2.2834\n",
            "Saving model (epoch = 23, validation loss = 2.1264\n",
            "Saving model (epoch = 24, validation loss = 1.9923\n",
            "Saving model (epoch = 25, validation loss = 1.8644\n",
            "Saving model (epoch = 26, validation loss = 1.7422\n",
            "Saving model (epoch = 27, validation loss = 1.6550\n",
            "Saving model (epoch = 28, validation loss = 1.5820\n",
            "Saving model (epoch = 29, validation loss = 1.5345\n",
            "Saving model (epoch = 30, validation loss = 1.4436\n",
            "Saving model (epoch = 31, validation loss = 1.3948\n",
            "Saving model (epoch = 34, validation loss = 1.3529\n",
            "Saving model (epoch = 35, validation loss = 1.3149\n",
            "Saving model (epoch = 36, validation loss = 1.3102\n",
            "Saving model (epoch = 37, validation loss = 1.2808\n",
            "Saving model (epoch = 38, validation loss = 1.2500\n",
            "Saving model (epoch = 39, validation loss = 1.2371\n",
            "Saving model (epoch = 40, validation loss = 1.2296\n",
            "Saving model (epoch = 43, validation loss = 1.2293\n",
            "Saving model (epoch = 44, validation loss = 1.2201\n",
            "Saving model (epoch = 47, validation loss = 1.1989\n",
            "Saving model (epoch = 54, validation loss = 1.1793\n",
            "Saving model (epoch = 55, validation loss = 1.1759\n",
            "Saving model (epoch = 58, validation loss = 1.1751\n",
            "Saving model (epoch = 59, validation loss = 1.1702\n",
            "Saving model (epoch = 62, validation loss = 1.1639\n",
            "Saving model (epoch = 65, validation loss = 1.1515\n",
            "Saving model (epoch = 67, validation loss = 1.1463\n",
            "Saving model (epoch = 69, validation loss = 1.1424\n",
            "Saving model (epoch = 82, validation loss = 1.1120\n",
            "Saving model (epoch = 84, validation loss = 1.1071\n",
            "Saving model (epoch = 90, validation loss = 1.0992\n",
            "Saving model (epoch = 93, validation loss = 1.0896\n",
            "Saving model (epoch = 96, validation loss = 1.0843\n",
            "Saving model (epoch = 98, validation loss = 1.0820\n",
            "Saving model (epoch = 99, validation loss = 1.0766\n",
            "Saving model (epoch = 104, validation loss = 1.0670\n",
            "Saving model (epoch = 108, validation loss = 1.0578\n",
            "Saving model (epoch = 111, validation loss = 1.0541\n",
            "Saving model (epoch = 114, validation loss = 1.0478\n",
            "Saving model (epoch = 119, validation loss = 1.0382\n",
            "Saving model (epoch = 120, validation loss = 1.0363\n",
            "Saving model (epoch = 124, validation loss = 1.0362\n",
            "Saving model (epoch = 125, validation loss = 1.0287\n",
            "Saving model (epoch = 128, validation loss = 1.0248\n",
            "Saving model (epoch = 131, validation loss = 1.0181\n",
            "Saving model (epoch = 134, validation loss = 1.0116\n",
            "Saving model (epoch = 135, validation loss = 1.0086\n",
            "Saving model (epoch = 141, validation loss = 0.9983\n",
            "Saving model (epoch = 147, validation loss = 0.9889\n",
            "Saving model (epoch = 152, validation loss = 0.9797\n",
            "Saving model (epoch = 153, validation loss = 0.9792\n",
            "Saving model (epoch = 158, validation loss = 0.9705\n",
            "Saving model (epoch = 160, validation loss = 0.9678\n",
            "Saving model (epoch = 163, validation loss = 0.9649\n",
            "Saving model (epoch = 164, validation loss = 0.9615\n",
            "Saving model (epoch = 171, validation loss = 0.9592\n",
            "Saving model (epoch = 172, validation loss = 0.9485\n",
            "Saving model (epoch = 175, validation loss = 0.9452\n",
            "Saving model (epoch = 177, validation loss = 0.9423\n",
            "Saving model (epoch = 182, validation loss = 0.9373\n",
            "Saving model (epoch = 189, validation loss = 0.9337\n",
            "Saving model (epoch = 194, validation loss = 0.9206\n",
            "Saving model (epoch = 201, validation loss = 0.9199\n",
            "Saving model (epoch = 203, validation loss = 0.9150\n",
            "Saving model (epoch = 208, validation loss = 0.9081\n",
            "Saving model (epoch = 210, validation loss = 0.9013\n",
            "Saving model (epoch = 220, validation loss = 0.8947\n",
            "Saving model (epoch = 229, validation loss = 0.8856\n",
            "Saving model (epoch = 230, validation loss = 0.8850\n",
            "Saving model (epoch = 238, validation loss = 0.8848\n",
            "Saving model (epoch = 241, validation loss = 0.8835\n",
            "Saving model (epoch = 242, validation loss = 0.8799\n",
            "Saving model (epoch = 245, validation loss = 0.8791\n",
            "Saving model (epoch = 250, validation loss = 0.8726\n",
            "Saving model (epoch = 253, validation loss = 0.8715\n",
            "Saving model (epoch = 262, validation loss = 0.8657\n",
            "Saving model (epoch = 270, validation loss = 0.8623\n",
            "Saving model (epoch = 277, validation loss = 0.8611\n",
            "Saving model (epoch = 283, validation loss = 0.8558\n",
            "Saving model (epoch = 284, validation loss = 0.8538\n",
            "Saving model (epoch = 297, validation loss = 0.8493\n",
            "Saving model (epoch = 298, validation loss = 0.8469\n",
            "Saving model (epoch = 304, validation loss = 0.8440\n",
            "Saving model (epoch = 312, validation loss = 0.8427\n",
            "Saving model (epoch = 323, validation loss = 0.8409\n",
            "Saving model (epoch = 324, validation loss = 0.8401\n",
            "Saving model (epoch = 329, validation loss = 0.8360\n",
            "Saving model (epoch = 336, validation loss = 0.8337\n",
            "Saving model (epoch = 353, validation loss = 0.8295\n",
            "Saving model (epoch = 365, validation loss = 0.8280\n",
            "Saving model (epoch = 395, validation loss = 0.8257\n",
            "Saving model (epoch = 412, validation loss = 0.8205\n",
            "Saving model (epoch = 424, validation loss = 0.8198\n",
            "Saving model (epoch = 452, validation loss = 0.8154\n",
            "Saving model (epoch = 485, validation loss = 0.8127\n",
            "Saving model (epoch = 493, validation loss = 0.8098\n",
            "Saving model (epoch = 505, validation loss = 0.8087\n",
            "Saving model (epoch = 549, validation loss = 0.8085\n",
            "Saving model (epoch = 551, validation loss = 0.8070\n",
            "Saving model (epoch = 575, validation loss = 0.8067\n",
            "Saving model (epoch = 583, validation loss = 0.8050\n",
            "Saving model (epoch = 604, validation loss = 0.8047\n",
            "Saving model (epoch = 605, validation loss = 0.8036\n",
            "Saving model (epoch = 612, validation loss = 0.8020\n",
            "Saving model (epoch = 693, validation loss = 0.8017\n",
            "Saving model (epoch = 738, validation loss = 0.8011\n",
            "Saving model (epoch = 747, validation loss = 0.7962\n",
            "Saving model (epoch = 749, validation loss = 0.7932\n",
            "Saving model (epoch = 780, validation loss = 0.7932\n",
            "Saving model (epoch = 799, validation loss = 0.7869\n",
            "Saving model (epoch = 992, validation loss = 0.7848\n",
            "Finished training after 1493 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_3.csv')"
      ],
      "metadata": {
        "id": "po7Z0J7c2rZr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be examined on Kaggle, the predictions in `pred_3.csv` lead to a test loss of 0.93649."
      ],
      "metadata": {
        "id": "sdQbtY7S2xQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4-2-2. Sequential backward selection**\n",
        "In this subsection (and the following), we will just repeat the workflow in the previous subsection. "
      ],
      "metadata": {
        "id": "qkojpwE78Kd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs2 = SFS(LinearRegression(),\n",
        "           k_features='best', \n",
        "           forward=False, \n",
        "           floating=False, \n",
        "           scoring='neg_mean_squared_error', \n",
        "           cv=5)\n",
        "\n",
        "# Note that sfs2.fit(x[x.columns[40:]], y) will lead to an error like \"TypeError: \n",
        "# (XXX) is an invalid key\" code will try to index the data frame. This is likely a bug that needs to be fixed in mlxtend.\n",
        "sfs2 = sfs2.fit(np.array(x[x.columns[40:]]), np.array(y))\n",
        "\n",
        "# Note that the features below are just ranked based on their indices, not importance.\n",
        "feats_list = np.array(sfs2.k_feature_idx_) + 40\n",
        "print(f'{len(sfs2.k_feature_idx_)} features were chosen. These features include the follows: \\n {x.columns[feats_list]}')\n",
        "print(f'Also, the corresponding indices (in the original feature set) are: \\n{feats_list}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNTXTRB13EiK",
        "outputId": "dee55b99-8e12-4769-bf7f-fa375613fe25"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19 features were chosen. These features include the follows: \n",
            " Index(['cli', 'ili', 'nohh_cmnty_cli', 'spent_time', 'large_event', 'anxious',\n",
            "       'depressed', 'ili.1', 'travel_outside_state.1', 'work_outside_home.1',\n",
            "       'anxious.1', 'depressed.1', 'tested_positive.1', 'ili.2',\n",
            "       'hh_cmnty_cli.2', 'travel_outside_state.2', 'work_outside_home.2',\n",
            "       'spent_time.2', 'large_event.2'],\n",
            "      dtype='object')\n",
            "Also, the corresponding indices (in the original feature set) are: \n",
            "[40 41 43 49 50 52 53 59 63 64 70 71 75 77 78 81 82 85 86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, 19 features were selected, which is less than the number of features selected in the sequential forward selection process. This also shows that different searching directions in sequential feature selection could lead to different results."
      ],
      "metadata": {
        "id": "kqpNyvwT8rRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': feats_list,             # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PipuSIMr6rT-",
        "outputId": "f0854c43-f643-4d2a-9d6a-ae66c02cd41d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 19)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 19)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 19)\n",
            "Saving model (epoch = 1, validation loss = 172.5340\n",
            "Saving model (epoch = 2, validation loss = 53.6685\n",
            "Saving model (epoch = 3, validation loss = 46.6069\n",
            "Saving model (epoch = 4, validation loss = 33.5969\n",
            "Saving model (epoch = 5, validation loss = 24.0443\n",
            "Saving model (epoch = 6, validation loss = 15.8381\n",
            "Saving model (epoch = 7, validation loss = 10.7968\n",
            "Saving model (epoch = 8, validation loss = 8.3942\n",
            "Saving model (epoch = 9, validation loss = 7.3270\n",
            "Saving model (epoch = 10, validation loss = 6.6187\n",
            "Saving model (epoch = 11, validation loss = 6.0743\n",
            "Saving model (epoch = 12, validation loss = 5.6209\n",
            "Saving model (epoch = 13, validation loss = 5.1833\n",
            "Saving model (epoch = 14, validation loss = 4.8652\n",
            "Saving model (epoch = 15, validation loss = 4.5406\n",
            "Saving model (epoch = 16, validation loss = 4.2289\n",
            "Saving model (epoch = 17, validation loss = 3.9585\n",
            "Saving model (epoch = 18, validation loss = 3.7478\n",
            "Saving model (epoch = 19, validation loss = 3.5251\n",
            "Saving model (epoch = 20, validation loss = 3.2757\n",
            "Saving model (epoch = 21, validation loss = 3.1048\n",
            "Saving model (epoch = 22, validation loss = 2.8764\n",
            "Saving model (epoch = 23, validation loss = 2.7123\n",
            "Saving model (epoch = 24, validation loss = 2.5690\n",
            "Saving model (epoch = 25, validation loss = 2.4373\n",
            "Saving model (epoch = 26, validation loss = 2.3629\n",
            "Saving model (epoch = 27, validation loss = 2.2122\n",
            "Saving model (epoch = 28, validation loss = 2.0133\n",
            "Saving model (epoch = 29, validation loss = 1.8970\n",
            "Saving model (epoch = 30, validation loss = 1.7927\n",
            "Saving model (epoch = 31, validation loss = 1.7101\n",
            "Saving model (epoch = 32, validation loss = 1.6104\n",
            "Saving model (epoch = 33, validation loss = 1.5318\n",
            "Saving model (epoch = 34, validation loss = 1.4511\n",
            "Saving model (epoch = 35, validation loss = 1.3884\n",
            "Saving model (epoch = 36, validation loss = 1.3237\n",
            "Saving model (epoch = 37, validation loss = 1.2979\n",
            "Saving model (epoch = 38, validation loss = 1.2413\n",
            "Saving model (epoch = 39, validation loss = 1.1799\n",
            "Saving model (epoch = 40, validation loss = 1.1493\n",
            "Saving model (epoch = 41, validation loss = 1.1107\n",
            "Saving model (epoch = 45, validation loss = 1.0697\n",
            "Saving model (epoch = 46, validation loss = 1.0026\n",
            "Saving model (epoch = 48, validation loss = 0.9811\n",
            "Saving model (epoch = 49, validation loss = 0.9696\n",
            "Saving model (epoch = 51, validation loss = 0.9556\n",
            "Saving model (epoch = 53, validation loss = 0.9473\n",
            "Saving model (epoch = 54, validation loss = 0.9456\n",
            "Saving model (epoch = 55, validation loss = 0.9437\n",
            "Saving model (epoch = 56, validation loss = 0.9316\n",
            "Saving model (epoch = 60, validation loss = 0.9226\n",
            "Saving model (epoch = 63, validation loss = 0.9217\n",
            "Saving model (epoch = 64, validation loss = 0.9216\n",
            "Saving model (epoch = 68, validation loss = 0.9215\n",
            "Saving model (epoch = 76, validation loss = 0.9173\n",
            "Saving model (epoch = 79, validation loss = 0.9156\n",
            "Saving model (epoch = 82, validation loss = 0.9150\n",
            "Saving model (epoch = 85, validation loss = 0.9133\n",
            "Saving model (epoch = 88, validation loss = 0.9131\n",
            "Saving model (epoch = 92, validation loss = 0.9085\n",
            "Saving model (epoch = 102, validation loss = 0.9081\n",
            "Saving model (epoch = 105, validation loss = 0.9072\n",
            "Saving model (epoch = 106, validation loss = 0.9052\n",
            "Saving model (epoch = 112, validation loss = 0.9017\n",
            "Saving model (epoch = 113, validation loss = 0.9012\n",
            "Saving model (epoch = 122, validation loss = 0.8994\n",
            "Saving model (epoch = 124, validation loss = 0.8984\n",
            "Saving model (epoch = 131, validation loss = 0.8973\n",
            "Saving model (epoch = 135, validation loss = 0.8952\n",
            "Saving model (epoch = 138, validation loss = 0.8923\n",
            "Saving model (epoch = 146, validation loss = 0.8918\n",
            "Saving model (epoch = 147, validation loss = 0.8901\n",
            "Saving model (epoch = 150, validation loss = 0.8887\n",
            "Saving model (epoch = 158, validation loss = 0.8880\n",
            "Saving model (epoch = 159, validation loss = 0.8871\n",
            "Saving model (epoch = 166, validation loss = 0.8859\n",
            "Saving model (epoch = 170, validation loss = 0.8830\n",
            "Saving model (epoch = 177, validation loss = 0.8828\n",
            "Saving model (epoch = 184, validation loss = 0.8825\n",
            "Saving model (epoch = 187, validation loss = 0.8784\n",
            "Saving model (epoch = 191, validation loss = 0.8774\n",
            "Saving model (epoch = 193, validation loss = 0.8767\n",
            "Saving model (epoch = 197, validation loss = 0.8760\n",
            "Saving model (epoch = 200, validation loss = 0.8730\n",
            "Saving model (epoch = 208, validation loss = 0.8725\n",
            "Saving model (epoch = 210, validation loss = 0.8710\n",
            "Saving model (epoch = 212, validation loss = 0.8700\n",
            "Saving model (epoch = 217, validation loss = 0.8682\n",
            "Saving model (epoch = 237, validation loss = 0.8665\n",
            "Saving model (epoch = 240, validation loss = 0.8627\n",
            "Saving model (epoch = 248, validation loss = 0.8612\n",
            "Saving model (epoch = 250, validation loss = 0.8609\n",
            "Saving model (epoch = 257, validation loss = 0.8588\n",
            "Saving model (epoch = 277, validation loss = 0.8569\n",
            "Saving model (epoch = 289, validation loss = 0.8518\n",
            "Saving model (epoch = 303, validation loss = 0.8507\n",
            "Saving model (epoch = 308, validation loss = 0.8507\n",
            "Saving model (epoch = 309, validation loss = 0.8499\n",
            "Saving model (epoch = 337, validation loss = 0.8473\n",
            "Saving model (epoch = 339, validation loss = 0.8467\n",
            "Saving model (epoch = 343, validation loss = 0.8457\n",
            "Saving model (epoch = 351, validation loss = 0.8436\n",
            "Saving model (epoch = 359, validation loss = 0.8435\n",
            "Saving model (epoch = 376, validation loss = 0.8404\n",
            "Saving model (epoch = 390, validation loss = 0.8396\n",
            "Saving model (epoch = 396, validation loss = 0.8368\n",
            "Saving model (epoch = 404, validation loss = 0.8354\n",
            "Saving model (epoch = 440, validation loss = 0.8326\n",
            "Saving model (epoch = 459, validation loss = 0.8319\n",
            "Saving model (epoch = 462, validation loss = 0.8317\n",
            "Saving model (epoch = 464, validation loss = 0.8312\n",
            "Saving model (epoch = 492, validation loss = 0.8291\n",
            "Saving model (epoch = 509, validation loss = 0.8286\n",
            "Saving model (epoch = 531, validation loss = 0.8261\n",
            "Saving model (epoch = 541, validation loss = 0.8259\n",
            "Saving model (epoch = 551, validation loss = 0.8247\n",
            "Saving model (epoch = 573, validation loss = 0.8242\n",
            "Saving model (epoch = 602, validation loss = 0.8229\n",
            "Saving model (epoch = 645, validation loss = 0.8217\n",
            "Saving model (epoch = 709, validation loss = 0.8207\n",
            "Saving model (epoch = 959, validation loss = 0.8207\n",
            "Saving model (epoch = 963, validation loss = 0.8205\n",
            "Saving model (epoch = 998, validation loss = 0.8196\n",
            "Saving model (epoch = 1012, validation loss = 0.8155\n",
            "Finished training after 1513 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_4.csv')"
      ],
      "metadata": {
        "id": "tOjSUXOZ8JWK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, the test loss of the predictions in `pred_4.csv` is 0.92332."
      ],
      "metadata": {
        "id": "f2R8xKdKDQ52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4-2-3. Sequential floating forward selection**\n",
        "Again, we repeat the workflow presented previously."
      ],
      "metadata": {
        "id": "KkGIydvf8Mdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs3 = SFS(LinearRegression(),\n",
        "           k_features='best', \n",
        "           forward=True, \n",
        "           floating=True, \n",
        "           scoring='neg_mean_squared_error', \n",
        "           cv=5)\n",
        "\n",
        "# Note that sfs2.fit(x[x.columns[40:]], y) will lead to an error like \"TypeError: \n",
        "# (XXX) is an invalid key\" code will try to index the data frame. This is likely a bug that needs to be fixed in mlxtend.\n",
        "sfs3 = sfs3.fit(np.array(x[x.columns[40:]]), np.array(y))\n",
        "\n",
        "# Note that the features below are just ranked based on their indices, not importance.\n",
        "feats_list = np.array(sfs3.k_feature_idx_) + 40\n",
        "print(f'{len(sfs3.k_feature_idx_)} features were chosen. These features include the follows: \\n {x.columns[feats_list]}')\n",
        "print(f'Also, the corresponding indices (in the original feature set) are: \\n{feats_list}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwRxR9xK8Yut",
        "outputId": "a677afbf-9224-4d89-c193-75696e795848"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 features were chosen. These features include the follows: \n",
            " Index(['cli', 'ili', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'work_outside_home',\n",
            "       'shop', 'spent_time', 'anxious', 'depressed', 'cli.1', 'ili.1',\n",
            "       'hh_cmnty_cli.1', 'nohh_cmnty_cli.1', 'work_outside_home.1', 'shop.1',\n",
            "       'large_event.1', 'anxious.1', 'depressed.1', 'tested_positive.1',\n",
            "       'ili.2', 'hh_cmnty_cli.2', 'work_outside_home.2', 'shop.2',\n",
            "       'spent_time.2', 'large_event.2'],\n",
            "      dtype='object')\n",
            "Also, the corresponding indices (in the original feature set) are: \n",
            "[40 41 42 43 46 47 49 52 53 58 59 60 61 64 65 68 70 71 75 77 78 82 83 85\n",
            " 86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': feats_list,             # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn6MNRtf8b3b",
        "outputId": "10cccf2f-f605-46f7-c75e-1bfcaf35b066"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 25)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 25)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 25)\n",
            "Saving model (epoch = 1, validation loss = 27.0874\n",
            "Saving model (epoch = 2, validation loss = 16.2258\n",
            "Saving model (epoch = 3, validation loss = 11.3607\n",
            "Saving model (epoch = 4, validation loss = 9.7578\n",
            "Saving model (epoch = 5, validation loss = 8.7228\n",
            "Saving model (epoch = 6, validation loss = 7.9965\n",
            "Saving model (epoch = 7, validation loss = 7.4073\n",
            "Saving model (epoch = 8, validation loss = 6.4913\n",
            "Saving model (epoch = 9, validation loss = 5.9130\n",
            "Saving model (epoch = 10, validation loss = 5.7819\n",
            "Saving model (epoch = 11, validation loss = 4.8869\n",
            "Saving model (epoch = 12, validation loss = 4.5827\n",
            "Saving model (epoch = 13, validation loss = 4.2375\n",
            "Saving model (epoch = 14, validation loss = 3.8928\n",
            "Saving model (epoch = 15, validation loss = 3.3907\n",
            "Saving model (epoch = 16, validation loss = 3.1442\n",
            "Saving model (epoch = 17, validation loss = 2.8654\n",
            "Saving model (epoch = 18, validation loss = 2.6357\n",
            "Saving model (epoch = 20, validation loss = 2.4703\n",
            "Saving model (epoch = 21, validation loss = 2.0233\n",
            "Saving model (epoch = 22, validation loss = 1.9915\n",
            "Saving model (epoch = 23, validation loss = 1.7608\n",
            "Saving model (epoch = 24, validation loss = 1.6896\n",
            "Saving model (epoch = 25, validation loss = 1.6517\n",
            "Saving model (epoch = 26, validation loss = 1.4905\n",
            "Saving model (epoch = 27, validation loss = 1.4196\n",
            "Saving model (epoch = 28, validation loss = 1.3602\n",
            "Saving model (epoch = 29, validation loss = 1.3182\n",
            "Saving model (epoch = 31, validation loss = 1.2728\n",
            "Saving model (epoch = 32, validation loss = 1.2445\n",
            "Saving model (epoch = 33, validation loss = 1.1585\n",
            "Saving model (epoch = 34, validation loss = 1.1289\n",
            "Saving model (epoch = 35, validation loss = 1.1087\n",
            "Saving model (epoch = 38, validation loss = 1.0442\n",
            "Saving model (epoch = 40, validation loss = 1.0145\n",
            "Saving model (epoch = 42, validation loss = 0.9895\n",
            "Saving model (epoch = 46, validation loss = 0.9491\n",
            "Saving model (epoch = 47, validation loss = 0.9428\n",
            "Saving model (epoch = 50, validation loss = 0.9207\n",
            "Saving model (epoch = 54, validation loss = 0.9095\n",
            "Saving model (epoch = 55, validation loss = 0.8990\n",
            "Saving model (epoch = 56, validation loss = 0.8945\n",
            "Saving model (epoch = 59, validation loss = 0.8858\n",
            "Saving model (epoch = 63, validation loss = 0.8785\n",
            "Saving model (epoch = 66, validation loss = 0.8720\n",
            "Saving model (epoch = 67, validation loss = 0.8716\n",
            "Saving model (epoch = 74, validation loss = 0.8667\n",
            "Saving model (epoch = 80, validation loss = 0.8595\n",
            "Saving model (epoch = 89, validation loss = 0.8556\n",
            "Saving model (epoch = 92, validation loss = 0.8517\n",
            "Saving model (epoch = 101, validation loss = 0.8471\n",
            "Saving model (epoch = 111, validation loss = 0.8430\n",
            "Saving model (epoch = 118, validation loss = 0.8414\n",
            "Saving model (epoch = 122, validation loss = 0.8399\n",
            "Saving model (epoch = 123, validation loss = 0.8372\n",
            "Saving model (epoch = 136, validation loss = 0.8339\n",
            "Saving model (epoch = 156, validation loss = 0.8303\n",
            "Saving model (epoch = 165, validation loss = 0.8290\n",
            "Saving model (epoch = 171, validation loss = 0.8283\n",
            "Saving model (epoch = 173, validation loss = 0.8268\n",
            "Saving model (epoch = 180, validation loss = 0.8257\n",
            "Saving model (epoch = 182, validation loss = 0.8245\n",
            "Saving model (epoch = 198, validation loss = 0.8238\n",
            "Saving model (epoch = 199, validation loss = 0.8234\n",
            "Saving model (epoch = 215, validation loss = 0.8206\n",
            "Saving model (epoch = 221, validation loss = 0.8185\n",
            "Saving model (epoch = 236, validation loss = 0.8180\n",
            "Saving model (epoch = 239, validation loss = 0.8163\n",
            "Saving model (epoch = 273, validation loss = 0.8111\n",
            "Saving model (epoch = 276, validation loss = 0.8110\n",
            "Saving model (epoch = 289, validation loss = 0.8098\n",
            "Saving model (epoch = 320, validation loss = 0.8090\n",
            "Saving model (epoch = 324, validation loss = 0.8084\n",
            "Saving model (epoch = 326, validation loss = 0.8071\n",
            "Saving model (epoch = 339, validation loss = 0.8047\n",
            "Saving model (epoch = 356, validation loss = 0.8047\n",
            "Saving model (epoch = 365, validation loss = 0.8040\n",
            "Saving model (epoch = 383, validation loss = 0.8040\n",
            "Saving model (epoch = 403, validation loss = 0.8037\n",
            "Saving model (epoch = 404, validation loss = 0.8019\n",
            "Saving model (epoch = 413, validation loss = 0.8018\n",
            "Saving model (epoch = 416, validation loss = 0.8005\n",
            "Saving model (epoch = 426, validation loss = 0.7997\n",
            "Saving model (epoch = 431, validation loss = 0.7981\n",
            "Saving model (epoch = 438, validation loss = 0.7971\n",
            "Saving model (epoch = 509, validation loss = 0.7961\n",
            "Saving model (epoch = 521, validation loss = 0.7949\n",
            "Saving model (epoch = 583, validation loss = 0.7934\n",
            "Saving model (epoch = 616, validation loss = 0.7923\n",
            "Saving model (epoch = 647, validation loss = 0.7917\n",
            "Saving model (epoch = 706, validation loss = 0.7917\n",
            "Saving model (epoch = 718, validation loss = 0.7907\n",
            "Saving model (epoch = 724, validation loss = 0.7876\n",
            "Saving model (epoch = 762, validation loss = 0.7863\n",
            "Saving model (epoch = 855, validation loss = 0.7856\n",
            "Finished training after 1356 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_5.csv')"
      ],
      "metadata": {
        "id": "RsK51cZd8klh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test loss of the predictions in `pred_5.csv` is 0.91355."
      ],
      "metadata": {
        "id": "lBLX896MDrnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4-2-4. Sequential floating backward selection**"
      ],
      "metadata": {
        "id": "NWyvbDKL8QRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs4 = SFS(LinearRegression(),\n",
        "           k_features='best', \n",
        "           forward=False, \n",
        "           floating=True, \n",
        "           scoring='neg_mean_squared_error', \n",
        "           cv=5)\n",
        "\n",
        "# Note that sfs2.fit(x[x.columns[40:]], y) will lead to an error like \"TypeError: \n",
        "# (XXX) is an invalid key\" code will try to index the data frame. This is likely a bug that needs to be fixed in mlxtend.\n",
        "sfs4 = sfs4.fit(np.array(x[x.columns[40:]]), np.array(y))\n",
        "\n",
        "# Note that the features below are just ranked based on their indices, not importance.\n",
        "feats_list = np.array(sfs4.k_feature_idx_) + 40\n",
        "print(f'{len(sfs4.k_feature_idx_)} features were chosen. These features include the follows: \\n {x.columns[feats_list]}')\n",
        "print(f'Also, the corresponding indices (in the original feature set) are: \\n{feats_list}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdlvrQlT8-yJ",
        "outputId": "522b3bce-a3c0-4ddf-9899-37641440a614"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19 features were chosen. These features include the follows: \n",
            " Index(['cli', 'ili', 'shop', 'anxious', 'depressed', 'cli.1', 'ili.1',\n",
            "       'hh_cmnty_cli.1', 'travel_outside_state.1', 'shop.1', 'restaurant.1',\n",
            "       'anxious.1', 'depressed.1', 'tested_positive.1', 'ili.2',\n",
            "       'hh_cmnty_cli.2', 'travel_outside_state.2', 'shop.2', 'large_event.2'],\n",
            "      dtype='object')\n",
            "Also, the corresponding indices (in the original feature set) are: \n",
            "[40 41 47 52 53 58 59 60 63 65 66 70 71 75 77 78 81 83 86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': feats_list,             # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,               # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',             # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {},              # Here we just use the default\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N2Dhza79zWq",
        "outputId": "6e852c31-cfa4-4f34-a017-cc6e7ddf8f7c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 19)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 19)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 19)\n",
            "Saving model (epoch = 1, validation loss = 59.9895\n",
            "Saving model (epoch = 2, validation loss = 30.9160\n",
            "Saving model (epoch = 3, validation loss = 18.9280\n",
            "Saving model (epoch = 4, validation loss = 11.7958\n",
            "Saving model (epoch = 5, validation loss = 7.6606\n",
            "Saving model (epoch = 6, validation loss = 5.6946\n",
            "Saving model (epoch = 7, validation loss = 4.7360\n",
            "Saving model (epoch = 8, validation loss = 4.1373\n",
            "Saving model (epoch = 9, validation loss = 3.6084\n",
            "Saving model (epoch = 10, validation loss = 3.2360\n",
            "Saving model (epoch = 11, validation loss = 3.0102\n",
            "Saving model (epoch = 12, validation loss = 2.7473\n",
            "Saving model (epoch = 13, validation loss = 2.5113\n",
            "Saving model (epoch = 14, validation loss = 2.3395\n",
            "Saving model (epoch = 15, validation loss = 2.1586\n",
            "Saving model (epoch = 16, validation loss = 2.0590\n",
            "Saving model (epoch = 17, validation loss = 1.8900\n",
            "Saving model (epoch = 18, validation loss = 1.8382\n",
            "Saving model (epoch = 19, validation loss = 1.6692\n",
            "Saving model (epoch = 20, validation loss = 1.5981\n",
            "Saving model (epoch = 21, validation loss = 1.4923\n",
            "Saving model (epoch = 22, validation loss = 1.4187\n",
            "Saving model (epoch = 23, validation loss = 1.3502\n",
            "Saving model (epoch = 24, validation loss = 1.2826\n",
            "Saving model (epoch = 25, validation loss = 1.2316\n",
            "Saving model (epoch = 26, validation loss = 1.2315\n",
            "Saving model (epoch = 27, validation loss = 1.1652\n",
            "Saving model (epoch = 28, validation loss = 1.1267\n",
            "Saving model (epoch = 29, validation loss = 1.0922\n",
            "Saving model (epoch = 30, validation loss = 1.0604\n",
            "Saving model (epoch = 31, validation loss = 1.0382\n",
            "Saving model (epoch = 32, validation loss = 1.0347\n",
            "Saving model (epoch = 33, validation loss = 1.0032\n",
            "Saving model (epoch = 35, validation loss = 0.9777\n",
            "Saving model (epoch = 36, validation loss = 0.9702\n",
            "Saving model (epoch = 39, validation loss = 0.9544\n",
            "Saving model (epoch = 40, validation loss = 0.9492\n",
            "Saving model (epoch = 41, validation loss = 0.9376\n",
            "Saving model (epoch = 43, validation loss = 0.9363\n",
            "Saving model (epoch = 48, validation loss = 0.9279\n",
            "Saving model (epoch = 49, validation loss = 0.9176\n",
            "Saving model (epoch = 51, validation loss = 0.9173\n",
            "Saving model (epoch = 55, validation loss = 0.9140\n",
            "Saving model (epoch = 57, validation loss = 0.9130\n",
            "Saving model (epoch = 64, validation loss = 0.9119\n",
            "Saving model (epoch = 69, validation loss = 0.9044\n",
            "Saving model (epoch = 81, validation loss = 0.9022\n",
            "Saving model (epoch = 87, validation loss = 0.8973\n",
            "Saving model (epoch = 91, validation loss = 0.8973\n",
            "Saving model (epoch = 93, validation loss = 0.8961\n",
            "Saving model (epoch = 94, validation loss = 0.8942\n",
            "Saving model (epoch = 97, validation loss = 0.8907\n",
            "Saving model (epoch = 112, validation loss = 0.8879\n",
            "Saving model (epoch = 113, validation loss = 0.8839\n",
            "Saving model (epoch = 120, validation loss = 0.8788\n",
            "Saving model (epoch = 128, validation loss = 0.8755\n",
            "Saving model (epoch = 136, validation loss = 0.8721\n",
            "Saving model (epoch = 148, validation loss = 0.8709\n",
            "Saving model (epoch = 151, validation loss = 0.8702\n",
            "Saving model (epoch = 154, validation loss = 0.8637\n",
            "Saving model (epoch = 165, validation loss = 0.8630\n",
            "Saving model (epoch = 168, validation loss = 0.8610\n",
            "Saving model (epoch = 178, validation loss = 0.8564\n",
            "Saving model (epoch = 184, validation loss = 0.8527\n",
            "Saving model (epoch = 207, validation loss = 0.8515\n",
            "Saving model (epoch = 212, validation loss = 0.8479\n",
            "Saving model (epoch = 217, validation loss = 0.8454\n",
            "Saving model (epoch = 218, validation loss = 0.8439\n",
            "Saving model (epoch = 231, validation loss = 0.8414\n",
            "Saving model (epoch = 240, validation loss = 0.8393\n",
            "Saving model (epoch = 262, validation loss = 0.8344\n",
            "Saving model (epoch = 277, validation loss = 0.8317\n",
            "Saving model (epoch = 290, validation loss = 0.8313\n",
            "Saving model (epoch = 292, validation loss = 0.8294\n",
            "Saving model (epoch = 301, validation loss = 0.8279\n",
            "Saving model (epoch = 313, validation loss = 0.8261\n",
            "Saving model (epoch = 336, validation loss = 0.8221\n",
            "Saving model (epoch = 358, validation loss = 0.8201\n",
            "Saving model (epoch = 376, validation loss = 0.8189\n",
            "Saving model (epoch = 387, validation loss = 0.8173\n",
            "Saving model (epoch = 392, validation loss = 0.8172\n",
            "Saving model (epoch = 398, validation loss = 0.8126\n",
            "Saving model (epoch = 441, validation loss = 0.8126\n",
            "Saving model (epoch = 448, validation loss = 0.8102\n",
            "Saving model (epoch = 476, validation loss = 0.8100\n",
            "Saving model (epoch = 484, validation loss = 0.8084\n",
            "Saving model (epoch = 496, validation loss = 0.8078\n",
            "Saving model (epoch = 498, validation loss = 0.8015\n",
            "Saving model (epoch = 601, validation loss = 0.7993\n",
            "Saving model (epoch = 642, validation loss = 0.7946\n",
            "Saving model (epoch = 655, validation loss = 0.7913\n",
            "Saving model (epoch = 870, validation loss = 0.7879\n",
            "Saving model (epoch = 1200, validation loss = 0.7829\n",
            "Finished training after 1701 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_6.csv')"
      ],
      "metadata": {
        "id": "pCDIetQN92vb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As calculated on Kaggle, the test loss of the predictions made in `pred_6.csv` is 0.93924."
      ],
      "metadata": {
        "id": "k6jS3QuRD7B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4-2-5. Exhaustive search**\n",
        "As mentioned before, exhaustive search is too computational expensive in our case, so we will not perform it here, but below is an example code for doing it if needed.\n",
        "```\n",
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
        "\n",
        "efs = EFS(LinearRegression(),\n",
        "          min_features=1,\n",
        "          max_features=x.shape[1],\n",
        "          scoring='neg_mean_squared_error',\n",
        "          print_progress=True,\n",
        "          cv=5)\n",
        "\n",
        "efs = efsfit(np.array(x[x.columns[40:]]), np.array(y))\n",
        "```\n"
      ],
      "metadata": {
        "id": "oJFpUO7I3cE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4-3. Permutation importance**"
      ],
      "metadata": {
        "id": "9RJ4I_V8et2R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kbUlGj-ZeMt"
      },
      "source": [
        "## **5. Embedded feature selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz_oW7kl5FCS"
      },
      "source": [
        "# **Section 7. Apply L1 and L2 regularization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpMExOL75H1r"
      },
      "source": [
        "Note that at this point, we are not sure if the second model also overfitted the data but just not as much as the first model did. To further prevent overfitting, we can apply common techniques like regularization, boosting or bagging. In this section, we will apply L1 and L2 regularization to the model. \n",
        "\n",
        "As a reminder, a regression model that uses L1 regularization is called Lasso regression (with Lasso standing for Least Absolute Shrinkage and Selection Operator) and the model which uses L2 regularization is called ridge regression. The key difference between these two is the penalty term (the term associated with $\\lambda$ below). Specifically, in the context of multiple linear regression,\n",
        "- The cost function of Lasso regression: $\\text{Loss}=\\text{Error}(y, \\hat{y})+\\lambda\\sum_{i=1}^pw_{i}^2$\n",
        "- The cost function of ridge regression: $\\text{Loss}=\\text{Error}(y, \\hat{y})+\\lambda\\sum_{i=1}^p|w_{i}|$\n",
        "\n",
        "The underlying working principle of both techniques is based on the fact that the predicted value is less sensitive to features associated with a smaller weight. \n",
        "\n",
        "Notably, if $\\lambda=0$, both Lasso regression and ridge regression degrade to ordinary linear regression in this case, while an excessively large $\\lambda$ value will lead to under-fitting. \n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=NyG-7nRpsW8&ab_channel=DeepLearningAI\n",
        "https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IwPY7sKM5FSy"
      },
      "outputs": [],
      "source": [
        "class NeuralNet_L1Regularized(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet_L1Regularized, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1))\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target, L1_lambda=1e-3):\n",
        "        ''' Calculate loss '''\n",
        "        loss = self.criterion(pred, target)\n",
        "        reg = 0\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                reg += torch.norm(param, 1)\n",
        "        loss += L1_lambda * reg\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdB3yAGH5N3X",
        "outputId": "f1250515-8358-407b-8fbb-f5b1414ff38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 42)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 42)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 42)\n",
            "Saving model (epoch = 1, validation loss = 255.7357\n",
            "Saving model (epoch = 2, validation loss = 20.4055\n",
            "Saving model (epoch = 3, validation loss = 13.9201\n",
            "Saving model (epoch = 5, validation loss = 5.0080\n",
            "Saving model (epoch = 6, validation loss = 2.2747\n",
            "Saving model (epoch = 7, validation loss = 2.0897\n",
            "Saving model (epoch = 9, validation loss = 1.7603\n",
            "Saving model (epoch = 10, validation loss = 1.6044\n",
            "Saving model (epoch = 11, validation loss = 1.5760\n",
            "Saving model (epoch = 12, validation loss = 1.5664\n",
            "Saving model (epoch = 13, validation loss = 1.5367\n",
            "Saving model (epoch = 14, validation loss = 1.5062\n",
            "Saving model (epoch = 15, validation loss = 1.4921\n",
            "Saving model (epoch = 16, validation loss = 1.4781\n",
            "Saving model (epoch = 17, validation loss = 1.4698\n",
            "Saving model (epoch = 18, validation loss = 1.4508\n",
            "Saving model (epoch = 19, validation loss = 1.4402\n",
            "Saving model (epoch = 20, validation loss = 1.4301\n",
            "Saving model (epoch = 21, validation loss = 1.4171\n",
            "Saving model (epoch = 22, validation loss = 1.4096\n",
            "Saving model (epoch = 23, validation loss = 1.4042\n",
            "Saving model (epoch = 24, validation loss = 1.3897\n",
            "Saving model (epoch = 25, validation loss = 1.3839\n",
            "Saving model (epoch = 26, validation loss = 1.3765\n",
            "Saving model (epoch = 27, validation loss = 1.3695\n",
            "Saving model (epoch = 28, validation loss = 1.3663\n",
            "Saving model (epoch = 29, validation loss = 1.3578\n",
            "Saving model (epoch = 30, validation loss = 1.3533\n",
            "Saving model (epoch = 31, validation loss = 1.3468\n",
            "Saving model (epoch = 32, validation loss = 1.3398\n",
            "Saving model (epoch = 33, validation loss = 1.3349\n",
            "Saving model (epoch = 34, validation loss = 1.3329\n",
            "Saving model (epoch = 35, validation loss = 1.3247\n",
            "Saving model (epoch = 36, validation loss = 1.3196\n",
            "Saving model (epoch = 38, validation loss = 1.3135\n",
            "Saving model (epoch = 39, validation loss = 1.3086\n",
            "Saving model (epoch = 40, validation loss = 1.3042\n",
            "Saving model (epoch = 42, validation loss = 1.2984\n",
            "Saving model (epoch = 43, validation loss = 1.2913\n",
            "Saving model (epoch = 45, validation loss = 1.2907\n",
            "Saving model (epoch = 46, validation loss = 1.2854\n",
            "Saving model (epoch = 47, validation loss = 1.2810\n",
            "Saving model (epoch = 48, validation loss = 1.2787\n",
            "Saving model (epoch = 49, validation loss = 1.2754\n",
            "Saving model (epoch = 51, validation loss = 1.2696\n",
            "Saving model (epoch = 52, validation loss = 1.2684\n",
            "Saving model (epoch = 54, validation loss = 1.2603\n",
            "Saving model (epoch = 57, validation loss = 1.2563\n",
            "Saving model (epoch = 58, validation loss = 1.2555\n",
            "Saving model (epoch = 59, validation loss = 1.2532\n",
            "Saving model (epoch = 60, validation loss = 1.2515\n",
            "Saving model (epoch = 62, validation loss = 1.2472\n",
            "Saving model (epoch = 64, validation loss = 1.2425\n",
            "Saving model (epoch = 65, validation loss = 1.2425\n",
            "Saving model (epoch = 66, validation loss = 1.2370\n",
            "Saving model (epoch = 68, validation loss = 1.2365\n",
            "Saving model (epoch = 69, validation loss = 1.2341\n",
            "Saving model (epoch = 70, validation loss = 1.2313\n",
            "Saving model (epoch = 72, validation loss = 1.2292\n",
            "Saving model (epoch = 73, validation loss = 1.2289\n",
            "Saving model (epoch = 74, validation loss = 1.2276\n",
            "Saving model (epoch = 75, validation loss = 1.2256\n",
            "Saving model (epoch = 77, validation loss = 1.2254\n",
            "Saving model (epoch = 78, validation loss = 1.2216\n",
            "Saving model (epoch = 79, validation loss = 1.2187\n",
            "Saving model (epoch = 81, validation loss = 1.2165\n",
            "Saving model (epoch = 82, validation loss = 1.2160\n",
            "Saving model (epoch = 84, validation loss = 1.2147\n",
            "Saving model (epoch = 85, validation loss = 1.2131\n",
            "Saving model (epoch = 87, validation loss = 1.2128\n",
            "Saving model (epoch = 88, validation loss = 1.2106\n",
            "Saving model (epoch = 90, validation loss = 1.2091\n",
            "Saving model (epoch = 92, validation loss = 1.2031\n",
            "Saving model (epoch = 95, validation loss = 1.2025\n",
            "Saving model (epoch = 98, validation loss = 1.2008\n",
            "Saving model (epoch = 101, validation loss = 1.1977\n",
            "Saving model (epoch = 104, validation loss = 1.1956\n",
            "Saving model (epoch = 106, validation loss = 1.1952\n",
            "Saving model (epoch = 107, validation loss = 1.1902\n",
            "Saving model (epoch = 113, validation loss = 1.1887\n",
            "Saving model (epoch = 114, validation loss = 1.1875\n",
            "Saving model (epoch = 118, validation loss = 1.1870\n",
            "Saving model (epoch = 119, validation loss = 1.1839\n",
            "Saving model (epoch = 122, validation loss = 1.1837\n",
            "Saving model (epoch = 124, validation loss = 1.1835\n",
            "Saving model (epoch = 126, validation loss = 1.1822\n",
            "Saving model (epoch = 128, validation loss = 1.1803\n",
            "Saving model (epoch = 131, validation loss = 1.1802\n",
            "Saving model (epoch = 132, validation loss = 1.1795\n",
            "Saving model (epoch = 134, validation loss = 1.1793\n",
            "Saving model (epoch = 135, validation loss = 1.1786\n",
            "Saving model (epoch = 136, validation loss = 1.1784\n",
            "Saving model (epoch = 138, validation loss = 1.1769\n",
            "Saving model (epoch = 142, validation loss = 1.1742\n",
            "Saving model (epoch = 146, validation loss = 1.1712\n",
            "Saving model (epoch = 155, validation loss = 1.1689\n",
            "Saving model (epoch = 158, validation loss = 1.1667\n",
            "Saving model (epoch = 161, validation loss = 1.1655\n",
            "Saving model (epoch = 169, validation loss = 1.1640\n",
            "Saving model (epoch = 172, validation loss = 1.1629\n",
            "Saving model (epoch = 176, validation loss = 1.1624\n",
            "Saving model (epoch = 180, validation loss = 1.1620\n",
            "Saving model (epoch = 184, validation loss = 1.1606\n",
            "Saving model (epoch = 188, validation loss = 1.1589\n",
            "Saving model (epoch = 191, validation loss = 1.1561\n",
            "Saving model (epoch = 197, validation loss = 1.1542\n",
            "Saving model (epoch = 207, validation loss = 1.1537\n",
            "Saving model (epoch = 212, validation loss = 1.1534\n",
            "Saving model (epoch = 213, validation loss = 1.1527\n",
            "Saving model (epoch = 214, validation loss = 1.1524\n",
            "Saving model (epoch = 216, validation loss = 1.1518\n",
            "Saving model (epoch = 218, validation loss = 1.1515\n",
            "Saving model (epoch = 223, validation loss = 1.1510\n",
            "Saving model (epoch = 225, validation loss = 1.1503\n",
            "Saving model (epoch = 228, validation loss = 1.1474\n",
            "Saving model (epoch = 231, validation loss = 1.1467\n",
            "Saving model (epoch = 235, validation loss = 1.1455\n",
            "Saving model (epoch = 247, validation loss = 1.1453\n",
            "Saving model (epoch = 248, validation loss = 1.1452\n",
            "Saving model (epoch = 251, validation loss = 1.1425\n",
            "Saving model (epoch = 259, validation loss = 1.1414\n",
            "Saving model (epoch = 263, validation loss = 1.1411\n",
            "Saving model (epoch = 264, validation loss = 1.1410\n",
            "Saving model (epoch = 267, validation loss = 1.1399\n",
            "Saving model (epoch = 270, validation loss = 1.1396\n",
            "Saving model (epoch = 276, validation loss = 1.1386\n",
            "Saving model (epoch = 279, validation loss = 1.1377\n",
            "Saving model (epoch = 280, validation loss = 1.1373\n",
            "Saving model (epoch = 283, validation loss = 1.1367\n",
            "Saving model (epoch = 286, validation loss = 1.1366\n",
            "Saving model (epoch = 288, validation loss = 1.1365\n",
            "Saving model (epoch = 293, validation loss = 1.1328\n",
            "Saving model (epoch = 296, validation loss = 1.1326\n",
            "Saving model (epoch = 299, validation loss = 1.1317\n",
            "Saving model (epoch = 310, validation loss = 1.1301\n",
            "Saving model (epoch = 312, validation loss = 1.1294\n",
            "Saving model (epoch = 318, validation loss = 1.1288\n",
            "Saving model (epoch = 321, validation loss = 1.1278\n",
            "Saving model (epoch = 325, validation loss = 1.1271\n",
            "Saving model (epoch = 329, validation loss = 1.1269\n",
            "Saving model (epoch = 338, validation loss = 1.1264\n",
            "Saving model (epoch = 340, validation loss = 1.1259\n",
            "Saving model (epoch = 344, validation loss = 1.1239\n",
            "Saving model (epoch = 346, validation loss = 1.1228\n",
            "Saving model (epoch = 349, validation loss = 1.1228\n",
            "Saving model (epoch = 355, validation loss = 1.1228\n",
            "Saving model (epoch = 357, validation loss = 1.1213\n",
            "Saving model (epoch = 363, validation loss = 1.1212\n",
            "Saving model (epoch = 366, validation loss = 1.1203\n",
            "Saving model (epoch = 370, validation loss = 1.1193\n",
            "Saving model (epoch = 375, validation loss = 1.1174\n",
            "Saving model (epoch = 383, validation loss = 1.1173\n",
            "Saving model (epoch = 387, validation loss = 1.1157\n",
            "Saving model (epoch = 394, validation loss = 1.1152\n",
            "Saving model (epoch = 397, validation loss = 1.1139\n",
            "Saving model (epoch = 400, validation loss = 1.1137\n",
            "Saving model (epoch = 406, validation loss = 1.1123\n",
            "Saving model (epoch = 409, validation loss = 1.1102\n",
            "Saving model (epoch = 417, validation loss = 1.1078\n",
            "Saving model (epoch = 439, validation loss = 1.1060\n",
            "Saving model (epoch = 445, validation loss = 1.1059\n",
            "Saving model (epoch = 447, validation loss = 1.1047\n",
            "Saving model (epoch = 449, validation loss = 1.1043\n",
            "Saving model (epoch = 456, validation loss = 1.1038\n",
            "Saving model (epoch = 460, validation loss = 1.1008\n",
            "Saving model (epoch = 475, validation loss = 1.0992\n",
            "Saving model (epoch = 482, validation loss = 1.0989\n",
            "Saving model (epoch = 495, validation loss = 1.0981\n",
            "Saving model (epoch = 496, validation loss = 1.0975\n",
            "Saving model (epoch = 499, validation loss = 1.0971\n",
            "Saving model (epoch = 500, validation loss = 1.0966\n",
            "Saving model (epoch = 505, validation loss = 1.0957\n",
            "Saving model (epoch = 507, validation loss = 1.0957\n",
            "Saving model (epoch = 510, validation loss = 1.0955\n",
            "Saving model (epoch = 512, validation loss = 1.0945\n",
            "Saving model (epoch = 518, validation loss = 1.0937\n",
            "Saving model (epoch = 521, validation loss = 1.0902\n",
            "Saving model (epoch = 531, validation loss = 1.0902\n",
            "Saving model (epoch = 534, validation loss = 1.0873\n",
            "Saving model (epoch = 554, validation loss = 1.0868\n",
            "Saving model (epoch = 560, validation loss = 1.0860\n",
            "Saving model (epoch = 563, validation loss = 1.0846\n",
            "Saving model (epoch = 576, validation loss = 1.0824\n",
            "Saving model (epoch = 590, validation loss = 1.0814\n",
            "Saving model (epoch = 598, validation loss = 1.0812\n",
            "Saving model (epoch = 603, validation loss = 1.0809\n",
            "Saving model (epoch = 606, validation loss = 1.0793\n",
            "Saving model (epoch = 619, validation loss = 1.0787\n",
            "Saving model (epoch = 621, validation loss = 1.0773\n",
            "Saving model (epoch = 630, validation loss = 1.0756\n",
            "Saving model (epoch = 638, validation loss = 1.0755\n",
            "Saving model (epoch = 641, validation loss = 1.0735\n",
            "Saving model (epoch = 648, validation loss = 1.0727\n",
            "Saving model (epoch = 653, validation loss = 1.0723\n",
            "Saving model (epoch = 656, validation loss = 1.0721\n",
            "Saving model (epoch = 663, validation loss = 1.0701\n",
            "Saving model (epoch = 668, validation loss = 1.0700\n",
            "Saving model (epoch = 677, validation loss = 1.0697\n",
            "Saving model (epoch = 678, validation loss = 1.0689\n",
            "Saving model (epoch = 681, validation loss = 1.0682\n",
            "Saving model (epoch = 692, validation loss = 1.0680\n",
            "Saving model (epoch = 693, validation loss = 1.0666\n",
            "Saving model (epoch = 697, validation loss = 1.0665\n",
            "Saving model (epoch = 700, validation loss = 1.0660\n",
            "Saving model (epoch = 703, validation loss = 1.0636\n",
            "Saving model (epoch = 714, validation loss = 1.0633\n",
            "Saving model (epoch = 717, validation loss = 1.0626\n",
            "Saving model (epoch = 724, validation loss = 1.0621\n",
            "Saving model (epoch = 732, validation loss = 1.0605\n",
            "Saving model (epoch = 746, validation loss = 1.0594\n",
            "Saving model (epoch = 756, validation loss = 1.0589\n",
            "Saving model (epoch = 761, validation loss = 1.0589\n",
            "Saving model (epoch = 763, validation loss = 1.0583\n",
            "Saving model (epoch = 764, validation loss = 1.0578\n",
            "Saving model (epoch = 767, validation loss = 1.0572\n",
            "Saving model (epoch = 773, validation loss = 1.0561\n",
            "Saving model (epoch = 776, validation loss = 1.0552\n",
            "Saving model (epoch = 789, validation loss = 1.0527\n",
            "Saving model (epoch = 802, validation loss = 1.0512\n",
            "Saving model (epoch = 809, validation loss = 1.0507\n",
            "Saving model (epoch = 820, validation loss = 1.0501\n",
            "Saving model (epoch = 825, validation loss = 1.0479\n",
            "Saving model (epoch = 837, validation loss = 1.0471\n",
            "Saving model (epoch = 860, validation loss = 1.0466\n",
            "Saving model (epoch = 865, validation loss = 1.0461\n",
            "Saving model (epoch = 867, validation loss = 1.0442\n",
            "Saving model (epoch = 878, validation loss = 1.0427\n",
            "Saving model (epoch = 888, validation loss = 1.0412\n",
            "Saving model (epoch = 891, validation loss = 1.0408\n",
            "Saving model (epoch = 912, validation loss = 1.0406\n",
            "Saving model (epoch = 916, validation loss = 1.0402\n",
            "Saving model (epoch = 919, validation loss = 1.0382\n",
            "Saving model (epoch = 926, validation loss = 1.0373\n",
            "Saving model (epoch = 938, validation loss = 1.0359\n",
            "Saving model (epoch = 948, validation loss = 1.0342\n",
            "Saving model (epoch = 971, validation loss = 1.0336\n",
            "Saving model (epoch = 983, validation loss = 1.0335\n",
            "Saving model (epoch = 985, validation loss = 1.0332\n",
            "Saving model (epoch = 987, validation loss = 1.0325\n",
            "Saving model (epoch = 999, validation loss = 1.0322\n",
            "Saving model (epoch = 1006, validation loss = 1.0318\n",
            "Saving model (epoch = 1008, validation loss = 1.0312\n",
            "Saving model (epoch = 1014, validation loss = 1.0294\n",
            "Saving model (epoch = 1021, validation loss = 1.0292\n",
            "Saving model (epoch = 1030, validation loss = 1.0291\n",
            "Saving model (epoch = 1034, validation loss = 1.0289\n",
            "Saving model (epoch = 1036, validation loss = 1.0287\n",
            "Saving model (epoch = 1038, validation loss = 1.0277\n",
            "Saving model (epoch = 1043, validation loss = 1.0269\n",
            "Saving model (epoch = 1059, validation loss = 1.0246\n",
            "Saving model (epoch = 1087, validation loss = 1.0243\n",
            "Saving model (epoch = 1097, validation loss = 1.0213\n",
            "Saving model (epoch = 1147, validation loss = 1.0193\n",
            "Saving model (epoch = 1153, validation loss = 1.0187\n",
            "Saving model (epoch = 1160, validation loss = 1.0187\n",
            "Saving model (epoch = 1171, validation loss = 1.0174\n",
            "Saving model (epoch = 1176, validation loss = 1.0168\n",
            "Saving model (epoch = 1182, validation loss = 1.0166\n",
            "Saving model (epoch = 1196, validation loss = 1.0166\n",
            "Saving model (epoch = 1205, validation loss = 1.0158\n",
            "Saving model (epoch = 1210, validation loss = 1.0150\n",
            "Saving model (epoch = 1222, validation loss = 1.0141\n",
            "Saving model (epoch = 1241, validation loss = 1.0138\n",
            "Saving model (epoch = 1245, validation loss = 1.0130\n",
            "Saving model (epoch = 1247, validation loss = 1.0130\n",
            "Saving model (epoch = 1254, validation loss = 1.0128\n",
            "Saving model (epoch = 1265, validation loss = 1.0123\n",
            "Saving model (epoch = 1268, validation loss = 1.0108\n",
            "Saving model (epoch = 1273, validation loss = 1.0102\n",
            "Saving model (epoch = 1290, validation loss = 1.0083\n",
            "Saving model (epoch = 1293, validation loss = 1.0082\n",
            "Saving model (epoch = 1313, validation loss = 1.0070\n",
            "Saving model (epoch = 1353, validation loss = 1.0067\n",
            "Saving model (epoch = 1358, validation loss = 1.0064\n",
            "Saving model (epoch = 1364, validation loss = 1.0051\n",
            "Saving model (epoch = 1382, validation loss = 1.0051\n",
            "Saving model (epoch = 1393, validation loss = 1.0046\n",
            "Saving model (epoch = 1395, validation loss = 1.0018\n",
            "Saving model (epoch = 1426, validation loss = 1.0010\n",
            "Saving model (epoch = 1448, validation loss = 1.0002\n",
            "Saving model (epoch = 1473, validation loss = 0.9993\n",
            "Saving model (epoch = 1481, validation loss = 0.9990\n",
            "Saving model (epoch = 1498, validation loss = 0.9989\n",
            "Saving model (epoch = 1508, validation loss = 0.9989\n",
            "Saving model (epoch = 1514, validation loss = 0.9981\n",
            "Saving model (epoch = 1543, validation loss = 0.9981\n",
            "Saving model (epoch = 1550, validation loss = 0.9972\n",
            "Saving model (epoch = 1589, validation loss = 0.9971\n",
            "Saving model (epoch = 1601, validation loss = 0.9963\n",
            "Saving model (epoch = 1619, validation loss = 0.9962\n",
            "Saving model (epoch = 1645, validation loss = 0.9951\n",
            "Saving model (epoch = 1695, validation loss = 0.9947\n",
            "Saving model (epoch = 1702, validation loss = 0.9941\n",
            "Saving model (epoch = 1734, validation loss = 0.9938\n",
            "Saving model (epoch = 1776, validation loss = 0.9935\n",
            "Saving model (epoch = 1859, validation loss = 0.9932\n",
            "Saving model (epoch = 1891, validation loss = 0.9927\n",
            "Saving model (epoch = 2009, validation loss = 0.9924\n",
            "Finished training after 2210 epochs\n"
          ]
        }
      ],
      "source": [
        "device = get_device()                    # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)     # The trained model will be saved to ./models/\n",
        "feats= list(range(40)) + [57, 75]\n",
        "\n",
        "config = {\n",
        "    'n_epochs': 3000,                # maximum number of epochs\n",
        "    'batch_size': 270,               # mini-batch size for dataloader\n",
        "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.001,                 # learning rate of SGD\n",
        "        'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], feats=feats)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], feats=feats)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], feats=feats)\n",
        "\n",
        "model = NeuralNet_L1Regularized(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ICN9TplB5Qll",
        "outputId": "5442bf54-ec72-4cae-868b-8549b353ed48"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-0e8a62d7c205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predict COVID-19 cases with your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred_3.csv'\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# save prediction file to pred_3.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Construct model and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: save_pred() missing 1 required positional argument: 'pred_path'"
          ]
        }
      ],
      "source": [
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred_3.csv')         # save prediction file to pred_3.csv\n",
        "\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiAT8pMc5SUJ"
      },
      "outputs": [],
      "source": [
        "class NeuralNet_L2Regularized(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet_L2Regularized, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1))\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target, L2_lambda=1e-3):\n",
        "        ''' Calculate loss '''\n",
        "        loss = self.criterion(pred, target)\n",
        "        reg = 0\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                reg += torch.norm(param, 2) ** 2\n",
        "        loss += L2_lambda * reg\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1e1aMWg5TZ0"
      },
      "outputs": [],
      "source": [
        "device = get_device()                    # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)     # The trained model will be saved to ./models/\n",
        "feats= list(range(40)) + [57, 75]\n",
        "\n",
        "config = {\n",
        "    'n_epochs': 10000,                # maximum number of epochs\n",
        "    'batch_size': 128,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "    },\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], feats=feats)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], feats=feats)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], feats=feats)\n",
        "\n",
        "model = NeuralNet_L2Regularized(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcNalST25VAw"
      },
      "outputs": [],
      "source": [
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred_4.csv')         # save prediction file to pred_4.csv\n",
        "\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPpW0GpHql7DXJRunMCeTmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}