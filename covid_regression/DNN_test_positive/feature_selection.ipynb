{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnW6YLd9Wi1ejMu7XPXpSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wehs7661/deep_learning_projects/blob/master/covid_regression/DNN_test_positive/feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring feature selection: Taking COVID-19 positive rate prediction as an example**\n",
        "\n",
        "In this notebook, we will explore the application of various feature selection methods to the neural network for predicting the COVID-19 positive rate, using the datasets adapted from [daily surveys conducted by the Delphi Group @ CMU](https://delphi.cmu.edu/covidcast/). This notebook is adapted from the [notebook](https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb) written by Heng-Jui Chang @ NTUEE as the first homework in the class of 2021 Machine Learning class taught by Dr. Hung-Yi Lee @ NTUEE. The final result of the original assignment is a CSV file containing the predictions of the COVID-19 positive rate generated from the neural network built and trained in the assignment, which should be uploaded to the corresponding [Kaggle competition](https://www.kaggle.com/c/ml2021spring-hw1/data) to assess the model by calculating the test loss. \n"
      ],
      "metadata": {
        "id": "YgqHiYQXU2Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Statement of the problem**\n",
        "Now, before we dive into what feature selection methods we are going to explore, let's first understand our task by taking a look at the training set and the test set."
      ],
      "metadata": {
        "id": "5jgt1COHZMrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data below can also be downloaded from Kaggle.\n",
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qop5PHg5W9zG",
        "outputId": "9f1ff0df-35a1-448e-db53-3640d647d91d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 86.4MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 24.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.set_option('display.max_columns', None) # to show all coumns below\n",
        "df = pd.read_csv('covid.train.csv'); df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "RVgMle8OU1UC",
        "outputId": "dd78161b-a6dd-4386-9588-835d419153a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id   AL   AK   AZ   AR   CA   CO   CT   FL   GA   ID   IL   IN   IA  \\\n",
              "0        0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1        1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2        2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3        3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4        4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2696  2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2697  2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2698  2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2699  2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "       KS   KY   LA   MD   MA   MI   MN   MS   MO   NE   NV   NJ   NM   NY  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "       NC   OH   OK   OR   PA   RI   SC   TX   UT   VA   WA   WV   WI  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "2695  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2696  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2697  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2698  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "2699  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
              "\n",
              "           cli       ili  hh_cmnty_cli  nohh_cmnty_cli  wearing_mask  \\\n",
              "0     0.814610  0.771356     25.648907       21.242063     84.644672   \n",
              "1     0.838995  0.807767     25.679101       21.280270     84.005294   \n",
              "2     0.897802  0.887893     26.060544       21.503832     84.438618   \n",
              "3     0.972842  0.965496     25.754087       21.016210     84.133873   \n",
              "4     0.955306  0.963079     25.947015       20.941798     83.995931   \n",
              "...        ...       ...           ...             ...           ...   \n",
              "2695  0.655823  0.659976     25.265366       20.468897     91.011756   \n",
              "2696  0.598352  0.602552     25.299465       20.756444     90.682057   \n",
              "2697  0.586713  0.597559     25.271178       20.770195     90.866100   \n",
              "2698  0.576435  0.595312     24.607461       20.176201     90.846126   \n",
              "2699  0.562426  0.572969     24.020275       19.654514     90.928655   \n",
              "\n",
              "      travel_outside_state  work_outside_home       shop  restaurant  \\\n",
              "0                13.462475          36.519841  63.139094   23.835119   \n",
              "1                13.467716          36.637887  63.318650   23.688882   \n",
              "2                13.038611          36.429119  62.434539   23.812411   \n",
              "3                12.581952          36.416557  62.024517   23.682974   \n",
              "4                12.938675          37.014578  62.116842   23.593983   \n",
              "...                    ...                ...        ...         ...   \n",
              "2695              6.801897          32.727184  50.265694   15.188547   \n",
              "2696              7.152368          33.638563  50.050349   15.462823   \n",
              "2697              6.857209          33.959012  50.024971   15.090116   \n",
              "2698              6.851475          33.932384  49.885129   14.779264   \n",
              "2699              6.642911          33.822577  50.056772   14.961085   \n",
              "\n",
              "      spent_time  large_event  public_transit    anxious  depressed  \\\n",
              "0      44.726055    16.946929        1.716262  15.494193  12.043275   \n",
              "1      44.385166    16.463551        1.664819  15.299228  12.051505   \n",
              "2      43.430423    16.151527        1.602635  15.409449  12.088688   \n",
              "3      43.196313    16.123386        1.641863  15.230063  11.809047   \n",
              "4      43.362200    16.159971        1.677523  15.717207  12.355918   \n",
              "...          ...          ...             ...        ...        ...   \n",
              "2695   31.597793     8.013637        1.768811  14.699027  11.227049   \n",
              "2696   31.656358     8.239559        1.789015  14.808636  11.371546   \n",
              "2697   30.839219     7.849525        1.760094  14.617563  11.163213   \n",
              "2698   30.617100     7.754800        1.780730  14.513419  11.281241   \n",
              "2699   30.595194     7.744075        1.921828  14.160990  11.163526   \n",
              "\n",
              "      felt_isolated  worried_become_ill  worried_finances  tested_positive  \\\n",
              "0         17.000647           53.439316         43.279629        19.586492   \n",
              "1         16.552264           53.256795         43.622728        20.151838   \n",
              "2         16.702086           53.991549         43.604229        20.704935   \n",
              "3         16.506973           54.185521         42.665766        21.292911   \n",
              "4         16.273294           53.637069         42.972417        21.166656   \n",
              "...             ...                 ...               ...              ...   \n",
              "2695      18.814486           68.115748         38.478143        13.869286   \n",
              "2696      19.257324           67.691795         38.953184        13.434180   \n",
              "2697      18.742673           68.024690         38.920206        13.008853   \n",
              "2698      18.539741           67.855755         39.224244        12.725638   \n",
              "2699      18.702564           67.731162         38.740651        12.613441   \n",
              "\n",
              "         cli.1     ili.1  hh_cmnty_cli.1  nohh_cmnty_cli.1  wearing_mask.1  \\\n",
              "0     0.838995  0.807767       25.679101         21.280270       84.005294   \n",
              "1     0.897802  0.887893       26.060544         21.503832       84.438618   \n",
              "2     0.972842  0.965496       25.754087         21.016210       84.133873   \n",
              "3     0.955306  0.963079       25.947015         20.941798       83.995931   \n",
              "4     0.947513  0.968764       26.350501         21.109971       83.819531   \n",
              "...        ...       ...             ...               ...             ...   \n",
              "2695  0.598352  0.602552       25.299465         20.756444       90.682057   \n",
              "2696  0.586713  0.597559       25.271178         20.770195       90.866100   \n",
              "2697  0.576435  0.595312       24.607461         20.176201       90.846126   \n",
              "2698  0.562426  0.572969       24.020275         19.654514       90.928655   \n",
              "2699  0.600671  0.611160       23.797738         19.519105       90.957424   \n",
              "\n",
              "      travel_outside_state.1  work_outside_home.1     shop.1  restaurant.1  \\\n",
              "0                  13.467716            36.637887  63.318650     23.688882   \n",
              "1                  13.038611            36.429119  62.434539     23.812411   \n",
              "2                  12.581952            36.416557  62.024517     23.682974   \n",
              "3                  12.938675            37.014578  62.116842     23.593983   \n",
              "4                  12.452336            36.270021  61.294809     22.576992   \n",
              "...                      ...                  ...        ...           ...   \n",
              "2695                7.152368            33.638563  50.050349     15.462823   \n",
              "2696                6.857209            33.959012  50.024971     15.090116   \n",
              "2697                6.851475            33.932384  49.885129     14.779264   \n",
              "2698                6.642911            33.822577  50.056772     14.961085   \n",
              "2699                6.800289            33.196095  49.620924     14.609582   \n",
              "\n",
              "      spent_time.1  large_event.1  public_transit.1  anxious.1  depressed.1  \\\n",
              "0        44.385166      16.463551          1.664819  15.299228    12.051505   \n",
              "1        43.430423      16.151527          1.602635  15.409449    12.088688   \n",
              "2        43.196313      16.123386          1.641863  15.230063    11.809047   \n",
              "3        43.362200      16.159971          1.677523  15.717207    12.355918   \n",
              "4        42.954574      15.544373          1.578030  15.295650    12.218123   \n",
              "...            ...            ...               ...        ...          ...   \n",
              "2695     31.656358       8.239559          1.789015  14.808636    11.371546   \n",
              "2696     30.839219       7.849525          1.760094  14.617563    11.163213   \n",
              "2697     30.617100       7.754800          1.780730  14.513419    11.281241   \n",
              "2698     30.595194       7.744075          1.921828  14.160990    11.163526   \n",
              "2699     30.420998       7.687974          1.992580  14.409427    11.330301   \n",
              "\n",
              "      felt_isolated.1  worried_become_ill.1  worried_finances.1  \\\n",
              "0           16.552264             53.256795           43.622728   \n",
              "1           16.702086             53.991549           43.604229   \n",
              "2           16.506973             54.185521           42.665766   \n",
              "3           16.273294             53.637069           42.972417   \n",
              "4           16.045504             52.446223           42.907472   \n",
              "...               ...                   ...                 ...   \n",
              "2695        19.257324             67.691795           38.953184   \n",
              "2696        18.742673             68.024690           38.920206   \n",
              "2697        18.539741             67.855755           39.224244   \n",
              "2698        18.702564             67.731162           38.740651   \n",
              "2699        19.134697             67.795100           38.595125   \n",
              "\n",
              "      tested_positive.1     cli.2     ili.2  hh_cmnty_cli.2  nohh_cmnty_cli.2  \\\n",
              "0             20.151838  0.897802  0.887893       26.060544         21.503832   \n",
              "1             20.704935  0.972842  0.965496       25.754087         21.016210   \n",
              "2             21.292911  0.955306  0.963079       25.947015         20.941798   \n",
              "3             21.166656  0.947513  0.968764       26.350501         21.109971   \n",
              "4             19.896607  0.883833  0.893020       26.480624         21.003982   \n",
              "...                 ...       ...       ...             ...               ...   \n",
              "2695          13.434180  0.586713  0.597559       25.271178         20.770195   \n",
              "2696          13.008853  0.576435  0.595312       24.607461         20.176201   \n",
              "2697          12.725638  0.562426  0.572969       24.020275         19.654514   \n",
              "2698          12.613441  0.600671  0.611160       23.797738         19.519105   \n",
              "2699          12.477227  0.560519  0.571126       23.467835         19.174193   \n",
              "\n",
              "      wearing_mask.2  travel_outside_state.2  work_outside_home.2     shop.2  \\\n",
              "0          84.438618               13.038611            36.429119  62.434539   \n",
              "1          84.133873               12.581952            36.416557  62.024517   \n",
              "2          83.995931               12.938675            37.014578  62.116842   \n",
              "3          83.819531               12.452336            36.270021  61.294809   \n",
              "4          84.049437               12.224644            35.380198  60.664482   \n",
              "...              ...                     ...                  ...        ...   \n",
              "2695       90.866100                6.857209            33.959012  50.024971   \n",
              "2696       90.846126                6.851475            33.932384  49.885129   \n",
              "2697       90.928655                6.642911            33.822577  50.056772   \n",
              "2698       90.957424                6.800289            33.196095  49.620924   \n",
              "2699       91.110463                6.931543            33.096657  49.510599   \n",
              "\n",
              "      restaurant.2  spent_time.2  large_event.2  public_transit.2  anxious.2  \\\n",
              "0        23.812411     43.430423      16.151527          1.602635  15.409449   \n",
              "1        23.682974     43.196313      16.123386          1.641863  15.230063   \n",
              "2        23.593983     43.362200      16.159971          1.677523  15.717207   \n",
              "3        22.576992     42.954574      15.544373          1.578030  15.295650   \n",
              "4        22.091433     43.290957      15.214655          1.641667  14.778802   \n",
              "...            ...           ...            ...               ...        ...   \n",
              "2695     15.090116     30.839219       7.849525          1.760094  14.617563   \n",
              "2696     14.779264     30.617100       7.754800          1.780730  14.513419   \n",
              "2697     14.961085     30.595194       7.744075          1.921828  14.160990   \n",
              "2698     14.609582     30.420998       7.687974          1.992580  14.409427   \n",
              "2699     14.464053     30.469791       7.692942          1.966064  14.616400   \n",
              "\n",
              "      depressed.2  felt_isolated.2  worried_become_ill.2  worried_finances.2  \\\n",
              "0       12.088688        16.702086             53.991549           43.604229   \n",
              "1       11.809047        16.506973             54.185521           42.665766   \n",
              "2       12.355918        16.273294             53.637069           42.972417   \n",
              "3       12.218123        16.045504             52.446223           42.907472   \n",
              "4       12.417256        16.134238             52.560315           43.321985   \n",
              "...           ...              ...                   ...                 ...   \n",
              "2695    11.163213        18.742673             68.024690           38.920206   \n",
              "2696    11.281241        18.539741             67.855755           39.224244   \n",
              "2697    11.163526        18.702564             67.731162           38.740651   \n",
              "2698    11.330301        19.134697             67.795100           38.595125   \n",
              "2699    11.522773        19.295834             68.284078           38.453820   \n",
              "\n",
              "      tested_positive.2  \n",
              "0             20.704935  \n",
              "1             21.292911  \n",
              "2             21.166656  \n",
              "3             19.896607  \n",
              "4             20.178428  \n",
              "...                 ...  \n",
              "2695          13.008853  \n",
              "2696          12.725638  \n",
              "2697          12.613441  \n",
              "2698          12.477227  \n",
              "2699          11.811719  \n",
              "\n",
              "[2700 rows x 95 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3aabd0e7-cc41-41e8-bea8-48d629c965e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>AL</th>\n",
              "      <th>AK</th>\n",
              "      <th>AZ</th>\n",
              "      <th>AR</th>\n",
              "      <th>CA</th>\n",
              "      <th>CO</th>\n",
              "      <th>CT</th>\n",
              "      <th>FL</th>\n",
              "      <th>GA</th>\n",
              "      <th>ID</th>\n",
              "      <th>IL</th>\n",
              "      <th>IN</th>\n",
              "      <th>IA</th>\n",
              "      <th>KS</th>\n",
              "      <th>KY</th>\n",
              "      <th>LA</th>\n",
              "      <th>MD</th>\n",
              "      <th>MA</th>\n",
              "      <th>MI</th>\n",
              "      <th>MN</th>\n",
              "      <th>MS</th>\n",
              "      <th>MO</th>\n",
              "      <th>NE</th>\n",
              "      <th>NV</th>\n",
              "      <th>NJ</th>\n",
              "      <th>NM</th>\n",
              "      <th>NY</th>\n",
              "      <th>NC</th>\n",
              "      <th>OH</th>\n",
              "      <th>OK</th>\n",
              "      <th>OR</th>\n",
              "      <th>PA</th>\n",
              "      <th>RI</th>\n",
              "      <th>SC</th>\n",
              "      <th>TX</th>\n",
              "      <th>UT</th>\n",
              "      <th>VA</th>\n",
              "      <th>WA</th>\n",
              "      <th>WV</th>\n",
              "      <th>WI</th>\n",
              "      <th>cli</th>\n",
              "      <th>ili</th>\n",
              "      <th>hh_cmnty_cli</th>\n",
              "      <th>nohh_cmnty_cli</th>\n",
              "      <th>wearing_mask</th>\n",
              "      <th>travel_outside_state</th>\n",
              "      <th>work_outside_home</th>\n",
              "      <th>shop</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>spent_time</th>\n",
              "      <th>large_event</th>\n",
              "      <th>public_transit</th>\n",
              "      <th>anxious</th>\n",
              "      <th>depressed</th>\n",
              "      <th>felt_isolated</th>\n",
              "      <th>worried_become_ill</th>\n",
              "      <th>worried_finances</th>\n",
              "      <th>tested_positive</th>\n",
              "      <th>cli.1</th>\n",
              "      <th>ili.1</th>\n",
              "      <th>hh_cmnty_cli.1</th>\n",
              "      <th>nohh_cmnty_cli.1</th>\n",
              "      <th>wearing_mask.1</th>\n",
              "      <th>travel_outside_state.1</th>\n",
              "      <th>work_outside_home.1</th>\n",
              "      <th>shop.1</th>\n",
              "      <th>restaurant.1</th>\n",
              "      <th>spent_time.1</th>\n",
              "      <th>large_event.1</th>\n",
              "      <th>public_transit.1</th>\n",
              "      <th>anxious.1</th>\n",
              "      <th>depressed.1</th>\n",
              "      <th>felt_isolated.1</th>\n",
              "      <th>worried_become_ill.1</th>\n",
              "      <th>worried_finances.1</th>\n",
              "      <th>tested_positive.1</th>\n",
              "      <th>cli.2</th>\n",
              "      <th>ili.2</th>\n",
              "      <th>hh_cmnty_cli.2</th>\n",
              "      <th>nohh_cmnty_cli.2</th>\n",
              "      <th>wearing_mask.2</th>\n",
              "      <th>travel_outside_state.2</th>\n",
              "      <th>work_outside_home.2</th>\n",
              "      <th>shop.2</th>\n",
              "      <th>restaurant.2</th>\n",
              "      <th>spent_time.2</th>\n",
              "      <th>large_event.2</th>\n",
              "      <th>public_transit.2</th>\n",
              "      <th>anxious.2</th>\n",
              "      <th>depressed.2</th>\n",
              "      <th>felt_isolated.2</th>\n",
              "      <th>worried_become_ill.2</th>\n",
              "      <th>worried_finances.2</th>\n",
              "      <th>tested_positive.2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.814610</td>\n",
              "      <td>0.771356</td>\n",
              "      <td>25.648907</td>\n",
              "      <td>21.242063</td>\n",
              "      <td>84.644672</td>\n",
              "      <td>13.462475</td>\n",
              "      <td>36.519841</td>\n",
              "      <td>63.139094</td>\n",
              "      <td>23.835119</td>\n",
              "      <td>44.726055</td>\n",
              "      <td>16.946929</td>\n",
              "      <td>1.716262</td>\n",
              "      <td>15.494193</td>\n",
              "      <td>12.043275</td>\n",
              "      <td>17.000647</td>\n",
              "      <td>53.439316</td>\n",
              "      <td>43.279629</td>\n",
              "      <td>19.586492</td>\n",
              "      <td>0.838995</td>\n",
              "      <td>0.807767</td>\n",
              "      <td>25.679101</td>\n",
              "      <td>21.280270</td>\n",
              "      <td>84.005294</td>\n",
              "      <td>13.467716</td>\n",
              "      <td>36.637887</td>\n",
              "      <td>63.318650</td>\n",
              "      <td>23.688882</td>\n",
              "      <td>44.385166</td>\n",
              "      <td>16.463551</td>\n",
              "      <td>1.664819</td>\n",
              "      <td>15.299228</td>\n",
              "      <td>12.051505</td>\n",
              "      <td>16.552264</td>\n",
              "      <td>53.256795</td>\n",
              "      <td>43.622728</td>\n",
              "      <td>20.151838</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.838995</td>\n",
              "      <td>0.807767</td>\n",
              "      <td>25.679101</td>\n",
              "      <td>21.280270</td>\n",
              "      <td>84.005294</td>\n",
              "      <td>13.467716</td>\n",
              "      <td>36.637887</td>\n",
              "      <td>63.318650</td>\n",
              "      <td>23.688882</td>\n",
              "      <td>44.385166</td>\n",
              "      <td>16.463551</td>\n",
              "      <td>1.664819</td>\n",
              "      <td>15.299228</td>\n",
              "      <td>12.051505</td>\n",
              "      <td>16.552264</td>\n",
              "      <td>53.256795</td>\n",
              "      <td>43.622728</td>\n",
              "      <td>20.151838</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.897802</td>\n",
              "      <td>0.887893</td>\n",
              "      <td>26.060544</td>\n",
              "      <td>21.503832</td>\n",
              "      <td>84.438618</td>\n",
              "      <td>13.038611</td>\n",
              "      <td>36.429119</td>\n",
              "      <td>62.434539</td>\n",
              "      <td>23.812411</td>\n",
              "      <td>43.430423</td>\n",
              "      <td>16.151527</td>\n",
              "      <td>1.602635</td>\n",
              "      <td>15.409449</td>\n",
              "      <td>12.088688</td>\n",
              "      <td>16.702086</td>\n",
              "      <td>53.991549</td>\n",
              "      <td>43.604229</td>\n",
              "      <td>20.704935</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>0.965496</td>\n",
              "      <td>25.754087</td>\n",
              "      <td>21.016210</td>\n",
              "      <td>84.133873</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>36.416557</td>\n",
              "      <td>62.024517</td>\n",
              "      <td>23.682974</td>\n",
              "      <td>43.196313</td>\n",
              "      <td>16.123386</td>\n",
              "      <td>1.641863</td>\n",
              "      <td>15.230063</td>\n",
              "      <td>11.809047</td>\n",
              "      <td>16.506973</td>\n",
              "      <td>54.185521</td>\n",
              "      <td>42.665766</td>\n",
              "      <td>21.292911</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "      <td>0.947513</td>\n",
              "      <td>0.968764</td>\n",
              "      <td>26.350501</td>\n",
              "      <td>21.109971</td>\n",
              "      <td>83.819531</td>\n",
              "      <td>12.452336</td>\n",
              "      <td>36.270021</td>\n",
              "      <td>61.294809</td>\n",
              "      <td>22.576992</td>\n",
              "      <td>42.954574</td>\n",
              "      <td>15.544373</td>\n",
              "      <td>1.578030</td>\n",
              "      <td>15.295650</td>\n",
              "      <td>12.218123</td>\n",
              "      <td>16.045504</td>\n",
              "      <td>52.446223</td>\n",
              "      <td>42.907472</td>\n",
              "      <td>19.896607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.955306</td>\n",
              "      <td>0.963079</td>\n",
              "      <td>25.947015</td>\n",
              "      <td>20.941798</td>\n",
              "      <td>83.995931</td>\n",
              "      <td>12.938675</td>\n",
              "      <td>37.014578</td>\n",
              "      <td>62.116842</td>\n",
              "      <td>23.593983</td>\n",
              "      <td>43.362200</td>\n",
              "      <td>16.159971</td>\n",
              "      <td>1.677523</td>\n",
              "      <td>15.717207</td>\n",
              "      <td>12.355918</td>\n",
              "      <td>16.273294</td>\n",
              "      <td>53.637069</td>\n",
              "      <td>42.972417</td>\n",
              "      <td>21.166656</td>\n",
              "      <td>0.947513</td>\n",
              "      <td>0.968764</td>\n",
              "      <td>26.350501</td>\n",
              "      <td>21.109971</td>\n",
              "      <td>83.819531</td>\n",
              "      <td>12.452336</td>\n",
              "      <td>36.270021</td>\n",
              "      <td>61.294809</td>\n",
              "      <td>22.576992</td>\n",
              "      <td>42.954574</td>\n",
              "      <td>15.544373</td>\n",
              "      <td>1.578030</td>\n",
              "      <td>15.295650</td>\n",
              "      <td>12.218123</td>\n",
              "      <td>16.045504</td>\n",
              "      <td>52.446223</td>\n",
              "      <td>42.907472</td>\n",
              "      <td>19.896607</td>\n",
              "      <td>0.883833</td>\n",
              "      <td>0.893020</td>\n",
              "      <td>26.480624</td>\n",
              "      <td>21.003982</td>\n",
              "      <td>84.049437</td>\n",
              "      <td>12.224644</td>\n",
              "      <td>35.380198</td>\n",
              "      <td>60.664482</td>\n",
              "      <td>22.091433</td>\n",
              "      <td>43.290957</td>\n",
              "      <td>15.214655</td>\n",
              "      <td>1.641667</td>\n",
              "      <td>14.778802</td>\n",
              "      <td>12.417256</td>\n",
              "      <td>16.134238</td>\n",
              "      <td>52.560315</td>\n",
              "      <td>43.321985</td>\n",
              "      <td>20.178428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2695</th>\n",
              "      <td>2695</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655823</td>\n",
              "      <td>0.659976</td>\n",
              "      <td>25.265366</td>\n",
              "      <td>20.468897</td>\n",
              "      <td>91.011756</td>\n",
              "      <td>6.801897</td>\n",
              "      <td>32.727184</td>\n",
              "      <td>50.265694</td>\n",
              "      <td>15.188547</td>\n",
              "      <td>31.597793</td>\n",
              "      <td>8.013637</td>\n",
              "      <td>1.768811</td>\n",
              "      <td>14.699027</td>\n",
              "      <td>11.227049</td>\n",
              "      <td>18.814486</td>\n",
              "      <td>68.115748</td>\n",
              "      <td>38.478143</td>\n",
              "      <td>13.869286</td>\n",
              "      <td>0.598352</td>\n",
              "      <td>0.602552</td>\n",
              "      <td>25.299465</td>\n",
              "      <td>20.756444</td>\n",
              "      <td>90.682057</td>\n",
              "      <td>7.152368</td>\n",
              "      <td>33.638563</td>\n",
              "      <td>50.050349</td>\n",
              "      <td>15.462823</td>\n",
              "      <td>31.656358</td>\n",
              "      <td>8.239559</td>\n",
              "      <td>1.789015</td>\n",
              "      <td>14.808636</td>\n",
              "      <td>11.371546</td>\n",
              "      <td>19.257324</td>\n",
              "      <td>67.691795</td>\n",
              "      <td>38.953184</td>\n",
              "      <td>13.434180</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2696</th>\n",
              "      <td>2696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.598352</td>\n",
              "      <td>0.602552</td>\n",
              "      <td>25.299465</td>\n",
              "      <td>20.756444</td>\n",
              "      <td>90.682057</td>\n",
              "      <td>7.152368</td>\n",
              "      <td>33.638563</td>\n",
              "      <td>50.050349</td>\n",
              "      <td>15.462823</td>\n",
              "      <td>31.656358</td>\n",
              "      <td>8.239559</td>\n",
              "      <td>1.789015</td>\n",
              "      <td>14.808636</td>\n",
              "      <td>11.371546</td>\n",
              "      <td>19.257324</td>\n",
              "      <td>67.691795</td>\n",
              "      <td>38.953184</td>\n",
              "      <td>13.434180</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2697</th>\n",
              "      <td>2697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.597559</td>\n",
              "      <td>25.271178</td>\n",
              "      <td>20.770195</td>\n",
              "      <td>90.866100</td>\n",
              "      <td>6.857209</td>\n",
              "      <td>33.959012</td>\n",
              "      <td>50.024971</td>\n",
              "      <td>15.090116</td>\n",
              "      <td>30.839219</td>\n",
              "      <td>7.849525</td>\n",
              "      <td>1.760094</td>\n",
              "      <td>14.617563</td>\n",
              "      <td>11.163213</td>\n",
              "      <td>18.742673</td>\n",
              "      <td>68.024690</td>\n",
              "      <td>38.920206</td>\n",
              "      <td>13.008853</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2698</th>\n",
              "      <td>2698</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.576435</td>\n",
              "      <td>0.595312</td>\n",
              "      <td>24.607461</td>\n",
              "      <td>20.176201</td>\n",
              "      <td>90.846126</td>\n",
              "      <td>6.851475</td>\n",
              "      <td>33.932384</td>\n",
              "      <td>49.885129</td>\n",
              "      <td>14.779264</td>\n",
              "      <td>30.617100</td>\n",
              "      <td>7.754800</td>\n",
              "      <td>1.780730</td>\n",
              "      <td>14.513419</td>\n",
              "      <td>11.281241</td>\n",
              "      <td>18.539741</td>\n",
              "      <td>67.855755</td>\n",
              "      <td>39.224244</td>\n",
              "      <td>12.725638</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "      <td>0.600671</td>\n",
              "      <td>0.611160</td>\n",
              "      <td>23.797738</td>\n",
              "      <td>19.519105</td>\n",
              "      <td>90.957424</td>\n",
              "      <td>6.800289</td>\n",
              "      <td>33.196095</td>\n",
              "      <td>49.620924</td>\n",
              "      <td>14.609582</td>\n",
              "      <td>30.420998</td>\n",
              "      <td>7.687974</td>\n",
              "      <td>1.992580</td>\n",
              "      <td>14.409427</td>\n",
              "      <td>11.330301</td>\n",
              "      <td>19.134697</td>\n",
              "      <td>67.795100</td>\n",
              "      <td>38.595125</td>\n",
              "      <td>12.477227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699</th>\n",
              "      <td>2699</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.562426</td>\n",
              "      <td>0.572969</td>\n",
              "      <td>24.020275</td>\n",
              "      <td>19.654514</td>\n",
              "      <td>90.928655</td>\n",
              "      <td>6.642911</td>\n",
              "      <td>33.822577</td>\n",
              "      <td>50.056772</td>\n",
              "      <td>14.961085</td>\n",
              "      <td>30.595194</td>\n",
              "      <td>7.744075</td>\n",
              "      <td>1.921828</td>\n",
              "      <td>14.160990</td>\n",
              "      <td>11.163526</td>\n",
              "      <td>18.702564</td>\n",
              "      <td>67.731162</td>\n",
              "      <td>38.740651</td>\n",
              "      <td>12.613441</td>\n",
              "      <td>0.600671</td>\n",
              "      <td>0.611160</td>\n",
              "      <td>23.797738</td>\n",
              "      <td>19.519105</td>\n",
              "      <td>90.957424</td>\n",
              "      <td>6.800289</td>\n",
              "      <td>33.196095</td>\n",
              "      <td>49.620924</td>\n",
              "      <td>14.609582</td>\n",
              "      <td>30.420998</td>\n",
              "      <td>7.687974</td>\n",
              "      <td>1.992580</td>\n",
              "      <td>14.409427</td>\n",
              "      <td>11.330301</td>\n",
              "      <td>19.134697</td>\n",
              "      <td>67.795100</td>\n",
              "      <td>38.595125</td>\n",
              "      <td>12.477227</td>\n",
              "      <td>0.560519</td>\n",
              "      <td>0.571126</td>\n",
              "      <td>23.467835</td>\n",
              "      <td>19.174193</td>\n",
              "      <td>91.110463</td>\n",
              "      <td>6.931543</td>\n",
              "      <td>33.096657</td>\n",
              "      <td>49.510599</td>\n",
              "      <td>14.464053</td>\n",
              "      <td>30.469791</td>\n",
              "      <td>7.692942</td>\n",
              "      <td>1.966064</td>\n",
              "      <td>14.616400</td>\n",
              "      <td>11.522773</td>\n",
              "      <td>19.295834</td>\n",
              "      <td>68.284078</td>\n",
              "      <td>38.453820</td>\n",
              "      <td>11.811719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2700 rows  95 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aabd0e7-cc41-41e8-bea8-48d629c965e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3aabd0e7-cc41-41e8-bea8-48d629c965e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3aabd0e7-cc41-41e8-bea8-48d629c965e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, the training set has 2700 samples and 95 columns, which include:\n",
        "- 1 column of the IDs\n",
        "- 40 columns showing the states encoded to one-hot vectors\n",
        "- 18 columns of Day 1 features\n",
        "- 18 columns of Day 2 features\n",
        "- 18 columns of Day 3 features\n",
        "\n",
        "More specifically, the 18 features for each day include the follows:\n",
        "- 4 columns of COVID-like illness, including\n",
        "  - `ili`: Percentage of people having influenza-like illness\n",
        "  - `cli`: Percentage of people having COVID-like illness\n",
        "  - `hh_cmnty_cli`: Percentage of people reporintg illness in their local community, including their household\n",
        "  - `noww_cmnty_cli`: Percentage of people reporting illness in their local community, not including their household \n",
        "- 8 columns of behavior indicators, including `wearing_mask`, `travel_outside_state`, `work_outside_home`, `shop`, `restaurant`, `spent_time`, `large_event`, and `public_transit`. Most names of indicators are self-explanatory, except for `spent_time`, which is the percentage of respondents who spent time indoors with someone who isn't currently staying with them in the past 24 hours. \n",
        "- 5 columns of mental health indicators, including `anxious`, `depressed`, `felt_isolated`, `worried_become_ill`, and `worried_finances`.\n",
        "- 1 column showing the percentage of people who tested positive\n",
        "\n",
        "All indicators above are expressed in percentages. For more details about how the data was collected and how the indicators were designed, please visit [this site](https://cmu-delphi.github.io/delphi-epidata/api/covidcast-signals/fb-survey.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "P2YF4MckYUDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the test set, there are 893 samples and 94 columns, with the only missing column compared with the training set being the test positive rate of Day 3 that we are going to predict using any of the 93 possible indicators as the features."
      ],
      "metadata": {
        "id": "fc9Ftke3YoHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Building our first neural network without feature selection**\n",
        "### **2-1. Setting things up**\n",
        "First, we import the following packages, set the random seed and define a function for getting the device to be used:"
      ],
      "metadata": {
        "id": "27jKiP6rYoJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def set_seed(seed=42069):\n",
        "    # set a random seed for reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "DIhcg6udZ4PC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2-2. Data preprocessing**\n",
        "First of all, we define `COVID19Dataset`, which reads in `.csv` files, extracts features, splits `covid.train.csv` into the training/validation sets (with a ratio of 9:1) and normalizes features. "
      ],
      "metadata": {
        "id": "w508z2QwalsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self, path, mode='train', feats=list(range(93))):\n",
        "        \"\"\"\n",
        "        Prepares a dataset as specified.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            The path of the training dataset or the test set. \n",
        "        mode : str\n",
        "            How the dataset should be prepared. Available options include \"train\", \n",
        "            \"dev\", and \"test\".\n",
        "        feats: \n",
        "            The list of feature indices to consider.\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "        # Step 1: Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float) # note that the ID has been left out\n",
        "        \n",
        "        # Step 2: Prepare the dataset\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Step 3: Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) / self.data[:, 40:].std(dim=0, keepdim=True)\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print(f'Finished reading the {mode} set of COVID19 Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "QPa7FA2cafMI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we need data loaders to load in the datasets as needed.  "
      ],
      "metadata": {
        "id": "4Q9aZm3Scv0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataloader(path, mode, batch_size, feats):\n",
        "    dataset = COVID19Dataset(path, mode=mode, feats=feats)\n",
        "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, pin_memory=True)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "cmGk9G-TcvWu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define a general neural network by defining the class `nn.Module` "
      ],
      "metadata": {
        "id": "shrfOFYLdL_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim, n_relu=64, n_hidden_layers=1):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.nn = nn.Sequential()\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.nn.add_module(f'Linear {i}', torch.nn.Linear(input_dim, n_relu))\n",
        "            self.nn.add_module(f'Activation {i}', torch.nn.ReLU())\n",
        "            input_dim = n_relu\n",
        "        self.nn.add_module('Output', torch.nn.Linear(n_relu, 1))\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nn(x).squeeze(1)\n",
        "        return x\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        return self.criterion(pred, target)"
      ],
      "metadata": {
        "id": "WckvU-n8dLNL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, here we have funcitons for training, validating, and testing the neural network."
      ],
      "metadata": {
        "id": "fzyKk9hSe53P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print(f'Saving model (epoch = {epoch + 1}, validation loss = {min_mse:.4f}')\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "metadata": {
        "id": "AOUWGGRNfATd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "T0HskN4JfJ3S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "metadata": {
        "id": "43InwkygfNU-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the function `nn_workflow` to combine all three functions above, as we are going to test out a lot of different models."
      ],
      "metadata": {
        "id": "xY00oirGfc9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_workflow(config, tr_path='covid.train.csv', tt_path='covid.test.csv'):\n",
        "    device = get_device()                    # get the current available device ('cpu' or 'cuda')\n",
        "    os.makedirs('models', exist_ok=True)     # The trained model will be saved to ./models/\n",
        "\n",
        "    tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], feats=config['feats'])\n",
        "    dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], feats=config['feats'])\n",
        "    tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], feats=config['feats'])\n",
        "\n",
        "    model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "\n",
        "    model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)    # start training the model!\n",
        "\n",
        "    return {'tr_set': tr_set, 'dv_set': dv_set, 'tt_set': tt_set}, model, model_loss_record"
      ],
      "metadata": {
        "id": "LweZV5sGfjCn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's build and train our first neural network, which will use all the possible 93 indicators as features (i.e. no feature selection at all). "
      ],
      "metadata": {
        "id": "f6pzx291htlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': list(range(93)),        # consider all features (i.e. no feature selection)    \n",
        "    'n_epochs': 3000,                # maximum number of epochs\n",
        "    'batch_size': 270,               # mini-batch size for dataloader\n",
        "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.001,                 # learning rate of SGD\n",
        "        'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD83G5LBg4Xg",
        "outputId": "246cbecb-59a9-4108-9501-ac528ef97c7a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 93)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 93)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 93)\n",
            "Saving model (epoch = 1, validation loss = 72.4640\n",
            "Saving model (epoch = 2, validation loss = 45.3732\n",
            "Saving model (epoch = 3, validation loss = 31.6274\n",
            "Saving model (epoch = 4, validation loss = 17.9039\n",
            "Saving model (epoch = 5, validation loss = 10.2930\n",
            "Saving model (epoch = 6, validation loss = 6.6676\n",
            "Saving model (epoch = 7, validation loss = 5.4839\n",
            "Saving model (epoch = 8, validation loss = 4.5378\n",
            "Saving model (epoch = 9, validation loss = 4.0292\n",
            "Saving model (epoch = 10, validation loss = 3.6040\n",
            "Saving model (epoch = 11, validation loss = 3.2605\n",
            "Saving model (epoch = 12, validation loss = 2.9977\n",
            "Saving model (epoch = 13, validation loss = 2.7261\n",
            "Saving model (epoch = 14, validation loss = 2.5697\n",
            "Saving model (epoch = 15, validation loss = 2.3301\n",
            "Saving model (epoch = 16, validation loss = 2.2109\n",
            "Saving model (epoch = 17, validation loss = 2.0680\n",
            "Saving model (epoch = 18, validation loss = 1.9694\n",
            "Saving model (epoch = 19, validation loss = 1.8814\n",
            "Saving model (epoch = 20, validation loss = 1.7644\n",
            "Saving model (epoch = 21, validation loss = 1.6991\n",
            "Saving model (epoch = 22, validation loss = 1.6713\n",
            "Saving model (epoch = 23, validation loss = 1.5787\n",
            "Saving model (epoch = 24, validation loss = 1.5718\n",
            "Saving model (epoch = 25, validation loss = 1.5067\n",
            "Saving model (epoch = 26, validation loss = 1.4616\n",
            "Saving model (epoch = 27, validation loss = 1.4308\n",
            "Saving model (epoch = 28, validation loss = 1.3956\n",
            "Saving model (epoch = 29, validation loss = 1.3703\n",
            "Saving model (epoch = 30, validation loss = 1.3403\n",
            "Saving model (epoch = 31, validation loss = 1.3188\n",
            "Saving model (epoch = 32, validation loss = 1.2940\n",
            "Saving model (epoch = 33, validation loss = 1.2743\n",
            "Saving model (epoch = 34, validation loss = 1.2650\n",
            "Saving model (epoch = 36, validation loss = 1.2134\n",
            "Saving model (epoch = 38, validation loss = 1.1895\n",
            "Saving model (epoch = 40, validation loss = 1.1458\n",
            "Saving model (epoch = 42, validation loss = 1.1365\n",
            "Saving model (epoch = 43, validation loss = 1.1198\n",
            "Saving model (epoch = 44, validation loss = 1.1176\n",
            "Saving model (epoch = 45, validation loss = 1.1010\n",
            "Saving model (epoch = 46, validation loss = 1.0943\n",
            "Saving model (epoch = 49, validation loss = 1.0694\n",
            "Saving model (epoch = 52, validation loss = 1.0606\n",
            "Saving model (epoch = 54, validation loss = 1.0412\n",
            "Saving model (epoch = 56, validation loss = 1.0199\n",
            "Saving model (epoch = 60, validation loss = 1.0014\n",
            "Saving model (epoch = 63, validation loss = 1.0010\n",
            "Saving model (epoch = 64, validation loss = 0.9801\n",
            "Saving model (epoch = 67, validation loss = 0.9669\n",
            "Saving model (epoch = 70, validation loss = 0.9657\n",
            "Saving model (epoch = 73, validation loss = 0.9539\n",
            "Saving model (epoch = 75, validation loss = 0.9314\n",
            "Saving model (epoch = 82, validation loss = 0.9283\n",
            "Saving model (epoch = 84, validation loss = 0.9102\n",
            "Saving model (epoch = 87, validation loss = 0.9072\n",
            "Saving model (epoch = 97, validation loss = 0.8922\n",
            "Saving model (epoch = 100, validation loss = 0.8909\n",
            "Saving model (epoch = 102, validation loss = 0.8868\n",
            "Saving model (epoch = 103, validation loss = 0.8862\n",
            "Saving model (epoch = 109, validation loss = 0.8836\n",
            "Saving model (epoch = 110, validation loss = 0.8785\n",
            "Saving model (epoch = 111, validation loss = 0.8775\n",
            "Saving model (epoch = 113, validation loss = 0.8542\n",
            "Saving model (epoch = 129, validation loss = 0.8392\n",
            "Saving model (epoch = 149, validation loss = 0.8266\n",
            "Saving model (epoch = 155, validation loss = 0.8264\n",
            "Saving model (epoch = 166, validation loss = 0.8194\n",
            "Saving model (epoch = 186, validation loss = 0.8169\n",
            "Saving model (epoch = 194, validation loss = 0.8168\n",
            "Saving model (epoch = 197, validation loss = 0.8157\n",
            "Saving model (epoch = 203, validation loss = 0.8068\n",
            "Saving model (epoch = 242, validation loss = 0.8020\n",
            "Saving model (epoch = 254, validation loss = 0.7997\n",
            "Saving model (epoch = 268, validation loss = 0.7993\n",
            "Saving model (epoch = 282, validation loss = 0.7941\n",
            "Saving model (epoch = 308, validation loss = 0.7933\n",
            "Saving model (epoch = 323, validation loss = 0.7899\n",
            "Saving model (epoch = 329, validation loss = 0.7877\n",
            "Saving model (epoch = 347, validation loss = 0.7844\n",
            "Saving model (epoch = 362, validation loss = 0.7821\n",
            "Saving model (epoch = 398, validation loss = 0.7818\n",
            "Saving model (epoch = 400, validation loss = 0.7816\n",
            "Saving model (epoch = 402, validation loss = 0.7777\n",
            "Saving model (epoch = 427, validation loss = 0.7752\n",
            "Saving model (epoch = 526, validation loss = 0.7732\n",
            "Saving model (epoch = 537, validation loss = 0.7702\n",
            "Saving model (epoch = 564, validation loss = 0.7663\n",
            "Saving model (epoch = 602, validation loss = 0.7650\n",
            "Saving model (epoch = 629, validation loss = 0.7635\n",
            "Saving model (epoch = 667, validation loss = 0.7612\n",
            "Saving model (epoch = 694, validation loss = 0.7600\n",
            "Saving model (epoch = 783, validation loss = 0.7568\n",
            "Saving model (epoch = 889, validation loss = 0.7565\n",
            "Saving model (epoch = 903, validation loss = 0.7508\n",
            "Saving model (epoch = 1051, validation loss = 0.7417\n",
            "Finished training after 1252 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, the model was trained for 1524 epochs and the minimum loss validation loss was 0.7598. The training loss and validation loss as functions of the number of training steps can be plotted as follows."
      ],
      "metadata": {
        "id": "H4Q9EADzicUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = len(loss_record['train'])\n",
        "x_1 = range(total_steps)\n",
        "x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "figure(figsize=(6, 4))\n",
        "plt.plot(x_1, loss_record['train'], c='tab:red', label='Training loss')\n",
        "plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='Validation loss')\n",
        "plt.ylim(0.0, 5.)\n",
        "plt.xlabel('Training steps')\n",
        "plt.ylabel('MSE loss')\n",
        "plt.legend()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "du8boCHqfXl1",
        "outputId": "533d3579-914c-4357-e407-1fa2d9d06342"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we can assess the neural network by comparing its prediction with the labels in the validation set."
      ],
      "metadata": {
        "id": "qXEOvpnaixWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "model.eval()\n",
        "preds, targets = [], []\n",
        "for x, y in datasets['dv_set']:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "        preds.append(pred.detach().cpu())\n",
        "        targets.append(y.detach().cpu())\n",
        "preds = torch.cat(preds, dim=0).numpy()\n",
        "targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "figure(figsize=(5, 5))\n",
        "plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "plt.plot([-0.2, 35], [-0.2, 35], c='b')\n",
        "plt.xlim(-0.2, 35)\n",
        "plt.ylim(-0.2, 35)\n",
        "plt.xlabel('ground truth value')\n",
        "plt.ylabel('predicted value')\n",
        "plt.title('Ground Truth v.s. Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Vs8xd5Jwi6pD",
        "outputId": "165f77d1-39a9-4915-dbb4-e974dbe4cc91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most importantly, we need to save predictions and upload it to Kaggle to assess the model by calculating the test loss."
      ],
      "metadata": {
        "id": "WHtsXcepk84Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pred(tt_set, model, pred_path):\n",
        "    device = get_device()\n",
        "    preds = test(tt_set, model, device)\n",
        "\n",
        "    with open(pred_path, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])"
      ],
      "metadata": {
        "id": "oLLQV2YelE0X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_0.csv')"
      ],
      "metadata": {
        "id": "dcvkUJNkmD0Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be examined on Kaggle, the predictions in `pred_0.csv` have a test loss of xxxx, which is much higher than the minimum validation loss of 0.7598. Therefore, there is clearly an issue of overfitting in the model that consider all the 93 features, which is why we want to perform feature selection."
      ],
      "metadata": {
        "id": "xYSFU8g6mZao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Filter-based feature selection**\n",
        "Filter-based selection methods filter out features based on a user-selected metric. For example, one could examine the absolute value of the Pearson's correlation coefficient ($r$) between the target and each of the features and keep the top $k$ features that have the highest value of $r$. In `scikit-learn`, the following metrics are available. (Check the [documentation](https://scikit-learn.org/stable/modules/feature_selection.html) of scikit-learn for more informaiton.)\n",
        "- For regression\n",
        "  - Pearson correlation coefficient ([`r_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression))\n",
        "  - F-statistics ([`f_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression))\n",
        "  - Mutual information for a continuous target variable ([`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression))\n",
        "- For classification\n",
        "  - $\\chi^2$ value ([chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2))\n",
        "  - ANOVA F-value ([`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2))\n",
        "  - Mutual information for a discrete target variable ([`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif))\n",
        "\n",
        "\n",
        "Notably, some people also use principal component analysis (PCA) to estimate the contribution of each of the transformed features (which is a linear combination of the original features) to the variance of the output variable, and leave out the ones that contribute the least. Here, let's just focus on non-transformed features. Below, let's try all the three metrics for regression in our filter-based feature selection. Note that below we use `SelectKBest` to select to $k$ highest features scoring features, but one could also select the highest scording percentage of features with `SelectPercentile` or other tools like `SelectFpr`, `SelectFdr`, `SelectFwe`, or `GenericUnivariateSelect`. Here, we will only use `SelectKBest` for now. "
      ],
      "metadata": {
        "id": "_xMYp2YQZeHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import r_regression             # metric 1 \n",
        "from sklearn.feature_selection import f_regression             # metric 2\n",
        "from sklearn.feature_selection import mutual_info_regression   # metric 3"
      ],
      "metadata": {
        "id": "MNcSq0fWWPCQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('covid.train.csv')\n",
        "x = data[data.columns[1:94]]\n",
        "y = data[data.columns[94]]"
      ],
      "metadata": {
        "id": "oqmqC9EPZG8I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_features(x, y, score_func):\n",
        "    best_feats = SelectKBest(score_func=score_func, k=5)  # the value of k doesn't matter since we will decide how many to pick later\n",
        "    fit = best_feats.fit(x, y)\n",
        "\n",
        "    result = pd.concat([pd.DataFrame(data.columns[1:94]), pd.DataFrame(fit.scores_)], axis=1)\n",
        "    result.columns = ['Features', 'Score']\n",
        "    print(result.nlargest(93, 'Score'))  # print the best 15 features\n",
        "\n",
        "    # Below we generate a list of indices corresponding to all features ranked from the highest score to the lowest\n",
        "    idx_list = [result.nlargest(len(result), 'Score').iloc[i].name for i in range(len(result))] \n",
        "\n",
        "    return idx_list\n"
      ],
      "metadata": {
        "id": "prplGftBeabc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we try to use the Pearson correlation coefficient as the metric:"
      ],
      "metadata": {
        "id": "awJdC4PcXyPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list_0 = filter_features(x, y, r_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W3hoyaSgXU5",
        "outputId": "7d489611-0c2f-410d-d21e-86a16fa37623"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Features     Score\n",
            "75  tested_positive.1  0.991012\n",
            "57    tested_positive  0.981165\n",
            "42       hh_cmnty_cli  0.879724\n",
            "60     hh_cmnty_cli.1  0.879438\n",
            "78     hh_cmnty_cli.2  0.878218\n",
            "..                ...       ...\n",
            "65             shop.1 -0.412705\n",
            "83             shop.2 -0.415130\n",
            "51     public_transit -0.448360\n",
            "69   public_transit.1 -0.449079\n",
            "87   public_transit.2 -0.450436\n",
            "\n",
            "[93 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we use F-statistics:"
      ],
      "metadata": {
        "id": "HAhASFxMnWo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list_1 = filter_features(x, y, f_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsMciZI7gl3z",
        "outputId": "e964c6f3-8ac7-48ba-d3db-beb8bb5571fb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Features          Score\n",
            "75  tested_positive.1  148069.658278\n",
            "57    tested_positive   69603.872591\n",
            "42       hh_cmnty_cli    9235.492094\n",
            "60     hh_cmnty_cli.1    9209.019558\n",
            "78     hh_cmnty_cli.2    9097.375172\n",
            "..                ...            ...\n",
            "89        depressed.2       2.362492\n",
            "11                 IN       1.291780\n",
            "39                 WI       0.489577\n",
            "15                 LA       0.117097\n",
            "25                 NM       0.012080\n",
            "\n",
            "[93 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we try the mutual information:"
      ],
      "metadata": {
        "id": "K8rlwLPiX67-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list_2 = filter_features(x, y, mutual_info_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wgGhlK9grbG",
        "outputId": "11a0faac-b3b6-46db-9801-343e256efc45"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Features     Score\n",
            "75  tested_positive.1  2.047044\n",
            "57    tested_positive  1.664484\n",
            "78     hh_cmnty_cli.2  0.980106\n",
            "42       hh_cmnty_cli  0.974405\n",
            "60     hh_cmnty_cli.1  0.971703\n",
            "..                ...       ...\n",
            "0                  AL  0.014870\n",
            "3                  AR  0.014597\n",
            "18                 MI  0.010579\n",
            "19                 MN  0.006282\n",
            "11                 IN  0.005286\n",
            "\n",
            "[93 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(idx_list_0[:15])\n",
        "print(idx_list_1[:15])\n",
        "print(idx_list_2[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nE65UAli_G",
        "outputId": "4b1ddaf3-4d7b-476c-eccc-c5d89fa93f75"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92]\n",
            "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92]\n",
            "[75, 57, 78, 42, 60, 43, 79, 61, 58, 77, 40, 59, 76, 41, 84]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list_0[:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPfTjoBjrAQj",
        "outputId": "4289284e-8242-4323-f772-d51fa0158b3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'feats': idx_list_0[:14],        # only consider the best 14 features as decided above\n",
        "    'n_epochs': 10000,                # maximum number of epochs\n",
        "    'batch_size': 200,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        #'lr': 0.0001,                 # learning rate of SGD\n",
        "        #'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 500,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "datasets, model, loss_record = nn_workflow(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkaDhTunqyo8",
        "outputId": "bc2ed4ec-3a52-485f-a0e9-092183c0e0ee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 14)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 14)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 14)\n",
            "Saving model (epoch = 1, validation loss = 275.4506\n",
            "Saving model (epoch = 2, validation loss = 73.5567\n",
            "Saving model (epoch = 3, validation loss = 14.0766\n",
            "Saving model (epoch = 5, validation loss = 13.2827\n",
            "Saving model (epoch = 6, validation loss = 12.5060\n",
            "Saving model (epoch = 7, validation loss = 12.0630\n",
            "Saving model (epoch = 8, validation loss = 11.6295\n",
            "Saving model (epoch = 9, validation loss = 11.1133\n",
            "Saving model (epoch = 10, validation loss = 10.4226\n",
            "Saving model (epoch = 11, validation loss = 9.6201\n",
            "Saving model (epoch = 12, validation loss = 8.7205\n",
            "Saving model (epoch = 13, validation loss = 7.7734\n",
            "Saving model (epoch = 14, validation loss = 6.9389\n",
            "Saving model (epoch = 15, validation loss = 6.1332\n",
            "Saving model (epoch = 16, validation loss = 5.3564\n",
            "Saving model (epoch = 17, validation loss = 4.6807\n",
            "Saving model (epoch = 18, validation loss = 4.0227\n",
            "Saving model (epoch = 19, validation loss = 3.4629\n",
            "Saving model (epoch = 20, validation loss = 2.9907\n",
            "Saving model (epoch = 21, validation loss = 2.5937\n",
            "Saving model (epoch = 22, validation loss = 2.3333\n",
            "Saving model (epoch = 23, validation loss = 2.0624\n",
            "Saving model (epoch = 24, validation loss = 1.9116\n",
            "Saving model (epoch = 25, validation loss = 1.7157\n",
            "Saving model (epoch = 26, validation loss = 1.6683\n",
            "Saving model (epoch = 27, validation loss = 1.5863\n",
            "Saving model (epoch = 28, validation loss = 1.5034\n",
            "Saving model (epoch = 29, validation loss = 1.4723\n",
            "Saving model (epoch = 30, validation loss = 1.4585\n",
            "Saving model (epoch = 31, validation loss = 1.4331\n",
            "Saving model (epoch = 32, validation loss = 1.4287\n",
            "Saving model (epoch = 34, validation loss = 1.4165\n",
            "Saving model (epoch = 35, validation loss = 1.4085\n",
            "Saving model (epoch = 37, validation loss = 1.4010\n",
            "Saving model (epoch = 40, validation loss = 1.3871\n",
            "Saving model (epoch = 41, validation loss = 1.3796\n",
            "Saving model (epoch = 44, validation loss = 1.3677\n",
            "Saving model (epoch = 47, validation loss = 1.3572\n",
            "Saving model (epoch = 48, validation loss = 1.3538\n",
            "Saving model (epoch = 50, validation loss = 1.3529\n",
            "Saving model (epoch = 51, validation loss = 1.3474\n",
            "Saving model (epoch = 52, validation loss = 1.3395\n",
            "Saving model (epoch = 57, validation loss = 1.3269\n",
            "Saving model (epoch = 58, validation loss = 1.3267\n",
            "Saving model (epoch = 61, validation loss = 1.3118\n",
            "Saving model (epoch = 63, validation loss = 1.3058\n",
            "Saving model (epoch = 64, validation loss = 1.3020\n",
            "Saving model (epoch = 72, validation loss = 1.2931\n",
            "Saving model (epoch = 74, validation loss = 1.2853\n",
            "Saving model (epoch = 76, validation loss = 1.2744\n",
            "Saving model (epoch = 79, validation loss = 1.2632\n",
            "Saving model (epoch = 80, validation loss = 1.2598\n",
            "Saving model (epoch = 81, validation loss = 1.2538\n",
            "Saving model (epoch = 84, validation loss = 1.2452\n",
            "Saving model (epoch = 87, validation loss = 1.2427\n",
            "Saving model (epoch = 89, validation loss = 1.2351\n",
            "Saving model (epoch = 94, validation loss = 1.2200\n",
            "Saving model (epoch = 95, validation loss = 1.2154\n",
            "Saving model (epoch = 97, validation loss = 1.2069\n",
            "Saving model (epoch = 102, validation loss = 1.2051\n",
            "Saving model (epoch = 103, validation loss = 1.1896\n",
            "Saving model (epoch = 106, validation loss = 1.1833\n",
            "Saving model (epoch = 108, validation loss = 1.1768\n",
            "Saving model (epoch = 112, validation loss = 1.1714\n",
            "Saving model (epoch = 115, validation loss = 1.1661\n",
            "Saving model (epoch = 117, validation loss = 1.1638\n",
            "Saving model (epoch = 118, validation loss = 1.1586\n",
            "Saving model (epoch = 120, validation loss = 1.1560\n",
            "Saving model (epoch = 121, validation loss = 1.1461\n",
            "Saving model (epoch = 124, validation loss = 1.1398\n",
            "Saving model (epoch = 128, validation loss = 1.1336\n",
            "Saving model (epoch = 130, validation loss = 1.1264\n",
            "Saving model (epoch = 137, validation loss = 1.1205\n",
            "Saving model (epoch = 143, validation loss = 1.0998\n",
            "Saving model (epoch = 147, validation loss = 1.0982\n",
            "Saving model (epoch = 148, validation loss = 1.0904\n",
            "Saving model (epoch = 150, validation loss = 1.0841\n",
            "Saving model (epoch = 153, validation loss = 1.0807\n",
            "Saving model (epoch = 156, validation loss = 1.0717\n",
            "Saving model (epoch = 158, validation loss = 1.0701\n",
            "Saving model (epoch = 164, validation loss = 1.0566\n",
            "Saving model (epoch = 165, validation loss = 1.0546\n",
            "Saving model (epoch = 166, validation loss = 1.0538\n",
            "Saving model (epoch = 167, validation loss = 1.0507\n",
            "Saving model (epoch = 168, validation loss = 1.0493\n",
            "Saving model (epoch = 170, validation loss = 1.0462\n",
            "Saving model (epoch = 171, validation loss = 1.0436\n",
            "Saving model (epoch = 172, validation loss = 1.0419\n",
            "Saving model (epoch = 173, validation loss = 1.0402\n",
            "Saving model (epoch = 181, validation loss = 1.0285\n",
            "Saving model (epoch = 182, validation loss = 1.0273\n",
            "Saving model (epoch = 183, validation loss = 1.0266\n",
            "Saving model (epoch = 184, validation loss = 1.0241\n",
            "Saving model (epoch = 187, validation loss = 1.0171\n",
            "Saving model (epoch = 188, validation loss = 1.0135\n",
            "Saving model (epoch = 196, validation loss = 0.9998\n",
            "Saving model (epoch = 197, validation loss = 0.9981\n",
            "Saving model (epoch = 199, validation loss = 0.9966\n",
            "Saving model (epoch = 204, validation loss = 0.9918\n",
            "Saving model (epoch = 209, validation loss = 0.9895\n",
            "Saving model (epoch = 210, validation loss = 0.9800\n",
            "Saving model (epoch = 220, validation loss = 0.9797\n",
            "Saving model (epoch = 221, validation loss = 0.9607\n",
            "Saving model (epoch = 222, validation loss = 0.9603\n",
            "Saving model (epoch = 224, validation loss = 0.9574\n",
            "Saving model (epoch = 228, validation loss = 0.9536\n",
            "Saving model (epoch = 229, validation loss = 0.9535\n",
            "Saving model (epoch = 233, validation loss = 0.9457\n",
            "Saving model (epoch = 235, validation loss = 0.9409\n",
            "Saving model (epoch = 240, validation loss = 0.9389\n",
            "Saving model (epoch = 241, validation loss = 0.9348\n",
            "Saving model (epoch = 245, validation loss = 0.9290\n",
            "Saving model (epoch = 246, validation loss = 0.9275\n",
            "Saving model (epoch = 247, validation loss = 0.9257\n",
            "Saving model (epoch = 251, validation loss = 0.9220\n",
            "Saving model (epoch = 255, validation loss = 0.9154\n",
            "Saving model (epoch = 256, validation loss = 0.9152\n",
            "Saving model (epoch = 260, validation loss = 0.9117\n",
            "Saving model (epoch = 263, validation loss = 0.9059\n",
            "Saving model (epoch = 267, validation loss = 0.9011\n",
            "Saving model (epoch = 272, validation loss = 0.8978\n",
            "Saving model (epoch = 281, validation loss = 0.8867\n",
            "Saving model (epoch = 283, validation loss = 0.8861\n",
            "Saving model (epoch = 285, validation loss = 0.8853\n",
            "Saving model (epoch = 291, validation loss = 0.8817\n",
            "Saving model (epoch = 292, validation loss = 0.8755\n",
            "Saving model (epoch = 297, validation loss = 0.8699\n",
            "Saving model (epoch = 301, validation loss = 0.8693\n",
            "Saving model (epoch = 302, validation loss = 0.8672\n",
            "Saving model (epoch = 304, validation loss = 0.8642\n",
            "Saving model (epoch = 314, validation loss = 0.8584\n",
            "Saving model (epoch = 318, validation loss = 0.8561\n",
            "Saving model (epoch = 321, validation loss = 0.8520\n",
            "Saving model (epoch = 322, validation loss = 0.8520\n",
            "Saving model (epoch = 323, validation loss = 0.8509\n",
            "Saving model (epoch = 329, validation loss = 0.8465\n",
            "Saving model (epoch = 338, validation loss = 0.8432\n",
            "Saving model (epoch = 345, validation loss = 0.8369\n",
            "Saving model (epoch = 348, validation loss = 0.8366\n",
            "Saving model (epoch = 357, validation loss = 0.8308\n",
            "Saving model (epoch = 365, validation loss = 0.8267\n",
            "Saving model (epoch = 370, validation loss = 0.8247\n",
            "Saving model (epoch = 372, validation loss = 0.8233\n",
            "Saving model (epoch = 374, validation loss = 0.8230\n",
            "Saving model (epoch = 375, validation loss = 0.8221\n",
            "Saving model (epoch = 383, validation loss = 0.8209\n",
            "Saving model (epoch = 385, validation loss = 0.8189\n",
            "Saving model (epoch = 388, validation loss = 0.8187\n",
            "Saving model (epoch = 392, validation loss = 0.8163\n",
            "Saving model (epoch = 400, validation loss = 0.8128\n",
            "Saving model (epoch = 408, validation loss = 0.8098\n",
            "Saving model (epoch = 418, validation loss = 0.8082\n",
            "Saving model (epoch = 422, validation loss = 0.8068\n",
            "Saving model (epoch = 423, validation loss = 0.8060\n",
            "Saving model (epoch = 433, validation loss = 0.8060\n",
            "Saving model (epoch = 438, validation loss = 0.8035\n",
            "Saving model (epoch = 444, validation loss = 0.8022\n",
            "Saving model (epoch = 460, validation loss = 0.8009\n",
            "Saving model (epoch = 466, validation loss = 0.7988\n",
            "Saving model (epoch = 467, validation loss = 0.7984\n",
            "Saving model (epoch = 475, validation loss = 0.7978\n",
            "Saving model (epoch = 477, validation loss = 0.7971\n",
            "Saving model (epoch = 478, validation loss = 0.7969\n",
            "Saving model (epoch = 484, validation loss = 0.7953\n",
            "Saving model (epoch = 488, validation loss = 0.7950\n",
            "Saving model (epoch = 527, validation loss = 0.7938\n",
            "Saving model (epoch = 533, validation loss = 0.7932\n",
            "Saving model (epoch = 554, validation loss = 0.7906\n",
            "Saving model (epoch = 583, validation loss = 0.7905\n",
            "Saving model (epoch = 618, validation loss = 0.7900\n",
            "Saving model (epoch = 620, validation loss = 0.7899\n",
            "Saving model (epoch = 628, validation loss = 0.7882\n",
            "Saving model (epoch = 704, validation loss = 0.7876\n",
            "Saving model (epoch = 981, validation loss = 0.7869\n",
            "Finished training after 1482 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "device = get_device()\n",
        "model = NeuralNet(datasets['tr_set'].dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "save_pred(datasets['tt_set'], model, 'pred_1.csv')"
      ],
      "metadata": {
        "id": "I-B5diHvrHwC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Wrapper-based feature selection**"
      ],
      "metadata": {
        "id": "1C_oAw54ZeKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Embedded feature selection**"
      ],
      "metadata": {
        "id": "0kbUlGj-ZeMt"
      }
    }
  ]
}